# **Chapter 5 - Retrieval-Augmented Generation (RAG) in N8N**

Retrieval-Augmented Generation (RAG) is one of the most powerful patterns in modern AI workflows. Instead of relying only on the LLM’s pre-trained knowledge, RAG combines **external data sources** with the LLM to provide accurate, context-specific answers. In N8N, RAG pipelines can be assembled visually, connecting vector databases, embeddings, and LLMs with minimal coding.

---

## 1. Connecting to Vector Databases (Qdrant, Pinecone, Chroma)

Vector databases store **embeddings** of documents in a way that allows fast semantic search. N8N provides integration nodes for popular vector DBs:

* **Qdrant Node**

  * Open-source, high-performance vector search engine.
  * Can run locally (Docker) or on the cloud.
  * Perfect for small-to-medium scale projects.

* **Pinecone Node**

  * Fully managed vector DB in the cloud.
  * Easy to scale with high availability.
  * Good for production workloads where uptime is critical.

* **Chroma Node**

  * Lightweight and open-source.
  * Ideal for prototyping RAG pipelines.
  * Stores data locally, good for personal assistants or small document sets.

**Typical steps in N8N:**

1. Use a **HTTP Request Node** or dedicated **Qdrant/Pinecone/Chroma node** to connect.
2. Provide API keys or host/port info in N8N’s **Credentials Manager**.
3. Create a workflow step to **insert embeddings** and another step to **query embeddings** by similarity.

---

## 2. Document Loaders and Embeddings in N8N

Before documents can be stored in a vector DB, they need to be **loaded, split, and embedded**:

* **Document Loaders in N8N**

  * **Google Drive/Dropbox/Notion Nodes**: Fetch docs automatically.
  * **HTTP Request Node**: Pull PDFs, text, or JSON from APIs.
  * **File Nodes**: Upload local documents.
  * **Custom Scripts (Code Node)**: Parse PDFs, CSVs, or HTML.

* **Generating Embeddings**

  * Use the **OpenAI Embeddings Node** (e.g., `text-embedding-ada-002`).
  * Each text chunk is converted into a vector representation.
  * Send vectors + metadata to Qdrant/Pinecone/Chroma.

* **Chunking Documents**

  * Long documents should be split into smaller segments (e.g., 500–1000 tokens).
  * Splitting ensures that the vector DB can retrieve the most relevant chunks.

---

## 3. Building a Q\&A Agent with Knowledge Retrieval

The core of RAG is enabling a **Question → Retrieve → Answer** pipeline.

**Step-by-step in N8N:**

1. **User Input**

   * Capture the user’s query (via Webhook Node, UI form, or Chat interface).

2. **Embed the Query**

   * Convert the user’s question into a vector (OpenAI Embeddings Node).

3. **Retrieve Relevant Chunks**

   * Query the vector DB (Qdrant/Pinecone/Chroma) for top-k similar results.

4. **Assemble Context**

   * Extract the retrieved text chunks and merge them into a context string.

5. **Send to LLM**

   * Pass the context + user question into an **LLM Node** (OpenAI/Anthropic/HuggingFace).
   * Example prompt template:

     ```
     You are a helpful assistant. Use the provided context to answer the user’s question. 
     If the context doesn’t contain the answer, say “I don’t know.”

     Context:
     {{retrieved_documents}}

     Question:
     {{user_input}}
     ```

6. **Return Answer**

   * Send the generated answer back to the user (Webhook Response, Email Node, Slack Node, etc.).

---

## 4. Example Workflow Outline

```
Webhook (User Q) → 
OpenAI Embeddings (Encode Q) → 
Qdrant Query (Retrieve Context) → 
Function Node (Merge Chunks) → 
OpenAI LLM (Generate Answer) → 
Webhook Response (Send Back)
```

This creates a simple but production-ready **Q\&A Agent** powered by RAG in N8N.

---

✅ **Key Benefits of RAG in N8N**

* Combine **LLM intelligence** with **custom/private data**.
* No need to retrain models.
* Scales from personal knowledge assistants to enterprise document search systems.

---

