# **Chapter 15: Caching & Performance**

### ðŸŽ¯ **Goal:**

Optimize app performance and reduce redundant computations using Streamlitâ€™s caching system.
Learn how to cache data and resources correctly â€” and how to invalidate caches safely when updates occur.

---

## ðŸ”¹ 15.1 Why Caching Matters

Streamlit re-runs your script from top to bottom every time the user interacts.
This keeps things simple but can slow down apps if expensive operations â€” like DB queries or API calls â€” are repeated.

**Caching** helps by storing the results of expensive computations, so they can be quickly reused.

---

## ðŸ”¹ 15.2 Types of Caching

Streamlit provides two decorators for caching:

| Decorator            | Purpose                                                          | Typical Use                     |
| -------------------- | ---------------------------------------------------------------- | ------------------------------- |
| `@st.cache_data`     | Cache **function outputs** that depend only on input data        | Query results, data processing  |
| `@st.cache_resource` | Cache **global resources** like connections, models, or sessions | DB engine, ML model, API client |

---

### Example â€” Cached Data Function

```python
import streamlit as st
import pandas as pd
import time

@st.cache_data
def load_patients():
    time.sleep(3)  # Simulate slow I/O
    df = pd.DataFrame({
        "id": [1, 2, 3],
        "name": ["Alice", "Bob", "Charlie"]
    })
    return df

st.write("Loading patients...")
df = load_patients()
st.dataframe(df)
```

ðŸŸ¢ The **first call** takes ~3 seconds.
ðŸŸ¢ The **next calls** return instantly â€” same function, same arguments.

---

## ðŸ”¹ 15.3 Caching Resources (e.g., Database Connections)

When working with external resources such as database engines or clients,
use `@st.cache_resource` instead of `@st.cache_data`.

```python
import sqlalchemy as sa

@st.cache_resource
def get_engine():
    engine = sa.create_engine("sqlite:///hospital.db")
    return engine
```

Now the engine is created **once** and reused across reruns.
This improves both speed and stability.

---

## ðŸ”¹ 15.4 Cache Invalidation Keys

Streamlit automatically invalidates caches when the function **source code or arguments** change.
You can also force invalidation manually by adding a **custom key**.

```python
@st.cache_data(ttl=3600, show_spinner=False)
def list_patients(version: int):
    ...
```

* `ttl` â†’ time-to-live in seconds (auto-expire cache)
* `version` â†’ changing this number manually clears the cache

---

## ðŸ”¹ 15.5 Clearing Cache on Write Events

When your app **creates, updates, or deletes** records,
you need to **invalidate** previously cached data so the UI reflects fresh data.

```python
import streamlit as st
from services import patient_service

def refresh_data():
    st.cache_data.clear()  # Clear all cached data functions

def add_patient(patient):
    patient_service.add_patient(patient)
    refresh_data()
```

After a write, `refresh_data()` clears caches.
Next call to `list_patients()` will fetch new data.

---

## ðŸ”¹ 15.6 Practical Cache Design Pattern

A common pattern in MVC-based Streamlit apps:

```
controllers/
    patients_controller.py
services/
    patient_service.py
views/
    patients_view.py
```

In **service layer**, we add cache decorators:

```python
# services/patient_service.py
import streamlit as st
import pandas as pd

@st.cache_data
def list_patients():
    # Example: simulate database query
    return pd.DataFrame([
        {"id": 1, "name": "Alice", "age": 30},
        {"id": 2, "name": "Bob", "age": 25},
    ])
```

In **controller layer**, we call this function safely:

```python
# controllers/patients_controller.py
from services import patient_service

def get_patients():
    return patient_service.list_patients()

def create_patient(new_patient):
    # Write operation (DB insert)
    print("Added:", new_patient)
    # Clear cache so next read is fresh
    patient_service.list_patients.clear()
```

---

## ðŸ§ª **Lab 15 â€” Cache Services with Invalidation**

### âœ… **Goal:**

Cache patient list data and clear cache automatically on writes.

---

### **Step 1 â€” Update `services/patient_service.py`**

```python
import streamlit as st
import pandas as pd

_patients = [
    {"id": 1, "name": "Alice", "age": 30},
    {"id": 2, "name": "Bob", "age": 25},
]

@st.cache_data
def list_patients():
    df = pd.DataFrame(_patients)
    return df

def add_patient(name, age):
    new_id = len(_patients) + 1
    _patients.append({"id": new_id, "name": name, "age": age})
    list_patients.clear()  # Invalidate cache
```

---

### **Step 2 â€” Create `controllers/patients_controller.py`**

```python
from services import patient_service

def show_patients():
    return patient_service.list_patients()

def add_patient(name, age):
    patient_service.add_patient(name, age)
```

---

### **Step 3 â€” Build UI in `pages/2_Patients.py`**

```python
import streamlit as st
from controllers import patients_controller

st.title("Patient Management")

tab1, tab2 = st.tabs(["List", "Add New"])

with tab1:
    st.dataframe(patients_controller.show_patients())

with tab2:
    name = st.text_input("Name")
    age = st.number_input("Age", min_value=0, step=1)
    if st.button("Add Patient"):
        patients_controller.add_patient(name, age)
        st.success(f"Patient {name} added successfully!")
```

ðŸ§© When a new patient is added:

* The serviceâ€™s cache clears.
* The next view refresh shows the updated list.

---

## ðŸ”¹ 15.7 Key Takeaways

âœ… Use `@st.cache_data` for data results
âœ… Use `@st.cache_resource` for global connections
âœ… Use `.clear()` or version keys to invalidate
âœ… Keep caching logic close to the service layer
âœ… Avoid caching mutable objects directly

---

## ðŸ§­ **Next Chapter Preview**

**Chapter 16 â€“ Session Persistence**
Learn how to persist session state across reruns or user sessions using Streamlitâ€™s `SessionState` and lightweight storage techniques.

---
