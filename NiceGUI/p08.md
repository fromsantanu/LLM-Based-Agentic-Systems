# **Lesson 8: Calling an OpenAI or LangChain API from the Chatbot**

In this lesson, youâ€™ll learn how to make your chatbot feel â€œaliveâ€ by connecting it to an **AI model**.

Until now, your chatbot replied only with simple fixed sentences.
Now we will make it reply using:

* **OpenAI API** (direct)
* OR **LangChain** (through a small chain)

You can choose whichever one you prefer.

Think of this as adding a **smart brain** behind your chatbot.

---

## ğŸ§  Real-life example

Imagine you have a personal assistant sitting in the next room.
Whenever a message comes:

1. The chatbot (NiceGUI) delivers the message
2. The assistant (OpenAI or LangChain) thinks and writes a smart reply
3. The chatbot shows the reply on screen

This is exactly what we will build.

---

# ğŸ§© Overall Structure

You will modify your **FastAPI backend** like this:

```
NiceGUI â†’ FastAPI â†’ (OpenAI OR LangChain) â†’ FastAPI â†’ NiceGUI
```

So you only change the backend.
Your NiceGUI code from Chapter 7 stays the same.

---

# â­ PART A â€” Calling OpenAI directly from FastAPI

---

## **Step 1: Install the OpenAI library**

In your terminal:

```bash
pip install openai
```

(If you are using the new SDK, this will install it.)

---

## **Step 2: Set your API key as an environment variable**

This is important so you do not hardcode keys in your code.

In Windows:

```bash
setx OPENAI_API_KEY "your-key-here"
```

In macOS/Linux:

```bash
export OPENAI_API_KEY="your-key-here"
```

---

## **Step 3: Update FastAPI backend**

Modify your previous backend into this:

Create file: **backend_chat_ai.py**

```python
import os
from fastapi import FastAPI
from pydantic import BaseModel
from openai import OpenAI

app = FastAPI()

# create the OpenAI client
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

class ChatRequest(BaseModel):
    message: str

class ChatResponse(BaseModel):
    reply: str

@app.post("/chat", response_model=ChatResponse)
def chat_endpoint(request: ChatRequest):
    user_message = request.message

    # call OpenAI model
    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are a friendly assistant."},
            {"role": "user", "content": user_message}
        ]
    )

    ai_reply = completion.choices[0].message["content"]

    return ChatResponse(reply=ai_reply)
```

---

## ğŸ§¾ Explanation (OpenAI version)

### `client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))`

This creates a helper that talks to the OpenAI servers.

### `model="gpt-4o-mini"`

You can change this model later (e.g., gpt-4.1, gpt-o1).

### `messages=[ system , user ]`

We tell the model:

* what role to play
* what the user said

### `completion.choices[0].message["content"]`

This is the AIâ€™s reply.

---

## â–¶ï¸ Run it

Terminal 1:

```bash
uvicorn backend_chat_ai:app --reload
```

Terminal 2:

```bash
python chat_frontend.py
```

Open browser:

```
http://localhost:8080
```

Now type something like:

> Hello, what can you do?

You will get a real AI reply ğŸ‰

---

# â­ PART B â€” Using LangChain inside the backend

Some people prefer LangChain because it helps structure prompts and tools.

---

## **Step 1: Install LangChain**

```
pip install langchain langchain-openai
```

---

## **Step 2: Create a chain**

Create file: **backend_chat_langchain.py**

```python
import os
from fastapi import FastAPI
from pydantic import BaseModel
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

app = FastAPI()

# LangChain model
llm = ChatOpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    model="gpt-4o-mini"
)

# LangChain prompt template
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful, friendly chatbot."),
    ("user", "{user_text}")
])

class ChatRequest(BaseModel):
    message: str

class ChatResponse(BaseModel):
    reply: str

@app.post("/chat", response_model=ChatResponse)
def chat_endpoint(request: ChatRequest):

    # fill the prompt template
    chain_input = {"user_text": request.message}

    # run the chain
    response = llm.invoke(prompt.format_messages(**chain_input))

    return ChatResponse(reply=response.content)
```

---

## ğŸ§¾ Explanation (LangChain version)

### `ChatOpenAI(...)`

This creates an AI model that LangChain manages for you.

### `prompt = ChatPromptTemplate.from_messages([...])`

This defines the structure of every message you send.

### `prompt.format_messages(**chain_input)`

This replaces `{user_text}` with the actual message.

### `llm.invoke(...)`

This sends the request to OpenAI and returns a smart reply.

---

# â–¶ï¸ Run with LangChain

Terminal 1:

```bash
uvicorn backend_chat_langchain:app --reload
```

Terminal 2:

```bash
python chat_frontend.py
```

Open browser:
`http://localhost:8080`

Your chatbot now replies using LangChain ğŸ‰

---

# ğŸ’¡ Tips & Best Practices

### âœ” Tip 1 â€” Keep system instructions simple

A too-large â€œsystem promptâ€ confuses beginners.
Start with:

> You are a friendly assistant.

You can make it smarter later.

---

### âœ” Tip 2 â€” Test with short messages

Type:

* â€œHiâ€
* â€œTell me a jokeâ€
* â€œExplain diabetes in simple wordsâ€

This helps you see if the backend connection is correct.

---

### âœ” Tip 3 â€” OpenAI = direct

LangChain = structured + expandable
Choose based on your preference.

---

### âœ” Tip 4 â€” Always keep API keys outside your code

Use environment variables or `.env` file for safety.

---

# ğŸ‰ Summary

In this lesson, you learned how to:

* connect your NiceGUI chat window to a FastAPI backend
* add OpenAI replies
* OR add LangChain replies
* run both parts together

Your chatbot is now **fully intelligent** and ready to be expanded.

---

Tell me which one you prefer!

