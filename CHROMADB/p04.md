# **Chapter 4. Working with Embeddings**

Embeddings are the foundation of semantic search and Retrieval-Augmented Generation (RAG) with ChromaDB. They convert unstructured data (like text, images, or code) into high-dimensional numerical vectors that capture semantic meaning. This enables similarity search, clustering, and context retrieval.

In this chapter, we will explore:

* Using OpenAI embeddings with ChromaDB
* Using other embedding models (Sentence Transformers, HuggingFace)
* Storing embeddings in Chroma
* Comparing results from different models

---

## 4.1 Using OpenAI Embeddings with ChromaDB

OpenAI provides powerful embedding models such as `text-embedding-ada-002` and `text-embedding-3-small/large`. These models generate high-quality embeddings for text.

### Example: Generating embeddings with OpenAI

```python
import chromadb
from openai import OpenAI

# Initialize Chroma client
client = chromadb.Client()

# Create a collection
collection = client.create_collection(name="openai_embeddings")

# Initialize OpenAI client
openai_client = OpenAI(api_key="YOUR_OPENAI_API_KEY")

# Example documents
documents = ["ChromaDB is a vector database.", 
             "Embeddings help capture semantic meaning of text."]

# Generate embeddings
embeddings = [
    openai_client.embeddings.create(
        model="text-embedding-3-small", 
        input=doc
    ).data[0].embedding for doc in documents
]

# Store in Chroma
collection.add(
    documents=documents,
    embeddings=embeddings,
    ids=["doc1", "doc2"]
)
```

---

## 4.2 Using Other Embedding Models (Sentence Transformers, HuggingFace)

Open-source models provide more flexibility, especially when running locally without API costs.

### Example: Sentence Transformers

```python
from sentence_transformers import SentenceTransformer
import chromadb

# Load a Sentence Transformer model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Create Chroma client and collection
client = chromadb.Client()
collection = client.create_collection("sbert_embeddings")

# Example documents
documents = ["Vector databases store embeddings.", 
             "Sentence Transformers provide efficient embeddings."]

# Generate embeddings
embeddings = model.encode(documents)

# Store in Chroma
collection.add(
    documents=documents,
    embeddings=embeddings.tolist(),
    ids=["doc1", "doc2"]
)
```

### HuggingFace Transformers

```python
from transformers import AutoTokenizer, AutoModel
import torch

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-mpnet-base-v2")
model = AutoModel.from_pretrained("sentence-transformers/all-mpnet-base-v2")

def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    # Mean pooling
    return outputs.last_hidden_state.mean(dim=1).squeeze().tolist()
```

---

## 4.3 Storing Embeddings in Chroma

Regardless of the model, embeddings can be stored in Chroma using the `collection.add()` method.

```python
collection.add(
    documents=["Example text"],
    embeddings=[[0.1, 0.2, 0.3, ...]],  # vector list
    ids=["unique_id_1"],
    metadatas=[{"source": "manual"}]
)
```

Chroma supports:

* **Documents**: the raw text (optional but useful for inspection)
* **Embeddings**: numerical vectors (required for similarity search)
* **IDs**: unique identifiers for each entry
* **Metadata**: additional info (author, date, tags, etc.)

---

## 4.4 Comparing Results from Different Models

The quality of embeddings varies across models. A simple way to compare is to perform **similarity searches** and evaluate the results.

### Example: Querying with OpenAI vs SBERT

```python
query = "What is a vector database?"

# Query OpenAI collection
results_openai = collection.query(
    query_embeddings=[openai_client.embeddings.create(
        model="text-embedding-3-small", 
        input=query
    ).data[0].embedding],
    n_results=2
)

# Query SBERT collection
results_sbert = collection.query(
    query_embeddings=[model.encode(query).tolist()],
    n_results=2
)

print("OpenAI Results:", results_openai)
print("SBERT Results:", results_sbert)
```

---

## Key Takeaways

* OpenAI embeddings provide **high-quality semantic representations**, but incur API costs.
* Sentence Transformers (SBERT) and HuggingFace models allow **local, cost-free embedding generation**.
* Chroma can store embeddings from **any model** as long as they are numerical vectors.
* Results may differ depending on model architecture and training dataâ€”experiment with both to find the best fit for your use case.

---
