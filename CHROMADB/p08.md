# **Chapter 8. Integration with Python Workflows**

ChromaDB becomes especially powerful when integrated into larger Python workflows. Instead of being a standalone vector store, it can serve as the **memory and retrieval layer** for AI applications. This chapter explores how to integrate Chroma into Retrieval-Augmented Generation (RAG) pipelines and connect it with popular frameworks like **LangChain**, **LlamaIndex**, and **FastAPI/Flask** for API-based access.

---

## 8.1 Building a RAG (Retrieval-Augmented Generation) Pipeline

Retrieval-Augmented Generation (RAG) combines **LLMs (Large Language Models)** with a vector database for **context-aware answering**. Instead of relying on the LLM’s internal memory, you enrich prompts with retrieved context from your ChromaDB collection.

**Workflow:**

1. Store documents and embeddings in ChromaDB.
2. Accept a user query.
3. Embed the query into vector space.
4. Use Chroma’s `query()` to fetch the most relevant chunks.
5. Inject retrieved text into the LLM prompt.
6. Generate a final answer with better grounding.

**Example (OpenAI + Chroma):**

```python
import chromadb
from chromadb.utils import embedding_functions
from openai import OpenAI

# 1. Setup Chroma client and collection
client = chromadb.PersistentClient(path="./chroma_store")
openai_ef = embedding_functions.OpenAIEmbeddingFunction(
    api_key="your-openai-key",
    model_name="text-embedding-ada-002"
)
collection = client.get_or_create_collection("knowledge_base", embedding_function=openai_ef)

# 2. Query user input
user_query = "What are the symptoms of tuberculosis?"

# 3. Search Chroma for relevant documents
results = collection.query(query_texts=[user_query], n_results=3)

# 4. Retrieve context
context = "\n".join(results["documents"][0])

# 5. Use context in LLM
client_ai = OpenAI(api_key="your-openai-key")
prompt = f"Answer the question using the context below:\n\nContext: {context}\n\nQuestion: {user_query}"
response = client_ai.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": prompt}]
)

print(response.choices[0].message.content)
```

This pipeline ensures your answers are **fact-based** rather than relying purely on the LLM’s pretrained knowledge.

---

## 8.2 Using Chroma with LangChain

LangChain integrates directly with Chroma through its `Chroma` wrapper. It manages the RAG pipeline in fewer lines of code.

**Example:**

```python
from langchain.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

# Setup embeddings and vector store
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
vectordb = Chroma(persist_directory="./chroma_store", embedding_function=embeddings)

# Define retriever
retriever = vectordb.as_retriever(search_kwargs={"k": 3})

# Create RAG chain
llm = ChatOpenAI(model="gpt-4")
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff"
)

# Query
result = qa_chain.run("Explain insulin resistance in simple terms.")
print(result)
```

LangChain abstracts away much of the boilerplate, making it easy to plug Chroma into end-to-end AI workflows.

---

## 8.3 Using Chroma with LlamaIndex

[LlamaIndex](https://www.llamaindex.ai/) provides a data framework to structure and query information with LLMs. You can set **Chroma as the vector store backend** for indexing and retrieval.

**Example:**

```python
from llama_index import VectorStoreIndex, ServiceContext
from llama_index.vector_stores import ChromaVectorStore
from llama_index.embeddings.openai import OpenAIEmbedding
import chromadb

# Setup Chroma
chroma_client = chromadb.PersistentClient(path="./chroma_store")
chroma_collection = chroma_client.get_or_create_collection("llama_docs")

# Wrap with LlamaIndex
vector_store = ChromaVectorStore(chroma_collection)
embed_model = OpenAIEmbedding(model="text-embedding-ada-002")

service_context = ServiceContext.from_defaults(embed_model=embed_model)
index = VectorStoreIndex.from_vector_store(vector_store, service_context=service_context)

# Query with LlamaIndex
query_engine = index.as_query_engine()
response = query_engine.query("What is the treatment for diabetes?")
print(response)
```

With LlamaIndex, you gain extra flexibility for document loaders, query engines, and structured outputs.

---

## 8.4 Connecting Chroma with FastAPI/Flask for API Access

Chroma can be exposed as a **backend retrieval service** with a lightweight API layer. This makes your RAG system usable by **frontend apps, chatbots, or external tools**.

### Example with FastAPI

```python
from fastapi import FastAPI
import chromadb
from chromadb.utils import embedding_functions

app = FastAPI()

# Setup Chroma
client = chromadb.PersistentClient(path="./chroma_store")
openai_ef = embedding_functions.OpenAIEmbeddingFunction(api_key="your-openai-key")
collection = client.get_or_create_collection("knowledge_base", embedding_function=openai_ef)

@app.get("/search")
def search(query: str, k: int = 3):
    results = collection.query(query_texts=[query], n_results=k)
    return {"matches": results["documents"]}
```

Run with:

```bash
uvicorn app:app --reload
```

### Example with Flask

```python
from flask import Flask, request, jsonify
import chromadb
from chromadb.utils import embedding_functions

app = Flask(__name__)

client = chromadb.PersistentClient(path="./chroma_store")
openai_ef = embedding_functions.OpenAIEmbeddingFunction(api_key="your-openai-key")
collection = client.get_or_create_collection("knowledge_base", embedding_function=openai_ef)

@app.route("/search", methods=["GET"])
def search():
    query = request.args.get("query")
    k = int(request.args.get("k", 3))
    results = collection.query(query_texts=[query], n_results=k)
    return jsonify({"matches": results["documents"]})

if __name__ == "__main__":
    app.run(debug=True)
```

These APIs let you plug Chroma into **chatbots, dashboards, or mobile apps** with minimal effort.

---

## ✅ Key Takeaways

* **RAG Pipelines**: Chroma enables retrieval-augmented answering with LLMs.
* **LangChain**: Provides a high-level interface to connect Chroma with agents, tools, and chains.
* **LlamaIndex**: Simplifies ingestion, indexing, and querying with Chroma as a backend.
* **API Integration**: FastAPI/Flask allows you to expose Chroma queries as REST APIs for external apps.

---

