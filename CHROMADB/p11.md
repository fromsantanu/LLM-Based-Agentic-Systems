# **Chapter 11. Case Studies & Projects**

This chapter brings together the concepts and techniques covered so far into **practical projects**. These case studies demonstrate how to use **ChromaDB** with embeddings and vector search in real-world applications, such as semantic search, Q\&A bots, recommendation systems, and knowledge base assistants.

---

## 11.1 Semantic Search over PDFs

Semantic search allows users to query documents based on **meaning** rather than just keywords. With ChromaDB, we can embed text extracted from PDFs and perform similarity queries.

### Steps:

1. **Extract text** from PDFs using libraries like `PyPDF2` or `pdfplumber`.
2. **Chunk text** into manageable segments (e.g., 500 tokens).
3. **Embed chunks** using OpenAI or HuggingFace models.
4. **Store embeddings** in a ChromaDB collection.
5. **Query ChromaDB** with user input and retrieve relevant passages.

```python
import chromadb
from chromadb.utils import embedding_functions
import pdfplumber

# 1. Extract text
pdf_text = ""
with pdfplumber.open("sample.pdf") as pdf:
    for page in pdf.pages:
        pdf_text += page.extract_text() + "\n"

# 2. Chunk text
chunks = [pdf_text[i:i+500] for i in range(0, len(pdf_text), 500)]

# 3. Setup ChromaDB with OpenAI embeddings
client = chromadb.Client()
openai_ef = embedding_functions.OpenAIEmbeddingFunction(api_key="YOUR_KEY")

collection = client.create_collection("pdf_semantic", embedding_function=openai_ef)

# 4. Insert chunks
collection.add(documents=chunks, ids=[str(i) for i in range(len(chunks))])

# 5. Query
results = collection.query(query_texts=["Explain the conclusion"], n_results=3)
print(results["documents"])
```

**Use case:** Searching research papers, contracts, or manuals for specific insights.

---

## 11.2 Building a Q\&A Bot with ChromaDB + OpenAI

This project combines **semantic search** with **LLM reasoning**. ChromaDB provides the most relevant context, while OpenAI generates natural responses.

### Workflow:

1. User asks a question.
2. Retrieve relevant chunks from ChromaDB.
3. Send chunks + question to OpenAI for answer generation.

```python
from openai import OpenAI
client = OpenAI(api_key="YOUR_KEY")

def ask_question(question):
    # Step 1: Retrieve context
    results = collection.query(query_texts=[question], n_results=3)
    context = "\n".join(results["documents"][0])
    
    # Step 2: Generate answer
    prompt = f"Answer the question based on the context:\n{context}\n\nQuestion: {question}"
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role":"user","content":prompt}]
    )
    return response.choices[0].message.content

print(ask_question("What are the main findings?"))
```

**Use case:** Chatbots for customer support, research assistants, or document Q\&A.

---

## 11.3 Recommendation System using Vector Search

Recommendation engines benefit from **semantic similarity** between items. Instead of collaborative filtering, ChromaDB can be used to recommend items based on **embedding proximity**.

### Example: Movie Recommendation

1. Create embeddings for movie descriptions.
2. Store in ChromaDB.
3. When a user likes a movie, query ChromaDB for similar embeddings.

```python
movies = [
    {"id": "1", "title": "Inception", "desc": "A mind-bending sci-fi about dreams."},
    {"id": "2", "title": "Interstellar", "desc": "Exploration of space and love across galaxies."},
    {"id": "3", "title": "The Matrix", "desc": "Virtual reality, AI, and freedom."},
]

collection = client.create_collection("movies", embedding_function=openai_ef)
collection.add(documents=[m["desc"] for m in movies], ids=[m["id"] for m in movies], metadatas=movies)

results = collection.query(query_texts=["A sci-fi about space exploration"], n_results=2)
print(results["metadatas"])
```

**Use case:** Product, content, or course recommendation systems.

---

## 11.4 Knowledge Base Assistant with ChromaDB

A **knowledge base assistant** integrates organizational documents (FAQs, policies, manuals) into ChromaDB for instant retrieval and summarization.

### Workflow:

* Ingest documents (policies, FAQs).
* Embed and store in ChromaDB.
* Query with natural language.
* Generate summarized answers with LLM.

```python
faq_data = [
    {"id": "1", "q": "How to reset password?", "a": "Click on forgot password link."},
    {"id": "2", "q": "Refund policy?", "a": "Refunds processed within 7 days."},
]

collection = client.create_collection("faq", embedding_function=openai_ef)
collection.add(
    documents=[f"{faq['q']} - {faq['a']}" for faq in faq_data],
    ids=[faq["id"] for faq in faq_data],
)

query = "How do I get my money back?"
results = collection.query(query_texts=[query], n_results=1)
print(results["documents"])
```

**Use case:** Internal company assistant, customer-facing chatbot, or educational helper.

---

## ðŸ”‘ Key Takeaways

* **Semantic search** with ChromaDB allows retrieval of context-rich passages from unstructured documents like PDFs.
* **Q\&A bots** combine ChromaDB retrieval with LLM reasoning for interactive applications.
* **Recommendation systems** leverage vector similarity to suggest relevant items.
* **Knowledge base assistants** enhance organizational productivity by turning static docs into interactive AI helpers.

---

