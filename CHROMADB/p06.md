# **Chapter 6. Querying in Depth**

Once documents and embeddings are stored in **ChromaDB**, the next step is to query them effectively. Querying is central to applications such as **semantic search, RAG (Retrieval-Augmented Generation), and recommendation engines**. In this chapter, we will explore different querying strategies in depth.

---

## 6.1 Similarity Search with `query()`

The core operation in ChromaDB is **similarity search**, where you provide a query text (or embedding), and Chroma retrieves the most relevant documents.

```python
import chromadb
from chromadb.utils import embedding_functions

# Initialize client and collection
client = chromadb.Client()
collection = client.get_or_create_collection("my_docs")

# Use OpenAI embeddings (can also use SentenceTransformers)
openai_ef = embedding_functions.OpenAIEmbeddingFunction(api_key="your_api_key", model_name="text-embedding-3-small")

# Query using similarity search
results = collection.query(
    query_texts=["machine learning in healthcare"],
    n_results=3
)

print(results)
```

üîë **Key parameters**:

* `query_texts`: List of search queries (strings).
* `n_results`: Number of closest results to return.
* `query_embeddings`: If you already have embeddings, you can pass them directly instead of text.

---

## 6.2 Filtering by Metadata

Metadata allows you to constrain searches beyond text similarity. For example, you might only want documents from a specific source or date.

```python
# Query with metadata filter
results = collection.query(
    query_texts=["machine learning"],
    n_results=3,
    where={"category": "healthcare"}
)

print(results)
```

Here, only documents with `metadata["category"] == "healthcare"` will be considered.

**Supported filters**:

* Equality: `{"field": "value"}`
* Inequality: `{"field": {"$ne": "value"}}`
* Comparisons: `{"year": {"$gt": 2020}}`
* Logical operations:

  ```python
  where = {
      "$or": [
          {"category": "finance"},
          {"year": {"$gte": 2021}}
      ]
  }
  ```

---

## 6.3 Combining Text and Metadata Filters

You can combine **semantic similarity search** with **structured filtering**.

```python
results = collection.query(
    query_texts=["neural networks"],
    n_results=5,
    where={
        "$and": [
            {"category": "healthcare"},
            {"year": {"$gte": 2022}}
        ]
    }
)

print(results)
```

This retrieves the **top 5 most semantically similar documents about neural networks** that also belong to the `healthcare` category and were added after 2022.

---

## 6.4 Understanding Distance Metrics

ChromaDB relies on **distance metrics** to measure similarity between embeddings. The choice of metric affects results.

1. **Cosine Similarity**

   * Measures the angle between vectors.
   * Insensitive to vector magnitude.
   * Common in NLP tasks.
   * Value range: `[-1, 1]` (higher is more similar).

   Formula:

   $$
   \text{cosine}(A, B) = \frac{A \cdot B}{||A|| \, ||B||}
   $$

2. **Euclidean Distance (L2)**

   * Measures straight-line distance in vector space.
   * Sensitive to vector magnitude.
   * Best for dense embeddings where scale matters.

   Formula:

   $$
   L2(A, B) = \sqrt{\sum_i (A_i - B_i)^2}
   $$

3. **Dot Product**

   * Measures raw projection of one vector onto another.
   * Works well if embeddings are normalized.
   * Used in some transformer models.

   Formula:

   $$
   \text{dot}(A, B) = \sum_i A_i \times B_i
   $$

‚öñÔ∏è **Choosing the right metric**:

* Use **cosine similarity** when working with text embeddings (default in most libraries).
* Use **L2 distance** when dealing with numeric feature vectors where magnitude is meaningful.
* Use **dot product** for performance in large-scale retrieval when embeddings are already normalized.

---

## ‚úÖ Key Takeaways

* Use `query()` for **similarity search**, returning relevant documents by embeddings.
* Apply **metadata filters** to constrain results by structured attributes like year, author, or category.
* Combine **text queries with metadata filtering** for more precise retrieval.
* Understand and select the **distance metric** based on your embedding type and use case.

---

