# Chapter 7: Debugging & Iteration

Building agentic AI systems is rarely a straight path—debugging and iterative refinement are central to creating reliable, production-ready workflows. LangFuse provides structured observability that makes it easier to pinpoint issues, replay traces, and evolve your agents over time.

---

## 7.1 Identifying Bottlenecks in Reasoning

* **Trace Visualization**: LangFuse traces show hierarchical spans (LLM calls, tool calls, decision branches). Look for spans that take significantly longer or return poor outputs.
* **Common Bottlenecks**:

  * **Slow LLM calls** → Try smaller models, prompt optimization, or caching.
  * **Overly long chains** → Reduce unnecessary reasoning steps.
  * **Repeated backtracking** → Indicates unclear instructions or missing context.
* **Metrics to Track**:

  * Latency per span.
  * Number of retries per agent.
  * Length of intermediate reasoning chains.

---

## 7.2 Debugging Failed Tool Calls or API Integrations

Agent workflows often depend on tools (databases, APIs, custom Python functions). Failures here must be debugged systematically:

* **Error Logging**: Capture error messages and HTTP response codes directly in spans.
* **Root Cause Patterns**:

  * Authentication/authorization failures.
  * Schema mismatch between LLM output and API requirements.
  * Timeout due to slow endpoints.
* **Best Practices**:

  * Add schema validation middleware.
  * Use fallback strategies (retry with backoff, alternate tool).
  * Keep mock APIs for reproducible debugging.

---

## 7.3 Replaying Past Traces for Iterative Improvement

* **Trace Replay**: LangFuse allows you to re-run specific traces with updated prompts, model versions, or agent logic.
* **When to Use Replay**:

  * Testing prompt variations against the same input.
  * Benchmarking new model versions with old user queries.
  * Debugging failures without waiting for new live traffic.
* **Workflow**:

  1. Select a failed trace in the dashboard.
  2. Modify agent logic/prompt.
  3. Replay with identical inputs.
  4. Compare results against the original.

---

## 7.4 Versioning and Comparing Multiple Agent Strategies

As your system matures, you’ll experiment with prompts, tools, and reasoning strategies. LangFuse supports **versioning** for controlled iteration.

* **Strategy A/B Testing**:

  * Run two agent versions side by side.
  * Compare task completion rates, latency, cost, and user feedback.
* **Version Tags in LangFuse**:

  * Assign semantic versions (e.g., `triage-agent:v1.2`).
  * Record all inputs/outputs per version.
* **Iterative Deployment**:

  * Start with shadow mode (new version runs but doesn’t affect user).
  * Gradually promote better-performing strategies.

---

## ✅ Key Takeaways

* Debugging starts with **trace-level observability**—identify where agents waste time or fail.
* Tool/API debugging requires **structured error spans and validation checks**.
* Replaying traces accelerates **prompt and logic iteration**.
* Versioning allows for **controlled experimentation** and **A/B testing** before production rollout.

---
