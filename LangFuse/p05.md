# Chapter 5: Evaluation & Quality Monitoring

Agentic AI applications are only as reliable as their weakest outputs. To ensure quality, we need continuous evaluation of responses, hallucination tracking, and structured feedback loops. LangFuse provides a flexible framework for **human-in-the-loop feedback** and **automatic evaluation pipelines**.

---

## 5.1 Human-in-the-Loop Feedback Collection

Even the best LLMs fail in unpredictable ways. Human review is therefore essential.

* **Inline Feedback in LangFuse Dashboard**
  Review traces and annotate model outputs directly from the LangFuse UI (üëç/üëé, free text comments, structured labels).

* **End-User Feedback Integration**
  Use LangFuse‚Äôs API or SDK to collect feedback from end-users (e.g., in a chatbot interface).

  ```python
  from langfuse import Langfuse

  lf = Langfuse()

  # attach user feedback to a trace
  lf.feedback.create(
      trace_id="trace_123",
      name="user_rating",
      value=1,  # 1 = positive, 0 = negative
      comment="The answer was clear and correct."
  )
  ```

* **Feedback Types**

  * Binary (thumbs up/down)
  * Likert scales (1‚Äì5 ratings)
  * Free-form comments
  * Domain-specific checklists (e.g., ‚Äúclinically safe?‚Äù for healthcare apps)

---

## 5.2 Automatic Evaluations

LangFuse supports integrating evaluation functions to assess quality without human review.

* **Built-in Metrics**

  * Latency
  * Token usage
  * Error rates

* **LLM-based Evaluations**
  Leverage another LLM to check responses for:

  * **Factuality** ‚Üí Is the content consistent with the provided context?
  * **Relevance** ‚Üí Did the response answer the user‚Äôs query?
  * **Coherence** ‚Üí Is the text logically structured and readable?

  Example with LangChain + LangFuse callback:

  ```python
  from langchain.evaluation import load_evaluator

  evaluator = load_evaluator("criteria", criteria="relevance")

  result = evaluator.evaluate_strings(
      prediction="Paris is the capital of France.",
      reference="Paris is the capital of France."
  )

  print(result)
  # {"relevance": "correct"}
  ```

---

## 5.3 Tracking Hallucinations and Failure Modes

* **Hallucination Monitoring**
  Flag outputs that invent facts not supported by context.
  Example: In a medical workflow, if the model suggests a non-existent drug.

* **Failure Categories**

  * Hallucinations (fabricated facts)
  * Irrelevant answers
  * Incomplete responses
  * Safety violations (toxic or biased output)

* **Trace Tagging in LangFuse**
  You can label and filter traces to identify systemic issues.

  ```python
  lf.trace.update(
      id="trace_123",
      metadata={"failure_mode": "hallucination"}
  )
  ```

---

## 5.4 Setting Up Evaluation Pipelines

You can build **custom evaluation pipelines** that run automatically after model inference.

* **Scoring Functions**
  Implement domain-specific scoring (e.g., BLEU, ROUGE, clinical safety checklist).

  ```python
  def custom_scoring_function(prediction, reference):
      return {"similarity": int(prediction == reference)}

  score = custom_scoring_function("Paris is capital of France", "Paris is capital of France")
  print(score)  # {"similarity": 1}
  ```

* **Automated Workflow**

  1. Capture trace ‚Üí
  2. Run scoring function ‚Üí
  3. Store evaluation in LangFuse ‚Üí
  4. Visualize in dashboard ‚Üí
  5. Iterate & improve models.

* **Evaluation Dashboard**
  LangFuse provides charts for:

  * Average evaluation scores
  * Trends over time
  * Breakdown by agent/tool

---

‚úÖ **Key Takeaway**: Evaluation is not a one-time task. By combining **human feedback**, **automatic evaluations**, and **failure monitoring**, LangFuse helps you continuously improve the quality and trustworthiness of your Agentic AI systems.

---
