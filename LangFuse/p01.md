# Chapter 1: Getting Started with LangFuse

## Introduction: What is LangFuse and Why It Matters in Agentic AI

Building **Agentic AI applications** often involves many moving parts—LLMs, vector databases, APIs, and orchestration frameworks like **LangChain** or **LangGraph**. While these systems can perform impressive tasks, they also bring challenges:

* How do you **trace** what an agent is doing at each step?
* How do you **measure latency, cost, and quality** of responses?
* How do you **evaluate** whether the workflow is meeting expectations?
* How do you **collect feedback** and refine the system over time?

This is where **LangFuse** comes in.

LangFuse is an **open-source observability and evaluation platform** for LLM-powered applications. It gives developers the ability to:

* Log every request, span, and result in a structured way.
* Monitor performance, errors, costs, and user feedback.
* Run evaluations on LLM outputs (e.g., factual correctness, safety).
* Integrate seamlessly with orchestration frameworks like LangChain and LangGraph.

In short, **LangFuse acts as the "black box recorder" for your agentic workflows**, enabling you to build **reliable, transparent, and production-ready AI applications**.

---

## Installing & Setting Up LangFuse

LangFuse can be used in two ways:

### 1. Cloud Option

* **Easiest way to start.**
* Sign up at [LangFuse Cloud](https://langfuse.com).
* Create a project and get your **Public Key** and **Secret Key**.
* Use these credentials in your Python or JS application to connect your traces to LangFuse.

### 2. Self-Hosted Option

* Recommended if you want **full control over data** (e.g., in regulated industries like healthcare or finance).
* Requires Docker and Postgres.
* Steps:

  1. Clone the LangFuse GitHub repo.
  2. Run `docker-compose up` to start services.
  3. Access the dashboard at `http://localhost:3000`.
  4. Set environment variables for **LANGFUSE_PUBLIC_KEY** and **LANGFUSE_SECRET_KEY** in your project.

---

## Core Concepts in LangFuse

LangFuse is built around a few simple but powerful concepts:

* **Traces** → High-level logs of a single user request or session.

  * Example: A patient query handled by a hospital triage bot.

* **Spans** → Sub-steps or components within a trace.

  * Example: Retrieving medical history, querying the vector DB, generating a diagnosis.

* **Metrics** → Quantitative measures attached to traces/spans.

  * Example: Latency (ms), Token usage, API cost.

* **Evaluations** → Automated or manual checks for quality.

  * Example: Does the diagnosis follow medical SOPs? Is the response factually correct?

* **Feedback** → User or system-generated labels.

  * Example: Thumbs up/down, star ratings, error reports.

Together, these give **complete visibility** into your agent’s decision-making process.

---

## Connecting LangFuse with LangChain / LangGraph

LangFuse provides **integrations** for popular frameworks:

### LangChain

```python
from langfuse import Langfuse
from langchain.llms import OpenAI

# Initialize LangFuse
langfuse = Langfuse(
    public_key="your_public_key",
    secret_key="your_secret_key"
)

# Wrap your LLM calls
llm = OpenAI()
with langfuse.trace(name="triage_session") as trace:
    response = llm.predict("What are the symptoms of pneumonia?")
    trace.log_output(response)
```

### LangGraph

LangGraph workflows often involve multiple steps (nodes, edges). You can connect each **node execution** as a span inside a LangFuse trace.

```python
from langfuse import Langfuse

langfuse = Langfuse(
    public_key="your_public_key",
    secret_key="your_secret_key"
)

with langfuse.trace(name="patient_diagnosis") as trace:
    with trace.span(name="symptom_analysis") as span:
        # Your LangGraph node execution
        result = analyze_symptoms(patient_data)
        span.log_output(result)
```

This integration ensures every step of your **agentic pipeline is monitored**, from input to final output.

---

✅ **Next Chapter Preview**: In **Chapter 2**, we’ll explore how to **log traces and spans effectively**, visualize them in the LangFuse dashboard, and start building **evaluation pipelines** for your Agentic AI workflows.

---

