# Chapter 2: Instrumentation Basics

In this chapter, we will explore how to instrument your application with **LangFuse** so that you can monitor and debug Large Language Model (LLM) interactions effectively. Instrumentation is the backbone of observability — it lets you capture what’s happening inside your AI application, visualize it, and improve it systematically.

---

## 2.1 Logging LLM Calls with LangFuse

LangFuse provides client libraries (for Python, TypeScript, etc.) to log every interaction with an LLM. At its core, you can record a **trace** that represents one complete task or workflow, and within it, multiple **spans** that capture individual model calls or steps.

**Example (Python with LangChain integration):**

```python
from langfuse import Langfuse
from langchain.chat_models import ChatOpenAI

# Initialize LangFuse client
langfuse = Langfuse()

# Start a new trace
trace = langfuse.trace(name="support_chat")

# Log an LLM call as a span
llm = ChatOpenAI(model="gpt-4")

with trace.span(name="customer_query") as span:
    response = llm.predict("What is your return policy?")
    span.log_output(response)
```

This logs a single trace with one span, making the interaction visible in the LangFuse dashboard.

---

## 2.2 Capturing Inputs, Outputs, and Intermediate Reasoning Steps

LLMs often work through multiple stages — prompt formatting, tool usage, or chain-of-thought steps (when safely exposed in structured form). LangFuse lets you log **inputs and outputs** at each stage.

```python
with trace.span(name="faq_lookup") as span:
    query = "What is your return policy?"
    span.log_input(query)

    # intermediate reasoning (if structured)
    reasoning = {"retrieval": "policy_docs.pdf", "confidence": 0.85}
    span.log_metadata(reasoning)

    # output
    answer = "You may return items within 30 days with proof of purchase."
    span.log_output(answer)
```

This gives you a detailed timeline of how the response was generated.

---

## 2.3 Adding Metadata

Metadata provides **contextual glue** for your logs. You can attach user identifiers, session info, or app state so later you can filter and analyze interactions.

```python
trace.update_metadata({
    "user_id": "u1234",
    "session_id": "s5678",
    "app_version": "1.0.2",
    "flow": "customer_support"
})
```

Benefits of metadata:

* Track **which users** experience errors.
* Compare results across **versions** of your app.
* Segment logs by **workflow** (e.g., FAQ bot vs. booking assistant).

---

## 2.4 Visualizing Traces in the LangFuse Dashboard

Once your application is instrumented, the **LangFuse dashboard** becomes your command center. You can:

* See each **trace** as a tree of spans (LLM calls, tool invocations, sub-agents).
* Inspect **inputs/outputs** side-by-side.
* Explore **latency and token usage**.
* Drill down into **metadata filters** (e.g., all sessions for a given user).

This makes debugging far easier — instead of guessing where something went wrong, you can follow the **execution flow step by step**.

---

✅ **Key Takeaway:** Instrumentation is not just about logging errors — it’s about building visibility into your AI workflows. By logging calls, capturing inputs/outputs, and attaching metadata, you lay the foundation for continuous improvement.

---
