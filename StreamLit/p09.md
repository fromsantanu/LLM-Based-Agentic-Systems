# Chapter 9 — Streaming Responses for Realistic Agent Interaction

> Goal: Make your agents feel alive. In this chapter you’ll stream partial outputs (token-by-token or line-by-line) so users see the reply forming in real time—just like a human typist.

---

## Why stream?

* **Perceived speed**: Users get instant feedback instead of waiting for the full response.
* **Trust & transparency**: Seeing reasoning or translations unfold reassures users the system is working.
* **Interruptibility**: You can let users cancel mid-stream if they already got what they needed.

---

## The API: `st.write_stream`

`st.write_stream` accepts a **generator** (sync or async) that yields text or delta-like chunks. Streamlit will render each piece as it arrives.

**Contract (mental model)**

* You pass a callable: `st.write_stream(generator_fn)` or `st.write_stream(generator_instance)`.
* The generator **yields strings** (common case) or **dict deltas** (advanced, e.g., structured chunks). Each yield immediately updates the UI.
* When the generator completes, Streamlit finalizes the element.

**Minimal demo**

```python
import time
import streamlit as st

def token_gen():
    tokens = ["Hello", ", ", "world", "!\n"]
    for t in tokens:
        time.sleep(0.2)
        yield t

st.subheader("Minimal streaming demo")
st.write_stream(token_gen)
```

---

## UX patterns that feel natural

1. **Chat bubbles**: Use `st.chat_message` and call `write_stream` inside the assistant bubble.
2. **Progress cues**: Pair streams with `st.status` or `st.progress` for background steps.
3. **Cancelable flows**: Save a `stop` flag in `st.session_state` and check it inside the generator.
4. **Line-by-line vs token-by-token**: For translations or transcripts, line-by-line often reads better.

---

## Example A — Translator Agent (line-by-line streaming)

**Scenario**: Translate a medical instruction into the target language, streaming one line at a time as soon as it’s ready.

> We’ll simulate a translation backend with a stub. Replace the `fake_translate_line` with your actual model/service call.

```python
# streamlit_app.py
import time
import asyncio
import streamlit as st

st.set_page_config(page_title="Translator Agent (Streaming)", page_icon="🌐")

# --------------------
# Helpers
# --------------------

LANGS = ["English", "Hindi", "Bengali", "Spanish"]

async def fake_translate_line(line: str, target: str) -> str:
    # Simulate latency and a simple "translation" transformation.
    await asyncio.sleep(0.3)
    return f"[{target}] {line.strip()}"

async def translator_stream(src_text: str, target_lang: str, stop_key: str):
    # Stream line-by-line
    for line in src_text.splitlines(keepends=False):
        # Cooperative cancel check at safe boundaries
        if st.session_state.get(stop_key):
            yield "\n\n**(stopped by user)**"
            return
        translated = await fake_translate_line(line, target_lang)
        yield translated + "\n"

# --------------------
# UI
# --------------------

st.title("🌐 Translator Agent — Streaming line by line")

if "streaming" not in st.session_state:
    st.session_state.streaming = False
if "stop" not in st.session_state:
    st.session_state.stop = False

with st.sidebar:
    st.markdown("### Settings")
    target = st.selectbox("Target language", options=LANGS, index=1)
    speed = st.slider("Simulated speed (ms/line)", 0, 1000, 300, step=50)
    st.caption("Adjust to see the effect of streaming pace.")

# Bind speed into our stub at runtime
async def fake_translate_line(line: str, target: str) -> str:  # override above
    await asyncio.sleep(speed / 1000)
    return f"[{target}] {line.strip()}"

src = st.text_area(
    "Medical instruction (source)",
    value=(
        "Take one tablet of amoxicillin 500 mg every 8 hours after meals.\n"
        "Drink plenty of water (2–3 liters/day).\n"
        "If rash or swelling occurs, stop and contact your doctor."
    ),
    height=160,
)

col1, col2 = st.columns([1,1])
start = col1.button("▶️ Translate (stream)", disabled=st.session_state.streaming)
stop_btn = col2.button("⏹ Stop", disabled=not st.session_state.streaming)

if start:
    st.session_state.stop = False
    st.session_state.streaming = True

if stop_btn:
    st.session_state.stop = True

if st.session_state.streaming:
    with st.chat_message("assistant"):
        # Wrap the async generator and feed it to write_stream
        async def run():
            async for chunk in translator_stream(src, target, stop_key="stop"):
                yield chunk
            # Reset state when done
            st.session_state.streaming = False

        st.write_stream(run)
```

**What to notice**

* We stream **per line**, not per token, which feels natural for instructions.
* We implement **cancel** via `stop` in session state and check it between lines.
* We use an **async generator**, which `st.write_stream` happily consumes.

---

## Example B — Token streaming in a chat bubble

For LLMs that emit tokens, yield them as soon as you receive them from your client SDK.

```python
import time
import streamlit as st

st.subheader("Token-by-token chat streaming (mock)")

PROMPT = "Summarize: Patient presents with fever, cough, and myalgia for 3 days.\n"

# Simulated token source
TOKENS = ["Patient ", "likely ", "has ", "a ", "viral ", "syndrome", ". ",
          "Recommend ", "rest, ", "hydration, ", "and ", "paracetamol."]

def token_source():
    for t in TOKENS:
        time.sleep(0.1)
        yield t

with st.chat_message("user"):
    st.markdown(PROMPT)
with st.chat_message("assistant"):
    st.write_stream(token_source)
```

**Drop-in with real LLMs**

* Replace `token_source()` with your client’s streaming iterator. Most SDKs expose an async/sync iterator over chunks or deltas; yield `.content` as each arrives.
* If your SDK yields objects, map to strings before yielding.

---

## Pattern — Combine streams with `st.status`

Give users context while text streams.

```python
import time
import streamlit as st

st.subheader("Streaming + status steps")

steps = ["Detect language", "Translate", "Quality check"]

with st.status("Running translation pipeline…", expanded=True) as status:
    for s in steps:
        st.write(f"• {s}…")
        time.sleep(0.4)
    status.update(label="Pipeline complete", state="complete")

st.write_stream(lambda: (t for t in ["Partial ", "output ", "appears ", "here."]))
```

---

## Cancellation & cleanup (robust flows)

* **Cooperative checks**: Inspect a `stop` flag between yields.
* **Timeouts**: Use `asyncio.wait_for` to avoid hanging.
* **Finalization**: After your generator completes (or is canceled), clear session flags and close any client streams.

```python
import asyncio
import streamlit as st

if "stop" not in st.session_state:
    st.session_state.stop = False

async def cancellable_stream():
    for i in range(10):
        if st.session_state.stop:
            yield "\n(cancelled)"
            return
        await asyncio.sleep(0.2)
        yield f"Chunk {i} "

colA, colB = st.columns(2)
if colA.button("Start"):
    st.session_state.stop = False
    st.write_stream(cancellable_stream)
if colB.button("Stop"):
    st.session_state.stop = True
```

---

## Latency smoothing tips

* **Chunk intelligently**: For translations, line-by-line; for free text, token or sentence chunks.
* **Early first paint**: Yield a small placeholder (e.g., a non‑breaking space) quickly so the bubble appears instantly.
* **Backpressure**: If your backend floods tokens, buffer into short bursts (e.g., every 30–60 ms) to reduce reflows.

---

## Error handling & retries

* Wrap backend calls in try/except inside your generator; yield a friendly message before exiting.
* Use exponential backoff for transient network failures; if you must abort, finalize the stream with a short explanation.

```python
import asyncio
import streamlit as st

async def resilient_stream():
    try:
        for i in range(3):
            await asyncio.sleep(0.2)
            yield f"Chunk {i} "
        # Simulate failure
        raise RuntimeError("Backend timeout")
    except Exception as e:
        yield f"\n**Oops:** {e}"

st.write_stream(resilient_stream)
```

---

## Putting it together — Full translator app (copy–paste ready)

```python
# app.py
import asyncio
import streamlit as st

st.set_page_config(page_title="Medical Translator (Streaming)", page_icon="🩺")

LANGS = ["English", "Hindi", "Bengali", "Spanish"]

if "streaming" not in st.session_state:
    st.session_state.streaming = False
if "stop" not in st.session_state:
    st.session_state.stop = False

st.title("🩺 Medical Translator — Live Streaming")

with st.sidebar:
    target = st.selectbox("Target language", LANGS, index=1)
    pace_ms = st.slider("Line latency (ms)", 0, 1000, 250, 25)

src = st.text_area(
    "Paste instruction(s)",
    """Take metformin 500 mg twice daily with meals.
Check your blood sugar every morning before breakfast.
If you feel dizzy or have vision changes, contact a clinician.""",
    height=180,
)

start, stop = st.columns(2)
if start.button("▶️ Start Streaming", disabled=st.session_state.streaming):
    st.session_state.stop = False
    st.session_state.streaming = True
if stop.button("⏹ Stop", disabled=not st.session_state.streaming):
    st.session_state.stop = True

async def translate_line(line: str) -> str:
    await asyncio.sleep(pace_ms / 1000)
    return f"[{target}] {line.strip()}"

async def stream_translation(text: str):
    lines = [ln for ln in text.splitlines() if ln.strip()]
    for i, ln in enumerate(lines, 1):
        if st.session_state.stop:
            yield "\n**(stopped by user)**"
            return
        out = await translate_line(ln)
        yield f"{out}\n"

with st.chat_message("user"):
    st.markdown(src)
with st.chat_message("assistant"):
    if st.session_state.streaming:
        st.write_stream(lambda: stream_translation(src))
    else:
        st.caption("Click **Start Streaming** to see line-by-line output here.")
```

---

## Checklist for production

* [ ] Stream **as early as possible**—don’t buffer the first chunk.
* [ ] Provide a **Stop** affordance and handle cleanup.
* [ ] Log timestamps for each chunk to analyze latency.
* [ ] Render within **chat bubbles** for familiarity.
* [ ] Add **status steps** if there’s meaningful pre/post work.
* [ ] Guard PII: don’t leak sensitive inputs while streaming debug info.

---

## Extension ideas

* **Batched sentence streaming** for better readability in long paragraphs.
* **Dual-pane view** (source on left, streaming target on right) for translators.
* **Word-level highlighting**: stream plus progressive alignment to show which word is being translated.
* **Speech-out**: pair streaming text with TTS to read lines as they arrive.

---
