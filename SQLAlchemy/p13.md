## **Chapter 13: Optimizations and Performance**

### **Overview**

As your application grows, database performance becomes increasingly important. SQLAlchemy provides several tools and techniques to optimize query execution, reduce database load, and improve responsiveness.
In this chapter, youâ€™ll learn how to:

* Perform **batching and bulk operations** efficiently.
* Implement **query caching** for repeated queries.
* Use **indexing and constraints** effectively.
* **Profile SQL queries** to detect bottlenecks.

---

## **13.1 Batching and Bulk Operations**

When inserting or updating large amounts of data, committing each record individually can be slow due to repeated network round-trips.
SQLAlchemy supports **bulk operations** that group multiple statements into one or more efficient database calls.

### **Example: Bulk Insert**

```python
from sqlalchemy import create_engine, Column, Integer, String
from sqlalchemy.orm import declarative_base, Session

engine = create_engine("sqlite:///students.db", echo=False)
Base = declarative_base()

class Student(Base):
    __tablename__ = "students"
    id = Column(Integer, primary_key=True)
    name = Column(String)
    age = Column(Integer)

Base.metadata.create_all(engine)

students = [
    {"name": "Alice", "age": 21},
    {"name": "Bob", "age": 22},
    {"name": "Charlie", "age": 20},
]

with Session(engine) as session:
    session.bulk_insert_mappings(Student, students)
    session.commit()
```

âœ… **Why itâ€™s efficient:**
`bulk_insert_mappings()` bypasses ORM-level object construction and inserts directly using mappings, reducing Python overhead.

---

### **Example: Bulk Update**

```python
updates = [
    {"id": 1, "age": 22},
    {"id": 2, "age": 23},
]

with Session(engine) as session:
    session.bulk_update_mappings(Student, updates)
    session.commit()
```

âœ… **Tip:**
Use bulk operations only when you donâ€™t need ORM event handling or relationship management, as they skip those layers for speed.

---

## **13.2 Query Caching**

If your application frequently executes the same query, itâ€™s beneficial to **cache results** in memory to reduce repetitive database calls.

### **Example: Using dogpile.cache for SQLAlchemy**

You can integrate SQLAlchemy with caching libraries like [`dogpile.cache`](https://dogpilecache.sqlalchemy.org/).

```python
from sqlalchemy.orm import Session
from dogpile.cache import make_region

region = make_region().configure("dogpile.cache.memory")

def get_student_by_name(session, name):
    cache_key = f"student_{name}"
    student = region.get(cache_key)
    if student is None:
        student = session.query(Student).filter_by(name=name).first()
        region.set(cache_key, student)
    return student

with Session(engine) as session:
    student = get_student_by_name(session, "Alice")
```

âœ… **Benefit:**
The first query fetches from the database, subsequent ones are served from cache until invalidated.

---

### **Alternative: ORM-Level Caching**

For short-lived caching during a session:

```python
session = Session(engine)
query = session.query(Student).filter(Student.age > 20)
students = query.all()  # Executes SQL once
students_again = query.all()  # Served from session identity map cache
```

âœ… **Session Identity Map**
Each session maintains an in-memory cache of loaded objects, avoiding duplicate fetches for the same row.

---

## **13.3 Indexing and Constraints**

Proper **indexing** improves lookup performance, while **constraints** ensure data integrity.

### **Adding Indexes**

Indexes can be declared directly on columns or using the `Index` construct.

```python
from sqlalchemy import Index

class Student(Base):
    __tablename__ = "students"
    id = Column(Integer, primary_key=True)
    name = Column(String, index=True)
    age = Column(Integer)

# or explicitly
Index('idx_name_age', Student.name, Student.age)
```

âœ… **Best Practices:**

* Index columns used in `WHERE`, `ORDER BY`, and `JOIN` clauses.
* Avoid indexing every columnâ€”it adds overhead during inserts/updates.

---

### **Constraints**

Constraints enforce rules like uniqueness and referential integrity.

```python
from sqlalchemy import UniqueConstraint, CheckConstraint

class Course(Base):
    __tablename__ = "courses"
    id = Column(Integer, primary_key=True)
    code = Column(String, unique=True)
    capacity = Column(Integer)
    enrolled = Column(Integer)

    __table_args__ = (
        CheckConstraint('enrolled <= capacity', name='check_enrollment'),
        UniqueConstraint('code', name='unique_course_code'),
    )
```

âœ… **Purpose:**

* `UniqueConstraint` prevents duplicates.
* `CheckConstraint` enforces logical consistency.

---

## **13.4 Profiling SQL Queries**

Optimizing without measurement can lead to guesswork. Profiling helps identify **slow queries**, **redundant joins**, and **excessive round-trips**.

### **Enable SQL Logging**

Use the `echo=True` flag when creating the engine:

```python
engine = create_engine("sqlite:///students.db", echo=True)
```

SQLAlchemy will print all executed SQL statements to the console.

---

### **Using SQLAlchemy Events for Timing**

```python
from sqlalchemy import event
import time

@event.listens_for(engine, "before_cursor_execute")
def before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    conn.info.setdefault('query_start_time', []).append(time.time())

@event.listens_for(engine, "after_cursor_execute")
def after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    total = time.time() - conn.info['query_start_time'].pop(-1)
    print(f"Query completed in {total:.4f} seconds: {statement}")
```

âœ… **Benefit:**
This helps you measure and log execution times for every SQL query.

---

### **Using `EXPLAIN` for Query Plans**

```python
with engine.connect() as conn:
    result = conn.execute(text("EXPLAIN QUERY PLAN SELECT * FROM students WHERE age > 20"))
    for row in result:
        print(row)
```

âœ… **What it does:**
`EXPLAIN` reveals how the database optimizer executes your queryâ€”useful for understanding when to add or modify indexes.

---

## **13.5 Summary**

| Concept                    | Purpose                                      | Benefit                                |
| -------------------------- | -------------------------------------------- | -------------------------------------- |
| **Batching & Bulk Ops**    | Insert/update many records efficiently       | Reduces ORM overhead                   |
| **Query Caching**          | Store and reuse results                      | Cuts down redundant DB hits            |
| **Indexing & Constraints** | Speed up queries and maintain data integrity | Ensures optimal search and consistency |
| **Profiling**              | Measure query performance                    | Identify bottlenecks and improve speed |

---

## **âœ… Key Takeaways**

* Always **batch inserts/updates** when working with large datasets.
* Implement **caching** for repeated queries or static data.
* Use **indexes** judiciously on frequently queried columns.
* Profile SQL statements regularly to maintain good performance.

---

## **ðŸ§© Practice Exercise**

1. Create a `Product` table with columns: `id`, `name`, `category`, `price`.

   * Add an index on `category` and a unique constraint on `name`.
2. Insert 100 records using `bulk_insert_mappings()`.
3. Write a query to get all products with `price > 500`, and profile it using the `event` listeners.
4. Try adding a cache layer and measure improvement.

---

