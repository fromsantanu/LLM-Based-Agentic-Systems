# **Chapter 4. Building Blocks of RAG**

Retrieval-Augmented Generation (RAG) systems are built on a set of essential steps that connect raw knowledge sources to large language models (LLMs). Each building block contributes to the overall pipeline, ensuring that relevant information is captured, stored, retrieved, and presented to the model in a way that maximizes accuracy and relevance.

---

## 4.1 Preprocessing and Chunking Text Data

Most real-world documents are lengthy and diverse in format (PDFs, Word files, web pages, research articles). Since LLMs have context window limits, feeding entire documents directly is inefficient. Instead, documents are **preprocessed** and broken down into smaller units called **chunks**.

* **Preprocessing tasks**:

  * Removing headers, footers, and boilerplate text.
  * Standardizing encodings and handling OCR errors.
  * Normalizing whitespace, punctuation, and formatting.

* **Chunking strategies**:

  * **Fixed-size chunks** (e.g., 500–1,000 tokens).
  * **Semantic chunking** (splitting based on sections, paragraphs, or topic changes).
  * **Sliding windows** for overlapping context to preserve continuity.

Good chunking ensures each piece contains self-contained meaning, improving retrieval accuracy.

---

## 4.2 Creating and Storing Embeddings

Once text is chunked, each chunk is transformed into a **vector embedding**—a numerical representation that captures semantic meaning.

* **Embedding models**:

  * OpenAI’s `text-embedding-ada-002`
  * Sentence Transformers (e.g., `all-MiniLM-L6-v2`)
  * Domain-specific embeddings (e.g., BioBERT for biomedical text).

* **Embedding properties**:

  * High-dimensional (typically 384–1,536 dimensions).
  * Preserve semantic similarity (similar texts → closer vectors).

* **Storage**:

  * Embeddings are stored in **vector databases** such as FAISS, Pinecone, Weaviate, Qdrant, or Chroma.
  * Metadata (e.g., document ID, source, timestamp) is stored alongside embeddings for filtering and provenance tracking.

---

## 4.3 Querying the Vector Database

When a user asks a question, the system converts the query into its own embedding using the same embedding model. This query vector is compared against stored embeddings in the vector database.

* **Similarity measures**:

  * **Cosine similarity** (most common).
  * **Dot product** or **Euclidean distance** in certain setups.

* **Retrieval process**:

  1. Encode user query → vector.
  2. Perform **k-nearest neighbor (kNN) search** in the vector database.
  3. Retrieve top *k* chunks with the highest similarity scores.
  4. Optionally apply **filters** (e.g., by date, author, topic).

Efficient retrieval ensures only the most relevant information is passed to the LLM, reducing noise and improving output quality.

---

## 4.4 Passing Retrieved Chunks to the LLM

The retrieved chunks are appended to the user’s query before being sent to the LLM. This provides the model with **grounding context**, ensuring its response is informed by external knowledge rather than relying purely on pretrained data.

* **Challenges**:

  * Limited context window → need to select only the most relevant chunks.
  * Avoiding redundancy → remove duplicate or overlapping chunks.
  * Preserving order and logical flow → arrange retrieved text coherently.

This step bridges retrieval and generation, ensuring the LLM has enough context to produce accurate, factual, and contextually relevant responses.

---

## 4.5 Prompt Construction for RAG Responses

The final step is constructing a **prompt** that combines the user’s query and retrieved knowledge in a structured way. Prompt design strongly influences the quality of responses.

* **Basic template**:

  ```
  You are a knowledgeable assistant. Use the following information to answer the question.

  Context:
  {retrieved_chunks}

  Question:
  {user_query}

  Answer:
  ```

* **Best practices**:

  * Clearly separate retrieved context from instructions.
  * Use delimiters (e.g., triple backticks, XML tags) to avoid confusion.
  * Specify desired output format (e.g., “Answer in bullet points” or “Provide a concise summary”).
  * Provide system-level instructions for factuality (e.g., “If you are unsure, state that the answer cannot be found in the provided context”).

---

## Summary

The building blocks of RAG can be seen as a pipeline:

1. **Preprocess and chunk documents** → make data digestible.
2. **Create embeddings** → represent semantic meaning numerically.
3. **Query the vector database** → retrieve the most relevant chunks.
4. **Pass retrieved chunks to the LLM** → supply context for generation.
5. **Construct prompts** → guide the LLM to produce grounded, reliable answers.

Mastering these components enables developers to build robust RAG systems that blend the strengths of information retrieval with the generative power of LLMs.

---


