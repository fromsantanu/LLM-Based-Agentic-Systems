# **Chapter 3. Architecture of RAG Systems**

Retrieval-Augmented Generation (RAG) systems combine information retrieval techniques with the generative capabilities of large language models (LLMs). The architecture determines how queries are processed, how knowledge is retrieved, and how responses are generated in context. Below, we break down the essential components and workflow of a RAG pipeline.

---

## 3.1 End-to-End Workflow

The standard RAG workflow follows a **four-stage pipeline**:

1. **User Query**
   The user provides an input (e.g., ‚ÄúSummarize this policy paper‚Äù or ‚ÄúExplain this medical report‚Äù).

2. **Retriever**

   * The system searches external knowledge sources (vector databases, search engines, document stores) to retrieve relevant documents or chunks.
   * Retrieval is based on semantic similarity or keyword matching.

3. **LLM Generator**

   * Retrieved documents are injected into the **context window** of the LLM.
   * The LLM uses both its **pre-trained knowledge** and the **retrieved evidence** to generate an informed response.

4. **Response Delivery**
   The final response is returned to the user, ideally grounded in retrieved knowledge to reduce hallucinations.

> **Example:**
> A doctor asks a RAG-based assistant: *‚ÄúWhat are the side effects of biologics used in Crohn‚Äôs disease?‚Äù*
> The retriever fetches recent clinical trial summaries, and the generator integrates them into a concise, accurate answer.

---

## 3.2 Retriever Models: Sparse vs Dense

The retriever is the backbone of a RAG system. Two main approaches dominate:

* **Sparse Retrievers (Lexical-based)**

  * Use inverted indexes (e.g., **BM25**, TF-IDF).
  * Match based on **exact keyword overlap**.
  * Strengths: fast, interpretable, and effective when exact wording matters.
  * Weaknesses: struggle with synonyms, paraphrasing, and semantic meaning.

* **Dense Retrievers (Embedding-based)**

  * Encode queries and documents into **high-dimensional vectors** using models like **Sentence-BERT**, **OpenAI embeddings**, or **E5**.
  * Retrieval is based on vector similarity (cosine, dot product).
  * Strengths: capture **semantic similarity** (e.g., ‚Äúheart attack‚Äù ‚âà ‚Äúmyocardial infarction‚Äù).
  * Weaknesses: require vector databases, more compute-heavy, may drift if embeddings are poorly aligned.

üëâ Modern RAG often uses **hybrid retrievers** that combine sparse and dense retrieval for the best of both worlds.

---

## 3.3 Document Chunking, Indexing, and Metadata Usage

Since LLMs cannot handle entire books or databases at once, documents must be **pre-processed**:

* **Chunking**

  * Split documents into smaller sections (e.g., 512‚Äì1000 tokens).
  * Overlap windows to preserve context continuity.
  * Example: breaking a research paper into abstract, methods, results, and discussion.

* **Indexing**

  * Chunks are embedded into vector representations and stored in a **vector database** (e.g., **Pinecone, Qdrant, Weaviate, FAISS, Chroma**).
  * Indexes accelerate similarity searches.

* **Metadata Usage**

  * Store **extra attributes** with chunks, such as author, date, source type, or section.
  * Useful for filtering results (e.g., ‚Äúonly return documents published after 2023‚Äù).

---

## 3.4 Generator Models: LLMs

The generator is the **creative brain** of the RAG pipeline. It integrates retrieved evidence into coherent responses. Commonly used LLMs include:

* **Proprietary APIs**: GPT-4, Claude, Gemini
* **Open-source models**:

  * **LLaMA** family (Meta)
  * **Mistral** (efficient, small-context models)
  * **Gemma** (Google‚Äôs open-source lightweight models)
  * **Falcon, Qwen, DeepSeek** and others

Key considerations for selecting a generator model:

* **Accuracy vs hallucination risk**
* **Latency and cost** (API vs local deployment)
* **Domain specialization** (general-purpose vs fine-tuned medical/legal/scientific models)

---

## 3.5 Memory and Context Windows in RAG

One of the biggest architectural challenges is the **context window**‚Äîthe maximum text length an LLM can handle in one pass.

* **Short-context models** (4k‚Äì8k tokens): require more aggressive chunking and filtering.
* **Long-context models** (32k‚Äì200k tokens): allow entire documents or larger retrieved sets to be considered.

To extend memory beyond fixed context windows, RAG systems use:

* **Hierarchical retrieval**: multi-step retrieval pipelines.
* **Memory layers**: storing prior interactions in external databases for continuity.
* **Summarization buffers**: condensing past exchanges before reinserting into the LLM.

üëâ Effective memory design ensures that RAG systems remain **coherent across sessions** and can handle **long-term knowledge accumulation**.

---

## Summary

The architecture of RAG systems integrates **retrievers** (sparse, dense, or hybrid), **document preprocessing** (chunking, indexing, metadata), and **generators** (LLMs) within the constraints of **context windows and memory management**. The end-to-end workflow ensures that user queries are enriched with relevant evidence, reducing hallucinations and improving factual grounding.

Well-designed RAG pipelines provide **scalable, domain-adaptable, and cost-effective** solutions for real-world applications in healthcare, finance, education, law, and beyond.

---

