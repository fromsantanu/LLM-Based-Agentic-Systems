# **Chapter 5. Implementing RAG in Practice**

This chapter focuses on how Retrieval-Augmented Generation (RAG) can be implemented in real-world projects. We will look at practical implementations using popular frameworks and libraries such as **LangChain, LlamaIndex, N8N, HuggingFace**, and also build a **step-by-step Python pipeline** from scratch.

---

## 5.1 RAG with LangChain

LangChain provides modular building blocks for constructing end-to-end RAG pipelines.

### Steps in LangChain RAG:

1. **Load Documents** → From PDF, text files, APIs, or databases.
2. **Chunking** → Split text into manageable chunks (e.g., 500 tokens).
3. **Embedding** → Convert chunks into vectors using OpenAI, HuggingFace, or local models.
4. **Vector Store** → Store embeddings in FAISS, Chroma, Pinecone, etc.
5. **Retriever** → Perform similarity search for relevant chunks.
6. **LLM Generator** → Pass retrieved chunks + query to LLM for response.

### Example (LangChain + ChromaDB):

```python
from langchain.chains import RetrievalQA
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader

# Load and split text
loader = TextLoader("document.txt")
documents = loader.load()
splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = splitter.split_documents(documents)

# Embeddings + Vector Store
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(docs, embeddings)

# RAG Chain
retriever = vectorstore.as_retriever()
llm = ChatOpenAI(model="gpt-4")
qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

print(qa.run("Summarize the key points of this document."))
```

---

## 5.2 RAG with LlamaIndex

LlamaIndex (formerly GPT Index) is designed for **knowledge augmentation** of LLMs. It emphasizes building **indexes** that bridge data sources and models.

### Steps:

1. Load data into `Document` objects.
2. Use `VectorStoreIndex` or `TreeIndex` to organize data.
3. Query using a retriever and integrate with LLM.

### Example:

```python
from llama_index import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms import OpenAI

# Load documents
documents = SimpleDirectoryReader("data").load_data()

# Create index
index = VectorStoreIndex.from_documents(documents)

# Query engine
query_engine = index.as_query_engine()

response = query_engine.query("What are the key insights?")
print(response)
```

*LlamaIndex provides higher-level abstraction than LangChain, making it easier for rapid prototyping.*

---

## 5.3 RAG in N8N Workflows

N8N is a low-code automation tool. With recent AI integrations, you can build **agentic RAG workflows** visually.

### Workflow Example:

1. **Trigger Node** → HTTP request or chatbot input.
2. **Text Splitter Node** → Preprocess documents.
3. **Vector Store Node** → Store embeddings (e.g., Pinecone/Weaviate).
4. **Retriever Node** → Perform semantic search.
5. **LLM Node** → Pass query + context to OpenAI/HuggingFace LLM.
6. **Response Node** → Return answer via chat UI, email, or Slack.

This makes RAG implementation accessible without writing extensive code.

---

## 5.4 RAG using HuggingFace Libraries

HuggingFace provides open-source tools for embeddings and LLM inference.

### Steps:

1. Use **`sentence-transformers`** for embeddings.
2. Store vectors in FAISS.
3. Retrieve relevant documents.
4. Use **`transformers`** pipeline for generation.

### Example:

```python
from sentence_transformers import SentenceTransformer
import faiss
from transformers import pipeline

# 1. Embeddings
model = SentenceTransformer('all-MiniLM-L6-v2')
documents = ["AI improves healthcare.", "RAG combines search with LLMs."]
doc_embeddings = model.encode(documents)

# 2. Store in FAISS
dim = doc_embeddings.shape[1]
index = faiss.IndexFlatL2(dim)
index.add(doc_embeddings)

# 3. Query + Retrieval
query = "What is RAG?"
q_emb = model.encode([query])
D, I = index.search(q_emb, k=1)

# 4. LLM Generation
generator = pipeline("text-generation", model="gpt2")
context = documents[I[0][0]]
response = generator(f"Context: {context}\nQuestion: {query}\nAnswer:", max_length=100)
print(response[0]['generated_text'])
```

---

## 5.5 Simple RAG Pipeline in Python (Step-by-Step)

To illustrate the core logic without external frameworks, let’s build a **barebones RAG pipeline**.

### Step 1: Define documents

```python
documents = [
    "The capital of France is Paris.",
    "Python is widely used for AI and machine learning.",
    "RAG improves LLMs by combining retrieval with generation."
]
```

### Step 2: Generate embeddings (using cosine similarity)

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

vectorizer = TfidfVectorizer()
doc_vectors = vectorizer.fit_transform(documents)
```

### Step 3: Retrieve relevant document

```python
query = "What is RAG?"
q_vector = vectorizer.transform([query])
similarities = cosine_similarity(q_vector, doc_vectors)
best_doc = documents[similarities.argmax()]
```

### Step 4: Generate response (simple template)

```python
response = f"Based on my knowledge, {best_doc}"
print(response)
```

**Output:**

```
Based on my knowledge, RAG improves LLMs by combining retrieval with generation.
```

This minimal pipeline demonstrates the **essence of RAG** without heavy frameworks.

---

✅ **Summary:**

* **LangChain** → Modular, production-ready pipelines.
* **LlamaIndex** → Simplified, index-based abstraction.
* **N8N** → Low-code visual workflows.
* **HuggingFace** → Open-source embeddings + LLM inference.
* **Pure Python** → Educational barebones RAG pipeline.

---
