# **Chapter 9. Evaluation of RAG Systems**

Evaluating Retrieval-Augmented Generation (RAG) systems is crucial to ensure they are not only returning relevant knowledge but also integrating it effectively into generated outputs. Unlike pure retrieval or pure generation tasks, RAG evaluation must jointly consider **retrieval quality** and **generation quality**, as well as the overall system efficiency and cost.

---

## Key Metrics for RAG Evaluation

1. **Precision**

   * Measures the fraction of retrieved or generated results that are correct and relevant.
   * In retrieval: out of all retrieved documents, how many were truly relevant?
   * In generation: out of all facts included, how many were accurate?

2. **Recall**

   * Measures the fraction of relevant items that were successfully retrieved.
   * High recall ensures the model doesn’t miss important evidence.
   * Example: If 10 relevant documents exist in the knowledge base and the retriever returns 7, recall is 0.7.

3. **Relevance**

   * Goes beyond strict correctness: how well do the retrieved chunks align with the user query intent?
   * Often measured using **normalized discounted cumulative gain (nDCG)** or **Mean Reciprocal Rank (MRR)**, which account for the ranking order.

4. **Factual Accuracy**

   * Critical in high-stakes domains like healthcare, finance, or law.
   * Evaluates whether the generated response faithfully reflects source documents without hallucination.
   * Metrics:

     * **Faithfulness scores** (whether claims are supported by evidence).
     * **Consistency checks** across multiple retrieved sources.

---

## Human-in-the-Loop Evaluation

* **Role of Human Judgment**

  * Automated metrics often fail to capture subtle aspects of relevance or factuality.
  * Human evaluators assess:

    * Fluency of the answer.
    * Appropriateness of retrieved evidence.
    * Whether critical information is missing or distorted.

* **Evaluation Protocols**

  * **Pairwise comparison:** Present answers from two systems and ask evaluators which is better.
  * **Rubric-based evaluation:** Use scoring sheets (e.g., 1–5 for correctness, completeness, clarity).
  * **Domain expert validation:** Especially important for specialized applications (e.g., medicine, law).

* **Active Learning Loop**

  * Human feedback can be fed back into the system to improve retrievers and generators through fine-tuning or preference optimization.

---

## Benchmark Datasets for RAG

Several standardized datasets help evaluate retrieval and generation jointly:

* **Natural Questions (NQ)**

  * Real user queries with answers drawn from Wikipedia.
  * Good for evaluating factual QA over large corpora.

* **HotpotQA**

  * Requires **multi-hop reasoning**, where answers need information from multiple documents.
  * Useful to test complex retrieval chains.

* **TriviaQA**

  * Open-domain QA benchmark with diverse topics.
  * Tests both retrieval coverage and LLM knowledge grounding.

* **MS MARCO (Machine Reading Comprehension)**

  * Large-scale dataset of real search queries.
  * Popular for retriever evaluation and passage ranking.

* **FEVER (Fact Extraction and Verification)**

  * Designed for fact-checking tasks.
  * Evaluates the ability of RAG to ground outputs in evidence.

---

## Evaluating Efficiency and Cost

* **Latency**

  * Time taken to retrieve documents and generate responses.
  * Important in interactive applications like chatbots or copilots.

* **Scalability**

  * Can the system handle millions of documents and concurrent users?
  * Techniques: sharding, approximate nearest neighbor (ANN) search, caching.

* **Cost Efficiency**

  * Balancing retrieval depth and generation size against computational cost.
  * Factors:

    * Embedding computation and storage cost.
    * Vector database query cost.
    * LLM token cost (retrieved context length directly impacts this).

* **Trade-offs**

  * Increasing recall often means retrieving more documents, which may increase cost and latency.
  * Optimal balance depends on application: e.g., healthcare requires higher accuracy even at higher cost, while customer support might prioritize speed.

---

✅ **Summary:**
Evaluating RAG systems requires a **multi-dimensional approach**. Precision, recall, relevance, and factual accuracy measure effectiveness; human-in-the-loop evaluation adds nuance; benchmark datasets provide comparability; and efficiency/cost evaluation ensures real-world deployability. Together, these dimensions ensure RAG systems are reliable, scalable, and suited to their intended domain.

---

