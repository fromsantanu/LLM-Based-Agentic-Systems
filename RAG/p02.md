# **Chapter 2. Core Concepts**

To understand Retrieval-Augmented Generation (RAG), it’s essential to master its **core building blocks**. These concepts form the foundation on which more advanced techniques and applications are built.

---

## 2.1 Embeddings: Turning Text into Vectors

* **Definition**: An *embedding* is a numerical representation of text (words, sentences, or even entire documents) in a high-dimensional vector space.
* **Purpose**: Embeddings allow machines to measure semantic similarity — i.e., whether two pieces of text are *about the same thing* rather than just using the same words.
* **How they work**:

  * A large neural network (often a Transformer) maps text into vectors (lists of floating-point numbers).
  * Semantically similar texts are mapped to nearby points in this vector space.

**Example**

* "doctor" and "physician" will have embeddings close to each other.
* "doctor" and "airplane" will be far apart in the vector space.

**Common embedding models**:

* OpenAI’s `text-embedding-3` series
* Sentence Transformers (e.g., `all-MiniLM-L6-v2`)
* Cohere embeddings
* Domain-specific models (biomedical, legal, multilingual)

---

## 2.2 Vector Databases

Once you have embeddings, you need a way to **store and efficiently query them**. That’s where vector databases come in. They specialize in handling high-dimensional vectors and finding nearest neighbors quickly.

**Popular options**:

* **Chroma** → lightweight, Python-friendly, often used with LangChain.
* **Pinecone** → managed service, scales to billions of vectors with low latency.
* **Qdrant** → open-source, high-performance, supports filtering and hybrid search.
* **Weaviate** → open-source, schema-based, integrates with external ML models.
* **FAISS** → library (by Facebook/Meta), optimized for dense vector search on CPU/GPU.

**Key features across these systems**:

* Indexing for fast similarity search
* Metadata storage (to filter results by category, date, author, etc.)
* Scalability (from thousands to billions of documents)

---

## 2.3 Similarity Search

Similarity search is the **heart of retrieval** in RAG. It answers: *Given a query embedding, which documents in the database are most similar?*

### Distance Metrics

* **Cosine similarity**: Measures the angle between two vectors. Common in NLP because it ignores vector length and focuses on direction.
* **Dot product**: A raw similarity score (often correlated with cosine if vectors are normalized).
* **Euclidean distance (L2)**: Straight-line distance between vectors. Useful when magnitude differences matter.

**Example**:
Query → "symptoms of diabetes"
Retrieved documents → medical notes, articles, or FAQs with embeddings closest to the query.

---

## 2.4 The Retrieval → Generation Pipeline Explained

The RAG pipeline is typically a **two-step process**:

1. **Retrieval**

   * User inputs a query: *"What are the side effects of metformin?"*
   * The system encodes it into an embedding.
   * The vector database finds the *top-k* most relevant documents (e.g., medical articles mentioning metformin).

2. **Generation**

   * The retrieved documents are passed, along with the query, into a Large Language Model (LLM).
   * The LLM uses both the prompt and the retrieved context to generate a factually-grounded answer.

**Advantages**:

* Reduces hallucination by anchoring the LLM to real data.
* Allows updating knowledge simply by updating the database (no retraining needed).
* Scales across domains (healthcare, legal, education, enterprise knowledge bases).

---

✅ **In summary**:

* **Embeddings** convert text into numerical form.
* **Vector databases** store and query embeddings.
* **Similarity search** finds relevant documents.
* The **retrieval → generation pipeline** combines them, powering RAG systems that are both scalable and trustworthy.

---

