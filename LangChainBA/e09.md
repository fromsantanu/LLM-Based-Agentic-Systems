Let’s **debug a hallucinating chatbot** by forcing it to ground answers in sources and **attach reference-style citations**—and fail gracefully when it can’t find support.

Below is a compact, production-ready pattern with:

* Retrieval-augmented generation (RAG)
* Numbered, verifiable citations like **\[1]**, **\[2]**
* Automatic validation (parser) that **rejects uncited answers** and **retries/falls back**
* Optional LangSmith tracing hooks

---

# Goal

1. The bot must cite sources for any factual claim.
2. If retrieval finds nothing relevant, it should say “I don’t know” (no bluffing).
3. If the LLM forgets to include citations, we **detect → retry → fallback**.

---

# Minimal runnable code (LangChain, Python)

> Works with LangChain 0.2+ style. Replace `OPENAI_API_KEY` and install deps:
> `pip install langchain langchain-openai langchain-chroma chromadb`

```python
# --- Setup
import os
os.environ["OPENAI_API_KEY"] = "YOUR_KEY"

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.schema import OutputParserException

# (Optional) LangSmith
# os.environ["LANGCHAIN_TRACING_V2"] = "true"
# os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
# os.environ["LANGCHAIN_API_KEY"] = "YOUR_LANGSMITH_KEY"
# os.environ["LANGCHAIN_PROJECT"] = "hallucination-debug-demo"

# --- Toy corpus with URLs (or file paths). Use real docs in production.
docs = [
    Document(page_content="The capital of France is Paris.", metadata={"title":"CIA World Factbook: France","url":"https://www.cia.gov/the-world-factbook/countries/france/"}),
    Document(page_content="The Eiffel Tower was completed in 1889.", metadata={"title":"Official Eiffel Tower Site","url":"https://www.toureiffel.paris/en"}),
    Document(page_content="London is the capital of the United Kingdom.", metadata={"title":"UK Parliament","url":"https://www.parliament.uk/about/"}),
    Document(page_content="The River Seine flows through Paris.", metadata={"title":"Encyclopaedia Britannica: Seine","url":"https://www.britannica.com/place/Seine-River-Europe"})
]

emb = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(docs, embedding=emb, collection_name="demo-citations")
retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 4, "fetch_k": 8})

# --- Helper: number sources deterministically for each question
def enumerate_sources(docs):
    """Return (rendered_context, index_map) where index_map maps [n] -> doc metadata."""
    lines = []
    index_map = {}
    for i, d in enumerate(docs, start=1):
        title = d.metadata.get("title","Untitled")
        url = d.metadata.get("url","")
        lines.append(f"[{i}] {title} — {url}")
        index_map[i] = d
    return "\n".join(lines), index_map

def format_context(docs):
    # Show short snippets to the model + references block it must cite
    refs_block, _ = enumerate_sources(docs)
    # Keep short chunks; in prod you might include highlighted spans
    snippets = "\n\n".join(f"• {d.page_content}" for d in docs)
    return f"SNIPPETS:\n{snippets}\n\nSOURCES:\n{refs_block}"

# --- Prompt that *requires* numbered citations
prompt = ChatPromptTemplate.from_messages(
    [
        ("system",
         "You are a careful assistant. Answer only using the provided SNIPPETS.\n"
         "For every factual statement, include citation numbers like [1], [2] tied to SOURCES below.\n"
         "If the answer is unknown from SNIPPETS, say: \"I don't know based on the provided sources.\""),
        ("human",
         "Question: {question}\n\n{context}\n\n"
         "Rules:\n"
         "1) Only cite numbers that appear in SOURCES.\n"
         "2) Do not invent sources or URLs.\n"
         "3) Prefer concise answers with citations inline.")
    ]
)

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
chain = prompt | llm | StrOutputParser()

# --- Parser guard: ensure at least one valid bracketed citation that exists in SOURCES
import re
def validate_citations_fn(inputs: dict, output_text: str):
    docs = inputs["docs"]
    _, index_map = enumerate_sources(docs)
    cited = set(int(x) for x in re.findall(r"\[(\d+)\]", output_text))
    valid_indices = set(index_map.keys())

    # If model said it doesn't know, allow it with no citations
    if "I don't know based on the provided sources" in output_text:
        return output_text

    if not cited:
        raise OutputParserException("No citations found.")
    if not cited.issubset(valid_indices):
        raise OutputParserException(f"Invalid citation ids: {sorted(cited - valid_indices)}")
    return output_text

validate_citations = RunnableLambda(lambda x: validate_citations_fn(x, x["raw"]))

# --- Compose: retrieve -> build context -> generate -> validate -> fallback
def build_inputs(question):
    retrieved = retriever.invoke(question)
    context = format_context(retrieved)
    return {"question": question, "docs": retrieved, "context": context}

def generate_with_retry(question, max_retries=2):
    inputs = build_inputs(question)
    # If no docs returned, short-circuit
    if not inputs["docs"]:
        return "I don't know based on the provided sources."

    last_err = None
    for _ in range(max_retries + 1):
        raw = chain.invoke(inputs)
        try:
            return validate_citations_fn(inputs, raw)
        except OutputParserException as e:
            last_err = e
            continue
    # Fallback if still invalid after retries
    return 'I don’t know based on the provided sources.'

# --- Try it
print("Q1) What is the capital of France?")
print(generate_with_retry("What is the capital of France?"), "\n")

print("Q2) When was the Eiffel Tower completed?")
print(generate_with_retry("When was the Eiffel Tower completed?"), "\n")

print("Q3) Who discovered penicillin?")
print(generate_with_retry("Who discovered penicillin?"), "\n")
```

### What you’ll see

* Q1: *“Paris \[1]. The River Seine … \[4]”* (citations from your SOURCES list)
* Q2: *“… completed in 1889 \[2].”*
* Q3: No supporting snippet → *“I don’t know based on the provided sources.”*

---

## Why this fixes hallucinations

1. **RAG** limits the model to retrieved context.
2. **Prompt contract** requires numbered citations mapped to the displayed SOURCES.
3. **Parser guard** rejects uncited/invalid answers → **retry** (often enough).
4. **Fallback** returns an honest “don’t know” instead of bluffing.

---

## Hardening (production tips)

* **Chunking & retriever tuning:** try `k=6`, use MMR, add domain filters.
* **Heavier validation:**

  * Require **N min citations** for non-trivial answers.
  * Require each citation to support **some overlapping keywords** from the sentence where it appears (simple heuristic).
* **Structured outputs:** Ask the model to return JSON with `answer` and `citations: [1,2]`, then render to text.
* **Answer-length budgets per claim:** split answer into claims; for each claim, verify a supporting source exists or drop the claim.
* **LangSmith**: turn tracing on and inspect failures; create eval datasets with expected citations and **automated checks**.

---

## Micro-evaluator (optional)

Add a tiny check that fails any answer with missing/invalid citations—great for CI:

```python
def citation_evaluator(question, answer, docs):
    import re
    _, index_map = enumerate_sources(docs)
    cited = set(int(x) for x in re.findall(r"\[(\d+)\]", answer))
    if "I don't know based on the provided sources" in answer:
        return True, "Abstained correctly"
    if not cited:
        return False, "No citations"
    if not cited.issubset(set(index_map.keys())):
        return False, "Invalid citation ids"
    return True, "OK"

# Example use during tests
q = "What is the capital of France?"
inputs = build_inputs(q)
ans = generate_with_retry(q)
ok, note = citation_evaluator(q, ans, inputs["docs"])
print(ok, note)
```

---

## Common debugging moves

* If the model still hallucinates: lower temperature to 0, shrink context, or increase retrieval `k`.
* If it cites the wrong source: show **shorter, more precise snippets** (or add keyword filtering).
* If it refuses to answer too often: increase corpus coverage; add synonyms to retriever; consider **hybrid search (BM25 + embeddings)**.

---

