# 12. Expert Level

## Custom Agents with Policies

When you reach the **expert level** of LangChain development, the focus shifts from building simple chains to **orchestrating intelligent agents**. These agents are not only capable of reasoning and acting but also follow **explicit policies** that define what they can and cannot do.

* **Why policies?**
  In production systems, unrestricted agents may generate unsafe, costly, or logically inconsistent actions. Policies act as guardrails.

* **Examples of agent policies:**

  * **Action constraints**: Limit API calls (e.g., max 5 per session).
  * **Domain rules**: Medical agent cannot provide final diagnoses, only triage + reference.
  * **Cost controls**: Stop queries if token use exceeds budget.
  * **Ethical filters**: Disallow certain data manipulations or unsafe prompts.

* **Implementation in LangChain:**

  * Define custom `AgentExecutor` with a **policy middleware**.
  * Use callbacks (`before_action`, `after_action`) to enforce rules.
  * Maintain logs for auditing compliance.

---

## Hybrid Search: Semantic + Keyword

Relying only on embeddings (semantic search) or raw keyword search is limiting. **Hybrid search** combines both to improve retrieval accuracy.

* **Semantic Search:** Captures meaning using vector similarity (e.g., "heart attack" ≈ "myocardial infarction").
* **Keyword Search:** Ensures precision for exact matches (e.g., `treatment=aspirin`).

**Hybrid strategies:**

1. **Weighted fusion:** Combine scores from BM25 (keyword) + cosine similarity (semantic).
2. **Two-step filter:** Keyword filter first → then rerank with embeddings.
3. **Parallel retrieval:** Query both systems, merge results with ranking heuristics.

* **LangChain integrations:**

  * Vector stores: Pinecone, Weaviate, FAISS.
  * Keyword indexes: ElasticSearch, OpenSearch.
  * Ready-made hybrid APIs: Vespa, Milvus hybrid.

---

## Long-term Memory: Vector DB + Episodic Recall

LLMs have **short-term context windows**, but production apps require **long-term knowledge**.
LangChain provides **memory modules** that can be backed by vector databases for persistence.

* **Types of memory:**

  * **Short-term:** Chat history within context window.
  * **Long-term episodic:** Important events, saved as embeddings.
  * **Declarative (semantic):** Facts and documents stored in a vector DB.

* **Design pattern:**

  1. Capture each session → embed key interactions.
  2. Store embeddings in vector DB (e.g., Chroma, Pinecone).
  3. On new queries → recall relevant memories → re-inject into context.

* **Use case:**
  A healthcare assistant remembers:

  * User’s prior conditions.
  * Medications prescribed in past sessions.
  * Episodic events (“last time, the patient had fever and cough”).

---

## Cost Optimization: Token Management + Caching

LLM-powered systems can become expensive if not managed well. **Expert-level LangChain apps** include cost controls.

* **Token Management:**

  * Use `tiktoken` to estimate token usage before sending prompts.
  * Trim unnecessary history/context.
  * Summarize older conversations for compression.

* **Caching strategies:**

  * **In-memory caching** for repeated queries in one session.
  * **Persistent caching** (Redis, SQLite, LangChain Cache API).
  * **Embedding cache** for repeated vector lookups.

* **Batching & Streaming:**

  * Batch queries to reduce overhead.
  * Stream partial results to improve UX without extra cost.

---

## Benchmarking: LangChain vs Direct API vs Alternatives

At expert level, you must know **when LangChain is worth the abstraction** and when direct API calls or alternative frameworks are better.

* **Direct API (OpenAI, Anthropic, Cohere):**

  * ✅ Lower latency, more control.
  * ❌ No built-in orchestration, memory, or retrieval.

* **LangChain:**

  * ✅ Rich ecosystem (agents, tools, retrievers, memory, evaluators).
  * ✅ Faster prototyping.
  * ❌ Overhead, more complexity than needed for simple apps.

* **Alternatives:**

  * **LlamaIndex (GPT Index):** Strong in data connectors and retrieval.
  * **Haystack:** Enterprise-grade retrieval and pipelines.
  * **DSPy / Guidance:** Fine-grained prompt programming.

* **Benchmarking dimensions:**

  * **Latency:** response time per request.
  * **Accuracy:** quality of retrieval & reasoning.
  * **Cost:** tokens consumed per session.
  * **Flexibility:** ease of integration with custom backends.

---

✅ **Summary for Chapter 12:**
At the expert level, you’re not just building apps—you’re optimizing **safety, recall, cost, and performance**. Custom agents follow **policies**, retrieval blends **semantic + keyword search**, memory is extended with **vector DBs**, costs are controlled via **token management & caching**, and you benchmark **LangChain vs direct APIs vs alternatives** to make informed production choices.

---


