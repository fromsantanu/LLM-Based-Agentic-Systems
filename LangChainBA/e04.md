Let‚Äôs build a **RAG chatbot** that answers questions from your **personal notes / company FAQs**. You‚Äôll get a minimal working version first, then optional upgrades (citations, better prompts, streaming, and a Streamlit UI).

---

# 0) What we‚Äôre building

* **Ingestion:** Load files (`.md`, `.txt`, `.pdf`, `.docx`, etc.), split into chunks, embed, and persist to a local vector DB (Chroma).
* **Chat:** History-aware retrieval + answer generation, with **source citations**.
* **Run:** Simple CLI; optional Streamlit app.

> Works great for a `/knowledge_base` folder containing your notes or FAQ docs.

---

# 1) Install & set up

```bash
pip install -U langchain langchain-openai langchain-community langchain-text-splitters langchain-chroma pypdf python-docx unstructured[local-inference]
```

Set your API key (or swap in a local embeddings/LLM later):

```bash
export OPENAI_API_KEY=sk-...
```

Project tree:

```
rag-notes/
  ingest.py
  chat_cli.py
  app_streamlit.py   # (optional)
  knowledge_base/    # put your .md/.txt/.pdf/.docx files here
  .env               # optional
```

---

# 2) Ingest: build the vector index (Chroma)

```python
# ingest.py
from pathlib import Path
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader, TextLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma

KB_DIR = "knowledge_base"
PERSIST_DIR = "chroma_notes"

def load_all_documents(kb_dir: str):
    kb_path = Path(kb_dir)

    # You can add more loaders (CSV, HTML, etc.) as needed.
    loaders = []
    # PDFs
    loaders.append(DirectoryLoader(kb_dir, glob="**/*.pdf", loader_cls=PyPDFLoader))
    # Markdown & Text
    loaders.append(DirectoryLoader(kb_dir, glob="**/*.md", loader_cls=TextLoader))
    loaders.append(DirectoryLoader(kb_dir, glob="**/*.txt", loader_cls=TextLoader))
    # Word
    loaders.append(DirectoryLoader(kb_dir, glob="**/*.docx", loader_cls=Docx2txtLoader))

    docs = []
    for loader in loaders:
        try:
            docs.extend(loader.load())
        except Exception as e:
            print("Loader error:", e)
    # Add simple metadata fallback
    for d in docs:
        d.metadata.setdefault("source", d.metadata.get("source", d.metadata.get("file_path", "unknown")))
    return docs

def main():
    print("Loading documents...")
    raw_docs = load_all_documents(KB_DIR)

    print(f"Loaded {len(raw_docs)} docs. Splitting...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = splitter.split_documents(raw_docs)

    print(f"Total chunks: {len(chunks)}. Embedding & writing to Chroma...")
    embeddings = OpenAIEmbeddings()  # swap to local embeddings if needed
    Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=PERSIST_DIR,
    )
    print(f"‚úÖ Ingestion complete. Chroma DB at: {PERSIST_DIR}")

if __name__ == "__main__":
    main()
```

Run it:

```bash
python ingest.py
```

---

# 3) Chat (CLI): history-aware retrieval + citations

This uses LangChain‚Äôs **history-aware retriever** + **stuff documents** chain for grounded answers. It also **returns sources**.

```python
# chat_cli.py
import os
from typing import List, Tuple
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

PERSIST_DIR = "chroma_notes"

def build_rag_chain():
    # 1) Vector store & retriever
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma(persist_directory=PERSIST_DIR, embedding_function=embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

    # 2) LLM
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    # 3) History-aware retriever: rewrites follow-ups into better search queries
    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system",
         "You are a helpful assistant re-writing the user's question to be a standalone query for retrieval. "
         "Use conversation history only to fill in missing context. Return only the rewritten query."),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}")
    ])
    history_aware_retriever = create_history_aware_retriever(
        llm=llm,
        retriever=retriever,
        prompt=contextualize_q_prompt,
    )

    # 4) Answer chain with sources (Stuff)
    answer_prompt = ChatPromptTemplate.from_messages([
        ("system",
         "You are a helpful domain assistant. Use the provided context snippets to answer faithfully. "
         "If the answer is not in the context, say you don't know. Cite sources as [source:<value>] "
         "from each document's metadata when useful."),
        MessagesPlaceholder("chat_history"),
        ("human", "Question: {input}\n\nContext:\n{context}"),
    ])

    document_chain = create_stuff_documents_chain(llm=llm, prompt=answer_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, document_chain)
    return rag_chain

def format_sources(context_docs) -> str:
    seen = []
    lines = []
    for d in context_docs:
        src = d.metadata.get("source") or "unknown"
        if src not in seen:
            seen.append(src)
            lines.append(f"- {src}")
    return "\n".join(lines) if lines else "No sources found."

def chat_loop():
    chain = build_rag_chain()
    chat_history: List[Tuple[str, str]] = []

    print("ü§ñ RAG chatbot ready. Ask about your notes/FAQs. Type 'exit' to quit.\n")
    while True:
        user_q = input("You: ").strip()
        if not user_q:
            continue
        if user_q.lower() in {"exit", "quit"}:
            break

        result = chain.invoke({"input": user_q, "chat_history": chat_history})
        answer = result["answer"]
        # retrieve supporting docs from the same call (key is 'context' on the return of create_retrieval_chain)
        context_docs = result.get("context", [])
        sources = format_sources(context_docs)

        print("\nAssistant:", answer)
        print("\nSources:\n", sources, "\n")

        chat_history.append((user_q, answer))

if __name__ == "__main__":
    chat_loop()
```

Run:

```bash
python chat_cli.py
```

**Try it:**

* ‚ÄúWhat is our refund policy?‚Äù
* ‚ÄúAnd how long after purchase can customers request it?‚Äù (follow-up uses chat history)

---

# 4) Optional: Streamlit UI (clean & quick)

```python
# app_streamlit.py
import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

PERSIST_DIR = "chroma_notes"

@st.cache_resource
def build_chain():
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma(persist_directory=PERSIST_DIR, embedding_function=embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", "Rewrite the human question as a standalone query using chat history."),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}")
    ])
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)

    answer_prompt = ChatPromptTemplate.from_messages([
        ("system", "Answer using the context; if unsure, say you don't know. Cite sources as [source:<value>]."),
        MessagesPlaceholder("chat_history"),
        ("human", "Question: {input}\n\nContext:\n{context}")
    ])
    doc_chain = create_stuff_documents_chain(llm, answer_prompt)
    rag = create_retrieval_chain(history_aware_retriever, doc_chain)
    return rag

def main():
    st.set_page_config("RAG Chatbot", layout="centered")
    st.title("üìö RAG Chatbot ‚Äî Notes / FAQ")

    if "history" not in st.session_state:
        st.session_state.history = []

    chain = build_chain()
    user_q = st.chat_input("Ask a question about your notes/FAQ")

    # display history
    for role, text in st.session_state.history:
        with st.chat_message("user" if role=="user" else "assistant"):
            st.markdown(text)

    if user_q:
        st.session_state.history.append(("user", user_q))
        result = chain.invoke({"input": user_q, "chat_history": st.session_state.history_pairs if False else st.session_state.history_to_pairs() if False else [(u,a) for (u,a) in zip(
            [m[1] for m in st.session_state.history if m[0]=="user"],
            [m[1] for m in st.session_state.history if m[0]=="assistant"])]})

        # The above line is messy in Streamlit; simpler approach:
        # Build pairs from the linear chat history:
        pairs = []
        last_user = None
        for role, text in st.session_state.history[:-1]:  # exclude current user entry
            if role == "user":
                last_user = text
            elif role == "assistant" and last_user is not None:
                pairs.append((last_user, text))
                last_user = None

        result = chain.invoke({"input": user_q, "chat_history": pairs})
        answer = result["answer"]
        context_docs = result.get("context", [])
        sources = sorted({d.metadata.get("source", "unknown") for d in context_docs})

        st.session_state.history.append(("assistant", answer))

        with st.chat_message("assistant"):
            st.markdown(answer)
            with st.expander("Sources"):
                for s in sources:
                    st.write(s)

# Helper to keep the example compact (optional):
# You could store chat in a structured way; Streamlit code above keeps it simple.

if __name__ == "__main__":
    main()
```

Run:

```bash
streamlit run app_streamlit.py
```

---

## 5) Tunings & upgrades (drop-in)

* **Citations inline:** Already included via `[source:<value>]` if you reference document metadata in your prompt. You can also render clickable links if your metadata includes URLs.
* **Smarter retrieval:**

  * `search_kwargs={"k": 6}` for broader recall
  * Add `score_threshold` to filter weak matches
  * Use **MMR**: `retriever.search_type="mmr", retriever.search_kwargs={"k":6, "fetch_k":20}`
* **Prompt hardening:** Add strict instructions: *‚ÄúIf the context lacks the answer, say ‚ÄòI don‚Äôt know based on the provided documents.‚Äô Do not invent.‚Äù*
* **Map-Reduce / Refine:** Replace the ‚Äústuff‚Äù combine step with `create_map_reduce_documents_chain` or `create_refine_documents_chain` for larger contexts.
* **Hybrid search:** Pair keyword + vector retrieval (e.g., Elasticsearch + Chroma), then merge results.
* **Local models:**

  * Embeddings: `HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")`
  * LLM: use OpenAI-compatible servers (Ollama, LM Studio) via `ChatOpenAI(base_url=..., api_key="fake")`
* **Privacy:** Keep your KB local; Chroma persists only on disk. Don‚Äôt send raw documents to the LLM‚Äîonly retrieved chunks.
* **Evaluation:** Keep a small `eval.jsonl` with pairs of questions & expected nuggets; measure hit-rate (did a supporting chunk get retrieved?) and exact match / semantic similarity of answers.

---

## 6) Quick test checklist

1. Put 2‚Äì10 files into `knowledge_base/` (e.g., `faq.md`, `refund_policy.txt`, `onboarding.pdf`).
2. `python ingest.py`
3. `python chat_cli.py` ‚Üí Ask:

   * ‚ÄúWhat‚Äôs our refund window?‚Äù
   * ‚ÄúAny exceptions for digital products?‚Äù
4. Verify the **Sources** list shows your filenames.

---

## 7) Common pitfalls

* **No results / irrelevant answers:** Increase `k`, improve chunking (`chunk_size=800‚Äì1200`, `overlap=100‚Äì200`), and ensure your loader extracts text (some PDFs are images; use OCR).
* **Hallucination:** Keep `temperature=0`, and **force** the model to answer only from context in the system prompt.
* **Token limits:** If answers truncate, switch to Map-Reduce or shorten chunks (`chunk_size ~ 700`) and reduce `k`.

---

