# 7. Advanced Chains

LangChain provides **Advanced Chains** for building complex workflows. Instead of simple input â†’ output flows, you can design branching, routing, or parallel execution of chains.

---

## ðŸ”¹ Sequential vs. Parallel Chains

* **Sequential Chains**: Output of one chain is the input of the next. Useful when steps must happen in order.
* **Parallel Chains**: Multiple chains run at the same time with the same input. Useful when tasks are independent and results are combined later.

---

## ðŸ”¹ Router Chains

Router Chains dynamically decide **which chain to send a query to**.

* Uses intent classification or prompt logic.
* Example: Route math questions to a calculator chain, and knowledge questions to a general LLM chain.

---

## ðŸ”¹ Transform Chains

Transform Chains let you **manipulate inputs or outputs**.

* Example: Preprocess user queries (lowercasing, translation, formatting).
* Example: Postprocess outputs (summarizing, filtering keywords).

---

# âœ… Example: Advanced Chain in Action

Weâ€™ll build an agent-like flow:

* If the question is math â†’ use a calculator.
* If the question is about news â†’ fetch a (dummy) news answer.
* Otherwise â†’ let GPT handle it.

```python
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, TransformChain, SimpleSequentialChain
from langchain.chains.router import MultiRouteChain, RouterChain
from langchain.schema import StrOutputParser

# 1. Base LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# 2. Calculator chain (dummy here, just LLM simulating math)
calc_prompt = PromptTemplate.from_template("You are a calculator. Solve: {question}")
calc_chain = LLMChain(llm=llm, prompt=calc_prompt)

# 3. News chain (dummy fetch)
news_prompt = PromptTemplate.from_template("You are a news assistant. Give a short update on: {question}")
news_chain = LLMChain(llm=llm, prompt=news_prompt)

# 4. General QA chain
qa_prompt = PromptTemplate.from_template("Answer the question clearly: {question}")
qa_chain = LLMChain(llm=llm, prompt=qa_prompt)

# 5. Router (decide intent)
router_prompt = PromptTemplate.from_template("""
Classify the user question into one of: [math, news, general].
Question: {question}
Answer with only the category name.
""")
router_chain = LLMChain(llm=llm, prompt=router_prompt, output_parser=StrOutputParser())

destinations = {
    "math": calc_chain,
    "news": news_chain,
    "general": qa_chain,
}

# Wrap into MultiRouteChain
advanced_chain = MultiRouteChain(
    router_chain=router_chain,
    destination_chains=destinations,
    default_chain=qa_chain
)

# 6. Run queries
queries = [
    "What is 15 * 12?",
    "What is the latest news on space exploration?",
    "Who wrote War and Peace?"
]

for q in queries:
    print(f"Q: {q}")
    print("A:", advanced_chain.invoke({"question": q})["output"])
    print("-" * 50)
```

---

## ðŸ”Ž What happens here?

1. User query â†’ goes to `router_chain` â†’ classified as **math / news / general**.
2. Depending on intent â†’ forwarded to correct chain.
3. Output returned to the user.

This shows **Router Chains + Sequential execution + Transform logic** in one flow.

---

