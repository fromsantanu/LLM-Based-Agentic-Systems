Let’s build a tiny **router** that sends math queries to a **calculator tool** and all other queries to an **LLM**. We’ll use modern LCEL primitives (`RunnableBranch`, `RunnableLambda`) so it’s clean and production-friendly.

---

# Example: Router → Calculator vs LLM

## What it does

1. **Classify** the incoming question as `math` or `general`.
2. If `math` → evaluate the expression with a safe Python calculator.
3. Otherwise → answer with the LLM.

## Code (drop-in script)

```python
# pip install -U langchain langchain-openai
# set OPENAI_API_KEY in your environment

from __future__ import annotations
import ast, operator as op
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnableBranch, RunnableParallel
from langchain_core.output_parsers import StrOutputParser

# --------- 1) Safe calculator (no eval) ----------
# Supports + - * / ** // % and parentheses
_ops = {
    ast.Add: op.add, ast.Sub: op.sub, ast.Mult: op.mul, ast.Div: op.truediv,
    ast.Pow: op.pow, ast.FloorDiv: op.floordiv, ast.Mod: op.mod, ast.USub: op.neg
}

def _eval_expr(node):
    if isinstance(node, ast.Num):         # py<3.8
        return node.n
    if isinstance(node, ast.Constant):    # py>=3.8
        if isinstance(node.value, (int, float)):
            return node.value
        raise ValueError("Only numbers allowed")
    if isinstance(node, ast.BinOp):
        return _ops[type(node.op)](_eval_expr(node.left), _eval_expr(node.right))
    if isinstance(node, ast.UnaryOp):
        return _ops[type(node.op)](_eval_expr(node.operand))
    if isinstance(node, ast.Expression):
        return _eval_expr(node.body)
    raise ValueError("Unsupported expression")

def safe_calculator(expression: str) -> str:
    try:
        tree = ast.parse(expression, mode="eval")
        return str(_eval_expr(tree))
    except Exception as e:
        return f"Calculator error: {e}"

calculator = RunnableLambda(lambda x: {"answer": safe_calculator(x["question"])})

# --------- 2) General LLM chain ----------
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
qa_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a concise helpful assistant."),
    ("user", "Answer briefly and clearly:\n{question}")
])
qa_chain = qa_prompt | llm | StrOutputParser()
qa_chain = RunnableLambda(lambda x: {"answer": qa_chain.invoke({"question": x["question"]})})

# --------- 3) Router (LLM-based) ----------
route_prompt = ChatPromptTemplate.from_messages([
    ("system",
     "Classify the user input as exactly one label: 'math' or 'general'. "
     "Return ONLY the label—no extra text."),
    ("user", "{question}")
])
router = (route_prompt | llm | StrOutputParser()).with_config(run_name="router")

# Combine original question with route in parallel
with_route = RunnableParallel(route=router, question=lambda x: x["question"])

# --------- 4) Branching logic ----------
router_branch = RunnableBranch(
    # If route == math -> calculator
    (lambda x: x["route"].strip().lower() == "math", calculator),
    # Else -> LLM
    qa_chain
)

# --------- 5) Full pipeline ----------
pipeline = with_route | router_branch

# --------- 6) Try it ----------
tests = [
    {"question": "What is (15*12) + 3**2 ?"},
    {"question": "Who wrote War and Peace?"},
    {"question": "Compute 144 // 12 + 7 % 3"}
]

for t in tests:
    result = pipeline.invoke(t)
    print(f"Q: {t['question']}\nA: {result['answer']}\n" + "-"*50)
```

## How it works (quick read)

* **`router`** LLM returns only `"math"` or `"general"`.
* **`RunnableParallel`** attaches that route to the original input.
* **`RunnableBranch`** sends the input:

  * to **`calculator`** (safe AST evaluator) when `route=="math"`,
  * else to the **LLM QA chain**.
* Everything returns a uniform `{"answer": ...}` payload, making downstream handling simple.

