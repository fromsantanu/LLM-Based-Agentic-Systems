let’s make this fully runnable and easy to adapt. Below is a complete, self-contained example that:

1. loads a PDF research paper,
2. splits it into chunks,
3. embeds those chunks (OpenAI **or** Hugging Face), and
4. stores them in a FAISS index (with save/load + search).

---

# Example: PDF → Chunks → Embeddings → FAISS

## 0) Install deps

```bash
pip install -U langchain langchain-community langchain-openai faiss-cpu pypdf tiktoken sentence-transformers
```

> If you plan to use OpenAI embeddings, set an API key:

```bash
export OPENAI_API_KEY="sk-..."        # macOS/Linux
# setx OPENAI_API_KEY "sk-..."        # Windows PowerShell
```

---

## 1) Python script (save as `pdf_to_faiss.py`)

```python
import os
from pathlib import Path

# --- LangChain core/community pieces ---
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS

# --- Embeddings (choose ONE block: OpenAI OR HuggingFace) ---
USE_OPENAI = bool(os.getenv("OPENAI_API_KEY"))  # auto-pick if key present

if USE_OPENAI:
    from langchain_openai import OpenAIEmbeddings
else:
    # local, no key needed
    from langchain_community.embeddings import HuggingFaceEmbeddings

# ---------- CONFIG ----------
PDF_PATH = "your_paper.pdf"          # <- replace with your PDF filename
INDEX_DIR = "faiss_index_pdf"        # directory to persist FAISS
CHUNK_SIZE = 1000                    # ~characters (not tokens)
CHUNK_OVERLAP = 150
TOP_K = 4
# ----------------------------

def load_pdf(pdf_path: str):
    """Load a PDF into LangChain Documents with page metadata."""
    if not Path(pdf_path).exists():
        raise FileNotFoundError(f"PDF not found at {pdf_path}")
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()  # one Document per page with metadata['page']
    return docs

def split_docs(docs):
    """Split into overlapping chunks while preserving useful metadata."""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,  # default works for most cases
        add_start_index=True,
    )
    return splitter.split_documents(docs)

def build_embeddings():
    """Return an embedding function (OpenAI or HF)."""
    if USE_OPENAI:
        # Recommended OpenAI embedding model; adjust if needed
        return OpenAIEmbeddings(model="text-embedding-3-large")
    else:
        # Great local default; small + fast
        return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

def create_or_load_faiss(chunks, embeddings, index_dir: str):
    """Create FAISS from chunks or load if already exists."""
    idx_path = Path(index_dir)
    if idx_path.exists():
        print(f"[i] Loading existing FAISS index from {index_dir}")
        return FAISS.load_local(index_dir, embeddings, allow_dangerous_deserialization=True)
    print(f"[i] Building FAISS index …")
    vs = FAISS.from_documents(chunks, embeddings)
    vs.save_local(index_dir)
    print(f"[i] Saved FAISS to {index_dir}")
    return vs

def demo_queries(vs):
    """Run a couple of searches to verify things work."""
    queries = [
        "What problem does this paper try to solve?",
        "Summarize the main methodology in 3 bullets.",
    ]
    for q in queries:
        print("\n=== Query ===")
        print(q)
        # Plain similarity search
        hits = vs.similarity_search(q, k=TOP_K)
        for i, d in enumerate(hits, 1):
            page = d.metadata.get("page", "N/A")
            print(f"\n[{i}] Page {page}")
            print(d.page_content[:400].replace("\n", " ") + ("..." if len(d.page_content) > 400 else ""))

        # With raw scores:
        print("\n--- With scores ---")
        hits_scores = vs.similarity_search_with_score(q, k=TOP_K)
        for i, (d, score) in enumerate(hits_scores, 1):
            page = d.metadata.get("page", "N/A")
            print(f"[{i}] score={score:.4f} | Page {page} | {d.page_content[:120].replace('\n', ' ')}...")

        # Optional: MMR (reduces redundancy)
        print("\n--- MMR (diverse) ---")
        diverse = vs.max_marginal_relevance_search(q, k=min(3, TOP_K), fetch_k=max(8, TOP_K))
        for i, d in enumerate(diverse, 1):
            page = d.metadata.get("page", "N/A")
            print(f"[{i}] Page {page} | {d.page_content[:160].replace('\n', ' ')}...")

def main():
    print(f"[i] Loading: {PDF_PATH}")
    docs = load_pdf(PDF_PATH)
    print(f"[i] Loaded {len(docs)} pages")

    print("[i] Splitting into chunks …")
    chunks = split_docs(docs)
    print(f"[i] Created {len(chunks)} chunks (size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP})")

    embeddings = build_embeddings()
    vs = create_or_load_faiss(chunks, embeddings, INDEX_DIR)

    demo_queries(vs)

if __name__ == "__main__":
    main()
```

---

## 2) Run it

```bash
python pdf_to_faiss.py
```

You’ll see:

* how many pages were loaded,
* how many chunks were created,
* top search results (with page numbers),
* results with scores, and an MMR demo.

---

## Notes & Tips

* **Choosing embeddings**

  * **OpenAI (`text-embedding-3-large`)**: best quality, needs API key.
  * **HuggingFace (`all-MiniLM-L6-v2`)**: free/local; good for testing and many use cases.

* **Persistence**

  * FAISS is saved to `faiss_index_pdf/`. Re-runs reuse it (fast!).
  * To rebuild, delete the folder or change `INDEX_DIR`.

* **Chunk sizing**

  * Start with `chunk_size=1000`, `chunk_overlap=150`.
  * If your paper has dense math/notation, try 700/120.

* **Metadata**

  * `PyPDFLoader` keeps `metadata['page']` which we print in results.
  * You can add your own fields (e.g., section titles) before indexing.

* **Troubleshooting**

  * If `faiss` install fails on Windows, ensure you used `faiss-cpu` and a recent Python.
  * Garbled PDF text? Try different loaders (e.g., `pdfminer.six`, `UnstructuredPDFLoader`) or OCR (e.g., `pytesseract`) depending on the source PDF.

---
