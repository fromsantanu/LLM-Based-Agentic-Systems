# 5. Memory

Large Language Models (LLMs) are inherently **stateless**—they do not remember previous interactions unless explicitly provided with conversation history. In many real-world applications (chatbots, assistants, diagnostic systems), we want our AI to **remember past context** and build on it.

LangChain introduces **Memory** to solve this problem. Memory enables conversations to retain past interactions, facts, and states, making applications more dynamic and human-like.

---

## Types of Memory

LangChain provides several types of memory, each suited for different use cases:

### 1. **ConversationBufferMemory**

* **How it works:** Stores all messages (user + AI) in a buffer (list).
* **Pros:** Retains full conversation history.
* **Cons:** Can get very long, leading to high token usage.
* **Use case:** Small-scale chatbots where the entire conversation context is useful.

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI()
memory = ConversationBufferMemory()
conversation = ConversationChain(llm=llm, memory=memory)

print(conversation.run("Hi, my name is Santanu."))
print(conversation.run("What is my name?"))
```

➡️ The model will correctly recall **“Santanu”** from the previous input.

---

### 2. **ConversationBufferWindowMemory**

* **How it works:** Stores only the **last `k` interactions** (sliding window).
* **Pros:** Saves tokens by limiting history.
* **Cons:** Loses older context.
* **Use case:** Customer support chats where only recent context matters.

```python
from langchain.memory import ConversationBufferWindowMemory

memory = ConversationBufferWindowMemory(k=2)  # keep last 2 exchanges
conversation = ConversationChain(llm=llm, memory=memory)

conversation.run("I live in Oxford.")
conversation.run("My favorite food is pasta.")
conversation.run("Do you remember where I live?")
```

➡️ The model will forget older messages once more than 2 turns pass.

---

### 3. **ConversationKGMemory**

* **How it works:** Uses a **Knowledge Graph (KG)** to store facts as triples (`subject`, `predicate`, `object`).
* **Pros:** Extracts structured knowledge, allowing reasoning over facts.
* **Cons:** More complex setup.
* **Use case:** Applications that require factual recall (e.g., medical assistants remembering conditions).

```python
from langchain.memory import ConversationKGMemory

memory = ConversationKGMemory(llm=llm)
conversation = ConversationChain(llm=llm, memory=memory)

conversation.run("Mimi is my daughter.")
conversation.run("Where is Mimi in relation to me?")
```

➡️ The KG memory captures `Mimi - is - daughter` and can reason about relationships.

---

## When to Use Memory

* **Use ConversationBufferMemory** if:
  You need **full recall** of all previous interactions.

* **Use ConversationBufferWindowMemory** if:
  Only the **last few turns** matter, and you want to reduce token costs.

* **Use ConversationKGMemory** if:
  You want the assistant to **remember and reason over facts** (knowledge-style memory).

---

## Persistent vs. Ephemeral Memory

* **Ephemeral Memory**: Lives only during the session. When the program restarts, all memory is lost.

  * Example: Chatbot session in a web app without saving history.

* **Persistent Memory**: Stored in a database or file system for long-term recall across sessions.

  * Example: A medical assistant remembering patient history over months.

---

## Storing Memory

LangChain supports multiple backends for persistence:

### 1. **Local File**

Save memory to JSON or text files.

```python
import json

# Save
with open("memory.json", "w") as f:
    json.dump(memory.load_memory_variables({}), f)

# Load
with open("memory.json", "r") as f:
    saved_memory = json.load(f)
```

---

### 2. **Database**

Use SQL/NoSQL databases (e.g., SQLite, PostgreSQL, MongoDB) to store session memory.

* Good for large-scale apps with many users.
* Ensures memory retrieval across multiple sessions/devices.

---

### 3. **Redis Integration**

Redis is a popular in-memory database with fast read/write.

* **Best suited for:** Scalable, real-time applications.
* LangChain provides `RedisChatMessageHistory`.

```python
from langchain.memory import ConversationBufferMemory
from langchain.memory.chat_message_histories import RedisChatMessageHistory

history = RedisChatMessageHistory(session_id="user123", url="redis://localhost:6379/0")
memory = ConversationBufferMemory(chat_memory=history, return_messages=True)
```

➡️ Here, all messages for `user123` are stored in Redis.

---

✅ **Summary**:
Memory in LangChain is what makes your agents feel **intelligent, continuous, and personalized**. Choose between buffer, window, or KG memory based on your use case, and decide whether you want it **persistent (file/db/Redis)** or **ephemeral**.

---
