Let’s build and ship a **LangChain-powered chatbot API** with **FastAPI** that streams tokens in real time. I’ll first explain the moving parts, then give you a complete, drop-in project (app code, requirements, Dockerfile, and test commands).

---

# What we’re building

* **/chat** (POST): accepts a user message and **streams** the model’s reply token-by-token.
* **Async** everywhere so we can handle many concurrent chats.
* **Simple caching** (optional toggle) to avoid paying for identical prompts repeatedly.
* **Basic rate-limit guard** with a semaphore.
* **/healthz** (GET): quick liveness probe for containers / load balancers.

---

# Project layout

```
langchain-chat-api/
├─ app.py
├─ requirements.txt
├─ Dockerfile
└─ .env.example
```

---

## app.py (FastAPI + LangChain + streaming)

```python
# app.py
import os
import asyncio
import hashlib
from typing import AsyncGenerator, Optional

from fastapi import FastAPI, HTTPException, Body
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel, Field

from langchain_openai import ChatOpenAI
from langchain.globals import set_llm_cache
from langchain.cache import InMemoryCache

# =========================
# Config & initialization
# =========================

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")  # set in environment or .env
if not OPENAI_API_KEY:
    raise RuntimeError("Missing OPENAI_API_KEY environment variable.")

# Toggle in-memory caching (avoid repeated LLM calls for identical prompts)
ENABLE_CACHE = os.getenv("ENABLE_CACHE", "true").lower() == "true"
if ENABLE_CACHE:
    set_llm_cache(InMemoryCache())

# Concurrency guard (simple rate limit)
MAX_CONCURRENT = int(os.getenv("MAX_CONCURRENT", "16"))
sema = asyncio.Semaphore(MAX_CONCURRENT)

# LLM: choose a small-but-fast default; override via env if you like
DEFAULT_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

# Important: streaming=True lets us consume tokens as they’re generated
llm = ChatOpenAI(model=DEFAULT_MODEL, streaming=True, temperature=0.2)

# FastAPI app
app = FastAPI(title="LangChain Chat API", version="1.0.0")

# CORS (adjust for your frontend origins)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # tighten in prod
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =========================
# Request/Response models
# =========================

class ChatRequest(BaseModel):
    message: str = Field(..., description="User message to the chatbot")
    system_prompt: Optional[str] = Field(
        default="You are a helpful assistant.",
        description="Optional system behavior instruction."
    )
    model: Optional[str] = Field(
        default=None,
        description="Override the default model."
    )
    stream_format: Optional[str] = Field(
        default="text", description="text | sse"
    )

class ChatPrefill(BaseModel):
    """Non-streaming JSON response for quick smoke tests."""
    reply: str

# =========================
# Utilities
# =========================

def cache_key(req: ChatRequest) -> str:
    sig = f"{req.model or DEFAULT_MODEL}||{req.system_prompt}||{req.message}"
    return hashlib.sha256(sig.encode("utf-8")).hexdigest()

# naive in-memory cache (okay for single-process containers)
CACHE: dict[str, str] = {}

async def stream_langchain(req: ChatRequest) -> AsyncGenerator[str, None]:
    """
    Token stream generator. If stream_format='sse', yields Server-Sent Events,
    otherwise yields plain text chunks suitable for chunked Transfer-Encoding.
    """
    model_name = req.model or DEFAULT_MODEL
    chat = llm if model_name == DEFAULT_MODEL else ChatOpenAI(model=model_name, streaming=True, temperature=0.2)

    # Compose the prompt as [system, human] messages
    prompt = [
        {"role": "system", "content": req.system_prompt or "You are a helpful assistant."},
        {"role": "user", "content": req.message},
    ]

    # astream() yields AIMessageChunk objects with `.content`
    if req.stream_format == "sse":
        # SSE format: "data: <chunk>\n\n"
        async for chunk in chat.astream(prompt):
            text = chunk.content or ""
            if text:
                yield f"data: {text}\n\n"
        # Signal completion for SSE clients
        yield "event: end\ndata: [DONE]\n\n"
    else:
        # Plain text chunks
        async for chunk in chat.astream(prompt):
            text = chunk.content or ""
            if text:
                yield text

# =========================
# Routes
# =========================

@app.get("/healthz")
async def healthz():
    return {"status": "ok"}

@app.post("/chat/stream")
async def chat_stream(req: ChatRequest = Body(...)):
    # Simple cache check (only when not using SSE, to keep example concise)
    if ENABLE_CACHE and req.stream_format != "sse":
        key = cache_key(req)
        if key in CACHE:
            return StreamingResponse(iter([CACHE[key]]), media_type="text/plain")

    # Concurrency guard
    if sema.locked():
        # Optional: you can still queue; here we just inform the client
        # Adjust to taste for your UX/SLOs
        pass

    await sema.acquire()
    try:
        # Build the async generator
        gen = stream_langchain(req)

        # Wire cache if not SSE (plain text). For SSE, caching token-by-token is noisy,
        # so we skip it in this simple example.
        if ENABLE_CACHE and req.stream_format != "sse":
            # We'll collect output as it streams, but still flush to client as we go.
            async def caching_wrapper():
                buf_parts = []
                async for part in gen:
                    buf_parts.append(part)
                    yield part
                # Save to cache after complete
                CACHE[cache_key(req)] = "".join(buf_parts)

            return StreamingResponse(caching_wrapper(), media_type="text/plain")

        # SSE or plain streaming without caching
        media = "text/event-stream" if req.stream_format == "sse" else "text/plain"
        return StreamingResponse(gen, media_type=media)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        sema.release()

@app.post("/chat")
async def chat_non_stream(req: ChatRequest = Body(...)) -> JSONResponse:
    """
    Convenience non-streaming endpoint that returns the full reply in JSON.
    Useful for quick checks and tools that don't handle streams.
    """
    # Cache first
    if ENABLE_CACHE:
        key = cache_key(req)
        if key in CACHE:
            return JSONResponse({"reply": CACHE[key]})

    model_name = req.model or DEFAULT_MODEL
    chat = llm if model_name == DEFAULT_MODEL else ChatOpenAI(model=model_name, streaming=False, temperature=0.2)

    prompt = [
        {"role": "system", "content": req.system_prompt or "You are a helpful assistant."},
        {"role": "user", "content": req.message},
    ]

    try:
        # ainvoke returns a Message; use .content
        result = await chat.ainvoke(prompt)
        text = result.content or ""
        if ENABLE_CACHE:
            CACHE[cache_key(req)] = text
        return JSONResponse({"reply": text})
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

## requirements.txt

```txt
fastapi==0.115.0
uvicorn[standard]==0.30.6
pydantic==2.9.2
langchain==0.3.3
langchain-openai==0.2.3
python-dotenv==1.0.1
```

> If you plan to use Redis or other persistent caches later, add their clients here.

---

## Dockerfile

```dockerfile
# Dockerfile
FROM python:3.11-slim

ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1

WORKDIR /app

# System deps (optional: add build tools if needed)
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl ca-certificates && \
    rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY app.py .
# .env is not copied—supply env vars at runtime

EXPOSE 8000
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

## .env.example

```bash
OPENAI_API_KEY=sk-xxxx
ENABLE_CACHE=true
MAX_CONCURRENT=16
OPENAI_MODEL=gpt-4o-mini
```

(Use a real key at runtime; don’t commit your actual secrets.)

---

# Run it locally

```bash
# 1) Set your key
export OPENAI_API_KEY="sk-..."

# 2) Install deps
pip install -r requirements.txt

# 3) Start the API
uvicorn app:app --reload --port 8000
```

Health check:

```bash
curl http://localhost:8000/healthz
```

### Stream (plain text chunks)

```bash
curl -N -X POST "http://localhost:8000/chat/stream" \
  -H "Content-Type: application/json" \
  -d '{"message":"Explain LangChain in two sentences."}'
```

### Stream (SSE)

```bash
curl -N -X POST "http://localhost:8000/chat/stream" \
  -H "Content-Type: application/json" \
  -d '{"message":"Give 3 bullet tips on RAG.","stream_format":"sse"}'
```

### Non-streaming JSON

```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{"message":"What is LangChain?"}'
```

---

# Containerize & run

```bash
docker build -t langchain-chat-api .
docker run -p 8000:8000 \
  -e OPENAI_API_KEY="sk-..." \
  -e ENABLE_CACHE=true \
  langchain-chat-api
```

---

## Notes & production tips

* **CORS**: Restrict `allow_origins` to your frontend domain(s).
* **Autoscaling**: Put behind a reverse proxy (NGINX) or a cloud LB; use multiple replicas.
* **Timeouts/Retries**: Add graceful retries around LLM calls if your platform is flaky.
* **Observability**: Add logging/tracing (LangSmith, OpenTelemetry) and request IDs.
* **Persistent cache**: Swap `InMemoryCache` for Redis when horizontally scaling.
* **Security**: Don’t expose your API key; use a backend-only deployment with secrets.
* **Backpressure**: Adjust `MAX_CONCURRENT` to your model/provider rate limits.

---

