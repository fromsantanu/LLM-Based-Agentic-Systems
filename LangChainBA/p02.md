# Chapter 2: Core Concepts

This chapter introduces the fundamental building blocks of LangChain. These are the concepts you will encounter repeatedly while designing workflows—whether for simple question answering or complex multi-agent medical triage systems.

---

## 1. LLMs and Chat Models

### What is an LLM?

A **Large Language Model (LLM)** is a text-in → text-out interface.

* You pass in a **string prompt**.
* The model generates a **string completion**.

Example:

```python
from langchain_openai import OpenAI

# Initialize a plain LLM
llm = OpenAI(model="gpt-3.5-turbo-instruct")

# Run a simple prompt
response = llm.invoke("Write a short poem about healthcare and technology.")
print(response)
```

---

### What is a ChatModel?

A **ChatModel** is designed for conversational workflows. Instead of a single string, it works with **structured messages** (system, user, assistant).

Example:

```python
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage

chat = ChatOpenAI(model="gpt-4")

messages = [
    SystemMessage(content="You are a helpful tutor."),
    HumanMessage(content="Explain the difference between a data lake and a data warehouse.")
]

response = chat.invoke(messages)
print(response.content)
```

---

### LLM vs ChatModel Differences

| Feature         | LLM                     | ChatModel                            |
| --------------- | ----------------------- | ------------------------------------ |
| Input format    | Plain string            | Structured messages (system/user/AI) |
| Output          | String                  | ChatMessage object                   |
| Best suited for | Single-shot completions | Conversations, multi-turn workflows  |

---

## 2. Handling System / User / Assistant Messages

* **SystemMessage** → defines behavior and role of the model (e.g., “You are a medical assistant”).
* **HumanMessage** → input from the user.
* **AIMessage (Assistant)** → output from the model.

Example:

```python
from langchain.schema import AIMessage

messages = [
    SystemMessage(content="You are an empathetic doctor."),
    HumanMessage(content="I have a headache and fever."),
    AIMessage(content="It sounds like you may have the flu.")
]
```

These message types let you simulate back-and-forth conversations and guide the model’s personality.

---

## 3. Prompt Templates

Prompt Templates allow you to define **reusable prompt structures** with placeholders for variables.

### Creating Reusable Prompt Templates

```python
from langchain.prompts import PromptTemplate

template = """You are a health advisor.
A patient presents with the following symptoms: {symptoms}.
Suggest possible causes in 2 sentences."""

prompt = PromptTemplate(
    input_variables=["symptoms"],
    template=template
)

# Render the prompt with inputs
print(prompt.format(symptoms="headache and dizziness"))
```

---

### Adding Input Variables

Templates can take multiple inputs:

```python
multi_template = """Summarize this text in {word_count} words:
{text}"""

prompt = PromptTemplate(
    input_variables=["word_count", "text"],
    template=multi_template
)

print(prompt.format(word_count=20, text="Artificial Intelligence is..."))
```

---

## 4. Chains

**Chains** connect multiple components (prompts, models, output parsers, etc.) into a pipeline.

### Sequential Chains

A **sequential chain** passes the output of one step as the input to the next.

#### Example 1: Simple Input → Output Flow

```python
from langchain_openai import OpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Step 1: Create a model
llm = OpenAI(model="gpt-3.5-turbo-instruct")

# Step 2: Define a template
prompt = PromptTemplate(
    input_variables=["topic"],
    template="Explain {topic} in simple terms for a beginner."
)

# Step 3: Build a chain
chain = LLMChain(llm=llm, prompt=prompt)

# Run the chain
result = chain.invoke({"topic": "DNA sequencing"})
print(result["text"])
```

---

#### Example 2: Multi-step Sequential Chain

```python
from langchain.chains import SimpleSequentialChain

# First chain: generate explanation
explain_prompt = PromptTemplate.from_template("Explain {topic} in simple terms.")
explain_chain = LLMChain(llm=llm, prompt=explain_prompt)

# Second chain: generate a quiz from the explanation
quiz_prompt = PromptTemplate.from_template("Make 3 quiz questions from this explanation:\n{explanation}")
quiz_chain = LLMChain(llm=llm, prompt=quiz_prompt)

# Combine them into a sequential chain
overall_chain = SimpleSequentialChain(chains=[explain_chain, quiz_chain])

response = overall_chain.invoke("Blockchain technology")
print(response)
```

---

✅ **Summary of Chapter 2**

* **LLMs** are plain text in/out.
* **ChatModels** handle structured multi-turn conversation.
* **Messages** (system, user, assistant) define interaction roles.
* **PromptTemplates** make prompts reusable with input variables.
* **Chains** link multiple steps into pipelines, from simple flows to complex multi-stage reasoning.

---
