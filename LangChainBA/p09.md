# 9. Evaluation & Debugging

When building applications with LangChain, evaluation and debugging are critical. Unlike traditional software, LLM-based workflows are probabilistic and can behave unpredictably. To ensure reliability, you must trace, log, and analyze model behavior systematically.

---

## ðŸ”¹ LangSmith (if available)

* **LangSmith** is LangChainâ€™s companion platform for debugging, testing, and monitoring LLM-powered applications.
* It provides:

  * **Traces**: visualize each step of your chain/agent execution.
  * **Evaluation**: test outputs against expected results.
  * **Monitoring**: analyze real-world usage and failure cases.

Example:

```python
from langchain_openai import ChatOpenAI
from langchain.smith import traceable

llm = ChatOpenAI()

@traceable
def ask_question(query: str):
    return llm.invoke(query)

response = ask_question("Explain quantum computing simply.")
```

This will automatically send traces to LangSmith (if configured).

---

## ðŸ”¹ Logging & Debugging Prompts

When things go wrong, inspect the **exact prompts** sent to the LLM:

* Capture system + user messages.
* Log temperature, max tokens, and stop sequences.
* Compare input/output variations.

```python
from langchain.globals import set_debug
set_debug(True)  # Logs all prompts and responses
```

This will print every LLM call with details.

---

## ðŸ”¹ Tracing Chain Execution

LangChain supports **verbose mode** for step-by-step execution tracking.

```python
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

template = "What is {x} + {y}?"
prompt = PromptTemplate.from_template(template)

chain = LLMChain(prompt=prompt, llm=llm, verbose=True)
print(chain.run({"x": 3, "y": 5}))
```

`verbose=True` shows each step so you can see where failures happen.

---

## ðŸ”¹ Prompt Engineering

Good debugging often means **better prompt design**:

* Make system instructions explicit.
* Use structured formatting (JSON, XML).
* Add **few-shot examples** for clarity.

### Few-Shot Examples

```python
prompt = PromptTemplate.from_template("""
Answer the math problem step by step.

Examples:
Q: 2 + 2
A: 4

Q: 3 * 5
A: 15

Now answer:
Q: {question}
A:
""")
```

---

## ðŸ”¹ Self-Consistency & CoT (Chain of Thought)

* **CoT (Chain of Thought):** Guide the model to reason step by step.
* **Self-Consistency:** Run multiple reasoning paths and choose the most common answer.

```python
from langchain.chains import LLMChain
import random

def self_consistent_answer(question, n=5):
    answers = []
    for _ in range(n):
        result = chain.run({"question": question})
        answers.append(result.strip())
    return max(set(answers), key=answers.count)

print(self_consistent_answer("What is 23 + 47?"))
```

---

## ðŸ”¹ Error Handling

LLM calls can fail due to:

* API rate limits
* Timeout errors
* Empty/invalid responses

You should **catch exceptions** and define **fallbacks**.

```python
from langchain.schema import OutputParserException

try:
    result = chain.run("Give answer in JSON only")
except OutputParserException:
    result = {"error": "Invalid format, please retry."}
```

---

## ðŸ”¹ Graceful Fallbacks

If an LLM call fails:

1. Retry with a simpler prompt.
2. Route to a backup model.
3. Return a default response.

```python
def safe_query(query):
    try:
        return llm.invoke(query)
    except Exception:
        return "Sorry, I could not process your request. Please try again."
```

---

# âœ… Example: Debugging a Math Assistant

Suppose you build an agent that solves math problems, but sometimes it fails.

```python
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

llm = ChatOpenAI(temperature=0)

prompt = PromptTemplate.from_template("""
Solve the math problem step by step, then give the final answer.

Q: {question}
A:
""")

chain = LLMChain(llm=llm, prompt=prompt, verbose=True)

def ask_math(question):
    try:
        return chain.run({"question": question})
    except Exception as e:
        return f"Error: {str(e)}"

print(ask_math("23 * 47"))
```

* **Verbose logging** shows the exact reasoning.
* If output format is wrong, add **few-shot examples**.
* If errors persist, add **retry/fallback logic**.

---

# ðŸ“Œ Key Takeaways

* **LangSmith** is the best debugging companion (if available).
* Always enable **logging & tracing** for visibility.
* **Prompt engineering** (few-shot + CoT) helps stabilize outputs.
* Implement **error handling & graceful fallbacks** for resilience.

---
