# Chapter 3: Document Handling

LangChain’s power really shines when working with documents — whether they’re PDFs, Word files, or even entire websites. In this chapter, we’ll cover how to load, split, embed, and store documents for efficient retrieval and downstream tasks like RAG (Retrieval-Augmented Generation).

---

## 3.1 Text Splitters

Large documents are often too big to send directly to an LLM. Instead, we break them into smaller pieces (“chunks”). This is important because:

* LLMs have input token limits.
* Smaller chunks help retrieval systems fetch the *most relevant* parts.
* Overlapping chunks preserve context across boundaries.

**Common Splitters**:

* `CharacterTextSplitter`: Splits text by character length.
* `RecursiveCharacterTextSplitter`: Smarter splitter that tries to break at paragraph → sentence → word levels.

**Example: Splitting Text**

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# A sample long text
text = """
LangChain is a framework for developing applications powered by language models...
(Imagine this is a big document)
"""

# Initialize a text splitter
splitter = RecursiveCharacterTextSplitter(
    chunk_size=100,    # max size of each chunk
    chunk_overlap=20   # overlap tokens between chunks
)

chunks = splitter.split_text(text)

print(f"Number of chunks: {len(chunks)}")
print(chunks[0])
```

---

## 3.2 Chunking Large Documents

* **Chunk Size**: The number of characters/tokens in one piece.
* **Overlap**: Extra content carried over from the previous chunk for context.

**Guideline**:

* For dense academic/technical text → smaller chunks (500–800 tokens).
* For simple narrative text → larger chunks (1000–1500 tokens).

---

## 3.3 Loaders

LangChain has a library of **Document Loaders** to read data from different formats. Each loader outputs `Document` objects with `page_content` and `metadata`.

**Examples**:

* **PDFs**

```python
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("sample.pdf")
docs = loader.load()
print(docs[0].page_content[:500])  # First 500 characters
```

* **Word Docs**

```python
from langchain.document_loaders import UnstructuredWordDocumentLoader

loader = UnstructuredWordDocumentLoader("sample.docx")
docs = loader.load()
```

* **Web Pages**

```python
from langchain.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://www.langchain.com/")
docs = loader.load()
```

* **CSV & JSON**

```python
from langchain.document_loaders import CSVLoader, JSONLoader

csv_loader = CSVLoader("data.csv")
csv_docs = csv_loader.load()

json_loader = JSONLoader("data.json", jq_schema=".[].content")
json_docs = json_loader.load()
```

---

## 3.4 Embeddings

Once documents are split, we convert them into **vector embeddings**. These numerical representations allow similarity search.

* **OpenAI Embeddings**: High-quality but requires API key.
* **HuggingFace Models**: Open-source, can run locally.

**Example: Generating Embeddings**

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.embeddings import HuggingFaceEmbeddings

# OpenAI
openai_embed = OpenAIEmbeddings()
vector = openai_embed.embed_query("Hello world")

# HuggingFace
hf_embed = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector = hf_embed.embed_query("Hello world")

print(len(vector))  # Dimension of embedding
```

---

## 3.5 Vector Stores

A **Vector Store** is a database optimized for similarity search. After embedding documents, we store them here.

**Popular Options**:

* **FAISS**: Lightweight, local, free.
* **Chroma**: Easy, supports persistence.
* **Pinecone**: Cloud-hosted, scalable.

**Example: FAISS**

```python
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.embeddings import OpenAIEmbeddings

# Load documents
loader = PyPDFLoader("sample.pdf")
docs = loader.load()

# Split
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
splits = splitter.split_documents(docs)

# Embedding
embeddings = OpenAIEmbeddings()

# Create FAISS index
db = FAISS.from_documents(splits, embeddings)

# Search
query = "What is LangChain?"
results = db.similarity_search(query, k=2)

for r in results:
    print(r.page_content[:200])
```

---

## Key Takeaways

1. **Text Splitters** handle token limits and preserve context.
2. **Loaders** make it easy to import documents from multiple formats.
3. **Embeddings** turn text into vectors for semantic understanding.
4. **Vector Stores** enable efficient retrieval for downstream tasks like RAG.

---

✅ By mastering document handling, you’re ready to move into **Retrieval-Augmented Generation (RAG)** pipelines, where LLMs can reference your documents to answer questions.

---
