# Chapter 4: Retrieval-Augmented Generation (RAG)

## 4.1 What is RAG?

Retrieval-Augmented Generation (RAG) is a technique that combines **retrieval** (fetching relevant context from external data sources) with **generation** (using an LLM to answer queries).

* Without RAG: The LLM only relies on its internal training.
* With RAG: The LLM can **ground its responses in up-to-date, domain-specific knowledge**.

This approach is widely used in chatbots, search assistants, medical AI, legal document analysis, and enterprise Q\&A systems.

---

## 4.2 Retrievers

Retrievers are components responsible for **fetching relevant documents or chunks** based on a query.

### **4.2.1 Basic Retrievers**

These are pre-built retrievers provided by LangChain that can fetch documents from a vector store or keyword-based search.
Examples:

* **VectorStoreRetriever**: Retrieves documents using similarity search in FAISS, Pinecone, Chroma, etc.
* **ElasticSearchRetriever**: Retrieves using ElasticSearch.

```python
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma

# Create embeddings and store
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(persist_directory="./chroma_db", embedding_function=embeddings)

# Use retriever
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
```

---

### **4.2.2 Custom Retrievers**

You can also build **custom retrievers** by subclassing `BaseRetriever`.
This is useful if you want to retrieve data from APIs, databases, or special search pipelines.

```python
from langchain.schema import Document
from langchain.retrievers import BaseRetriever

class CustomRetriever(BaseRetriever):
    def _get_relevant_documents(self, query):
        # Example: search from a local dictionary
        results = []
        if "Python" in query:
            results.append(Document(page_content="Python is a programming language"))
        return results
    
    async def _aget_relevant_documents(self, query):
        return self._get_relevant_documents(query)

retriever = CustomRetriever()
```

---

## 4.3 Retrieval Chains

Retrievers by themselves just return text chunks. To **combine retrievers with LLMs**, we use **Retrieval Chains**.

### **4.3.1 Stuff Method**

* Fetch documents and **stuff all of them into the prompt**.
* Works well for small contexts but may hit token limits.

```python
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4")
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff"
)
```

---

### **4.3.2 Map-Reduce Method**

* Each document is passed **individually** to the LLM (‚Äúmap‚Äù).
* The results are **summarized together** (‚Äúreduce‚Äù).
* Good for large document sets.

```python
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="map_reduce"
)
```

---

### **4.3.3 Refine Method**

* Start with one document ‚Üí LLM creates a draft.
* Each next document **refines** the draft.
* Works well for progressive reasoning.

```python
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="refine"
)
```

---

## 4.4 Conversational Retrieval

In real-world Q\&A systems, users often ask **follow-up questions**.
Conversational Retrieval maintains **chat history + retrieval** for contextual answers.

```python
from langchain.chains import ConversationalRetrievalChain

qa = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever
)

chat_history = []
query = "What is Python?"
result = qa({"question": query, "chat_history": chat_history})
chat_history.append((query, result["answer"]))

# Follow-up query
query2 = "Who created it?"
result2 = qa({"question": query2, "chat_history": chat_history})
print(result2["answer"])
```

This ensures the second query (‚ÄúWho created it?‚Äù) is **understood in relation to Python**.

---

## ‚úÖ Example: RAG Pipeline

**Task:** Load a PDF research paper, embed it, store in Chroma, and query it with conversational retrieval.

```python
from langchain_community.document_loaders import PyPDFLoader

# Step 1: Load PDF
loader = PyPDFLoader("research_paper.pdf")
documents = loader.load()

# Step 2: Split into chunks
from langchain.text_splitter import RecursiveCharacterTextSplitter
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
docs = splitter.split_documents(documents)

# Step 3: Create embeddings and vector store
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(docs, embeddings, persist_directory="./chroma_db")

# Step 4: Conversational Retrieval Chain
retriever = vectorstore.as_retriever()
qa = ConversationalRetrievalChain.from_llm(llm=ChatOpenAI(model="gpt-4"), retriever=retriever)

# Step 5: Ask Questions
chat_history = []
print(qa({"question": "What is the main research question?", "chat_history": chat_history}))
```

---

## üîë Key Takeaways

* **RAG** = Retrieval + Generation ‚Üí grounds LLMs with external knowledge.
* **Retrievers** fetch documents (basic or custom).
* **Retrieval Chains** (Stuff, Map-Reduce, Refine) balance completeness vs token usage.
* **Conversational Retrieval** allows multi-turn contextual Q\&A.

---

