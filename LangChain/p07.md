# **Chapter 7. Memory and Context Management**

Building intelligent agents is not only about responding to queries but also about **remembering past interactions** and **using context effectively**. LangChain provides multiple ways to handle memory, from simple buffer-based approaches to long-term vector database storage. In this chapter, we will explore the different types of memory and how they enable context-aware conversations.

---

## Types of Memory in LangChain

LangChain offers a variety of memory classes, each suited to different use cases:

1. **ConversationBufferMemory**

   * Stores the entire conversation history as a plain buffer.
   * Useful for short conversations where preserving full context is essential.
   * Downside: history grows indefinitely, which may exceed the LLM’s context window.

2. **ConversationBufferWindowMemory**

   * Keeps only the last *k* interactions.
   * Prevents overflow of long conversations.
   * Best for chatbots where only the recent context matters.

3. **ConversationSummaryMemory**

   * Summarizes past exchanges using an LLM.
   * Maintains long conversations without exceeding context limits.
   * Useful for customer support or tutoring agents.

4. **ConversationKGMemory (Knowledge Graph)**

   * Stores facts extracted from conversations as triples (subject–relation–object).
   * Allows reasoning over structured knowledge from dialogues.

5. **Custom Memory Implementations**

   * Developers can design memory that integrates with external databases, CRMs, or APIs for specific needs.

---

## Conversation Persistence Across Sessions

Most chatbots reset once a session ends. But for many applications—like healthcare, education, or personal assistants—users expect continuity.

* **Session persistence** can be achieved by storing memory objects in a database (e.g., PostgreSQL, MongoDB, or Redis).
* When a user returns, the stored memory is reloaded, giving the impression of a continuous dialogue.
* Example: a medical assistant remembers a patient’s previous symptoms even if they return after a week.

```python
from langchain.memory import ConversationBufferMemory
import pickle

# Save memory
memory = ConversationBufferMemory()
memory.save_context({"input": "I have a headache"}, {"output": "Take rest and drink water."})
with open("session_memory.pkl", "wb") as f:
    pickle.dump(memory, f)

# Reload memory
with open("session_memory.pkl", "rb") as f:
    loaded_memory = pickle.load(f)
print(loaded_memory.load_memory_variables({}))
```

---

## Long-Term Memory with Vector Databases

While short-term memory handles immediate context, **long-term memory** ensures agents recall information from older sessions.

* **Vector databases (Chroma, FAISS, Qdrant, Weaviate)** store embeddings of past conversations or documents.
* The system retrieves relevant memories using semantic similarity, rather than relying only on the most recent dialogue.
* This creates a knowledge-rich agent that can recall information months later.

**Workflow:**

1. Convert each user message into an embedding.
2. Store embeddings in a vector store with metadata (timestamp, user ID, session ID).
3. At query time, retrieve top-*k* similar embeddings to refresh memory.
4. Combine retrieved memory with the LLM’s prompt.

Example use case:

* A tutoring agent that remembers what topics a student studied weeks ago.
* A doctor’s assistant that recalls a patient’s full medical history across visits.

---

## Multi-Turn Dialogue with Context Awareness

**Context awareness** allows an agent to respond naturally over multiple turns without losing track of the conversation.

For instance:

* **User:** "I want to book a flight to London."
* **Agent:** "Sure, when do you plan to travel?"
* **User:** "Next Friday."
* **Agent:** "Got it, a flight to London next Friday. Do you prefer morning or evening?"

Here, the agent links “Next Friday” to the prior intent of booking a flight to London—this is **context chaining**.

LangChain enables multi-turn dialogue through:

* **Chained memory objects** (buffer + summary + vector retrieval).
* **Context injection** into prompts, so the model always sees the relevant conversation history.
* **Hybrid memory systems**: recent turns from buffer + older details from vector store.

This ensures conversations remain coherent even in long and complex interactions.

---

✅ **Key Takeaways**

* LangChain provides multiple memory types: buffer, window, summary, and knowledge-graph based.
* Persistence allows conversations to continue across sessions.
* Vector databases enable long-term memory with semantic search.
* Combining short- and long-term memory supports natural multi-turn dialogue with context awareness.

---

