# **Chapter 3. LangChain Core Concepts**

Before we can build complex agentic AI applications, we need a strong foundation in the **core building blocks of LangChain**. This chapter introduces the most important concepts: **chains, agents, tools, prompts, LLMs, embeddings, memory, vector stores, document loaders, and text splitters**. Together, these elements form the "Lego bricks" from which we assemble intelligent systems.

---

## 3.1 Chains vs Agents vs Tools

LangChain provides three fundamental abstractions to structure AI workflows:

### **Chains**

* A **Chain** is a *sequence of steps* executed in order, usually passing the output of one component to the next.
* Example: A chain that takes a question ‚Üí retrieves documents ‚Üí summarizes them ‚Üí returns an answer.
* Simple, predictable, and rule-based.

üëâ **Use when** the workflow is fixed and does not need decision-making.

---

### **Agents**

* An **Agent** is more flexible: it decides *what actions to take next* based on the user input and the context.
* Agents can:

  * Call **tools** (APIs, functions, databases)
  * Ask follow-up questions
  * Plan and reason dynamically
* Example: A medical assistant agent that decides whether to search medical records, run a drug interaction check, or summarize a guideline based on the user‚Äôs query.

üëâ **Use when** you need decision-making and adaptability.

---

### **Tools**

* A **Tool** is any external function or API that the agent can use.
* Examples:

  * Calculator for math
  * Google search API
  * Vector database retrieval
  * Custom Python functions

üëâ **Use when** your agent needs access to external capabilities.

---

‚öñÔ∏è **Comparison**

| Concept | Purpose                              | Example                                                   |
| ------- | ------------------------------------ | --------------------------------------------------------- |
| Chain   | Fixed sequence of steps              | Question ‚Üí Search ‚Üí Summarize                             |
| Agent   | Dynamic decision-making              | Customer support chatbot that decides whether to escalate |
| Tool    | External capability used by an agent | Weather API, Database query                               |

---

## 3.2 Prompts, LLMs, and Embeddings

### **Prompts**

* A **Prompt** is the input you give to an LLM (Large Language Model).
* Prompts can be:

  * **Static**: predefined templates (e.g., ‚ÄúTranslate this text to French: {text}‚Äù)
  * **Dynamic**: generated at runtime with user input and context
* Designing effective prompts = **Prompt Engineering**.

---

### **LLMs (Large Language Models)**

* The brain of LangChain.
* Popular choices: **OpenAI GPT models, Anthropic Claude, Cohere, Hugging Face models**.
* LangChain wraps these models into a standard interface.

---

### **Embeddings**

* Embeddings are **vector representations of text**.
* They allow:

  * Semantic search
  * Document similarity comparison
  * Retrieval-Augmented Generation (RAG)
* Example: "heart attack" and "cardiac arrest" will have similar embeddings.

---

## 3.3 Memory

Memory allows AI agents to **remember previous interactions**. Without memory, every prompt is stateless.

LangChain provides several memory classes:

1. **ConversationBufferMemory**

   * Stores the entire conversation history.
   * Good for small chats but can grow too large.

2. **ConversationBufferWindowMemory**

   * Stores only the *last N turns* of conversation.
   * Useful when you want short-term context without bloating memory.

3. **ConversationSummaryMemory**

   * Summarizes old interactions instead of storing them all.
   * Keeps conversation context manageable.

---

‚öñÔ∏è **Memory Types Comparison**

| Memory Type  | What it Stores     | Best For                    |
| ------------ | ------------------ | --------------------------- |
| Buffer       | Full history       | Small, casual conversations |
| BufferWindow | Last N messages    | Context-limited assistants  |
| Summary      | Summarized history | Long, ongoing conversations |

---

## 3.4 VectorStores

VectorStores are databases optimized for **storing and searching embeddings**. They enable **semantic retrieval**, which is crucial for RAG applications.

Popular options in LangChain:

1. **Chroma**

   * Open-source, lightweight, great for local projects.
   * Easy to set up with Python.

2. **FAISS**

   * Developed by Facebook AI.
   * Very fast similarity search.
   * Works offline, but lacks advanced features.

3. **Qdrant**

   * Open-source, production-ready.
   * Scalable, supports filters and metadata.
   * Can be self-hosted or cloud-hosted.

4. **Weaviate**

   * Cloud-native vector database.
   * Rich schema support, hybrid search (text + vector).
   * Integrates well with enterprise use cases.

---

‚öñÔ∏è **VectorStore Comparison**

| VectorStore | Strengths                     | Use Case                      |
| ----------- | ----------------------------- | ----------------------------- |
| Chroma      | Easy to use, local            | Personal projects, small apps |
| FAISS       | Fast, offline                 | High-speed similarity search  |
| Qdrant      | Scalable, flexible            | Production RAG pipelines      |
| Weaviate    | Schema-rich, enterprise-ready | Enterprise knowledge bases    |

---

## 3.5 Document Loaders and Text Splitters

### **Document Loaders**

* Used to **ingest data into LangChain**.
* Support many formats:

  * PDF, Word, CSV, HTML, Notion, Slack, Google Docs
* Example: `PyPDFLoader`, `CSVLoader`.

---

### **Text Splitters**

* Large documents must be **split into smaller chunks** before embedding.
* Splitting ensures:

  * Each chunk is within LLM token limits
  * Retrieval is precise
* Common strategies:

  * **CharacterTextSplitter** ‚Üí Split by character length
  * **RecursiveCharacterTextSplitter** ‚Üí Smarter splitting by paragraphs/sentences
  * **TokenTextSplitter** ‚Üí Split by model tokens

---

‚öñÔ∏è **When to Use Splitters**

| Splitter  | When to Use                           |
| --------- | ------------------------------------- |
| Character | Simple, uniform chunks                |
| Recursive | Mixed document types, structured text |
| Token     | Token-aware splitting for LLMs        |

---

## ‚úÖ Key Takeaways

* **Chains** = fixed workflows
* **Agents** = decision-makers that use **Tools**
* **Prompts, LLMs, and Embeddings** form the intelligence core
* **Memory** types balance history vs efficiency
* **VectorStores** like Chroma, FAISS, Qdrant, Weaviate power semantic search
* **Document Loaders and Text Splitters** prepare raw data for use

With these concepts, you now have the essential vocabulary of LangChain. Next, we‚Äôll see **how to implement them in Python** with hands-on code examples.

---
