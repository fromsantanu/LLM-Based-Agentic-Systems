# **Chapter 5. Working with Vector Databases**

Modern AI agents are only as powerful as their ability to **recall, reason, and reuse knowledge**. While LLMs excel at generating text, they are inherently limited by their **context window** (the maximum number of tokens they can “see” at once). To build scalable agentic systems, we need vector databases (vector stores) that extend memory far beyond these token constraints.

---

## Why Agents Need Memory Beyond Context Length

* **Context Window Limits**
  Even state-of-the-art LLMs like GPT-4, Claude, or Gemini have a finite context size (e.g., 8k, 32k, or 200k tokens). This restricts the volume of text they can directly process at one time.

* **Long-Term Knowledge Retention**
  Imagine a healthcare assistant agent managing patient histories over years. It’s unrealistic to re-feed the entire history into the LLM each time. Instead, relevant chunks should be retrieved on demand.

* **Efficient Search**
  Storing documents in raw text makes keyword search brittle. Semantic search (via embeddings in a vector database) allows retrieval of conceptually related text, even when the wording is different.

* **Pipeline Support**
  Agents often orchestrate multiple steps—question answering, reasoning, code execution—where past knowledge must be consistently available. Vector databases act as **external, persistent memory**.

---

## Setting up ChromaDB Locally with Python

[**ChromaDB**](https://docs.trychroma.com/) is an open-source, developer-friendly vector store. It works well for personal projects or as a local embedding store for RAG (Retrieval-Augmented Generation).

### Installation

```bash
pip install chromadb
```

### Basic Usage

```python
import chromadb

# 1. Start a client
client = chromadb.Client()

# 2. Create (or get) a collection
collection = client.create_collection("my_documents")

# 3. Add documents with embeddings
collection.add(
    documents=["AI agents need memory", "ChromaDB supports semantic search"],
    ids=["doc1", "doc2"]
)

# 4. Query for semantic similarity
results = collection.query(
    query_texts=["How do AI agents remember things?"],
    n_results=2
)

print(results)
```

* **documents** → the raw text.
* **ids** → unique identifiers for each entry.
* **query\_texts** → the question or input for which you want semantically similar documents.

By default, Chroma uses in-memory storage. For persistence, you can set a local directory:

```python
client = chromadb.PersistentClient(path="./chroma_storage")
```

---

## Using FAISS for Lightweight Embeddings

[**FAISS**](https://github.com/facebookresearch/faiss) (by Meta) is a fast library for similarity search. It’s ideal for local, lightweight projects with large embedding collections.

### Installation

```bash
pip install faiss-cpu
```

### Example

```python
import faiss
import numpy as np

# 1. Create dummy embeddings
dim = 128
index = faiss.IndexFlatL2(dim)
vectors = np.random.random((10, dim)).astype('float32')

# 2. Add vectors
index.add(vectors)

# 3. Query with a new vector
query = np.random.random((1, dim)).astype('float32')
distances, indices = index.search(query, k=3)

print("Nearest neighbors:", indices)
```

FAISS is not a database in itself—it’s a **similarity search engine**. You’ll often pair it with your own metadata store (SQLite, JSON, etc.) to manage documents.

---

## Qdrant & Weaviate: Scalable Vector Stores

When projects move beyond local setups, you need **production-ready vector databases**.

* **Qdrant**

  * Open-source, written in Rust.
  * Provides high-performance similarity search with filtering.
  * Has Docker images and cloud-hosted versions.
  * Integrates directly with LangChain.

  ```bash
  docker run -p 6333:6333 qdrant/qdrant
  ```

* **Weaviate**

  * A feature-rich, cloud-native vector database.
  * Supports hybrid search (keyword + vector).
  * Has modules for automatic embedding (e.g., with OpenAI, Cohere).
  * GraphQL-based API for queries.

These solutions are best suited when:

* You expect **millions of documents**.
* You need **distributed scaling**.
* You require **metadata-based filtering** alongside vector similarity.

---

## Building a Document Retrieval Pipeline

A common agentic AI pattern is **Retrieval-Augmented Generation (RAG)**:

1. **Load Documents**
   Use LangChain loaders (e.g., `PyPDFLoader`, `TextLoader`) to bring data into memory.

2. **Split into Chunks**
   Use `RecursiveCharacterTextSplitter` to break text into 500–1000 token chunks.

3. **Embed**
   Use an embedding model (e.g., OpenAI’s `text-embedding-ada-002` or local alternatives).

4. **Store in Vector DB**
   Insert chunks into Chroma, FAISS, Qdrant, or Weaviate.

5. **Query on Demand**
   For each user query, embed the query → perform similarity search → retrieve relevant chunks.

6. **Feed into LLM**
   Combine retrieved text with the user’s question in a prompt template and pass to the LLM.

### Example with LangChain

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import TextLoader

# 1. Load and split docs
loader = TextLoader("example.txt")
documents = loader.load()
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = splitter.split_documents(documents)

# 2. Embed + store
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(docs, embeddings, persist_directory="./chroma_db")

# 3. Query
query = "What are the benefits of vector databases?"
results = vectorstore.similarity_search(query, k=2)

for r in results:
    print(r.page_content)
```

This forms the **retrieval backbone** for an agent—extending its memory, grounding it in domain-specific knowledge, and keeping responses relevant.

---

✅ **Key Takeaways**

* LLMs alone cannot remember everything; vector databases provide scalable memory.
* ChromaDB is best for local development; FAISS for lightweight similarity search.
* Qdrant and Weaviate shine in production with large-scale needs.
* A RAG pipeline connects loading → splitting → embedding → storing → retrieving → LLM response.

---

