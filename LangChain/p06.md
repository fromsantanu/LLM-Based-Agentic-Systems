# **Chapter 6. Retrieval-Augmented Generation (RAG)**

## Introduction to RAG

Large Language Models (LLMs) are powerful, but they have two major limitations:

1. **Knowledge cutoff** – they can’t access information beyond their training date.
2. **Hallucinations** – they may generate plausible but incorrect information.

**Retrieval-Augmented Generation (RAG)** addresses these issues by combining **external knowledge retrieval** with LLM reasoning. Instead of relying only on the model’s internal weights, RAG pipelines **fetch relevant documents** from a knowledge base (e.g., vector database, PDF library, website corpus) and pass them to the LLM to ground its answers in reality.

**Workflow of RAG**:

1. User provides a query.
2. The system **retrieves relevant documents** using semantic search.
3. Retrieved documents are combined with the query into a **prompt**.
4. The LLM generates a grounded answer.

RAG is especially useful in:

* Question answering over private data (company docs, medical guidelines, legal contracts).
* Conversational assistants with domain knowledge.
* Academic research tools.
* Chatbots that need to stay factually correct.

---

## Designing Retrieval Pipelines in LangChain

LangChain provides building blocks for RAG pipelines. A typical RAG pipeline has:

1. **Document Loaders** – import text from PDFs, Word, CSVs, websites.

   ```python
   from langchain_community.document_loaders import PyPDFLoader
   loader = PyPDFLoader("policy.pdf")
   documents = loader.load()
   ```

2. **Text Splitters** – break documents into manageable chunks.

   ```python
   from langchain.text_splitter import RecursiveCharacterTextSplitter
   splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
   docs = splitter.split_documents(documents)
   ```

3. **VectorStores** – store embeddings for efficient similarity search.

   ```python
   from langchain_openai import OpenAIEmbeddings
   from langchain_chroma import Chroma

   embeddings = OpenAIEmbeddings()
   vectordb = Chroma.from_documents(docs, embedding=embeddings, persist_directory="./chroma_store")
   ```

4. **Retrievers** – interface for searching relevant chunks.

   ```python
   retriever = vectordb.as_retriever(search_type="similarity", search_kwargs={"k":3})
   ```

5. **RAG Chain** – combine retriever with an LLM for grounded responses.

   ```python
   from langchain.chains import RetrievalQA
   from langchain_openai import ChatOpenAI

   llm = ChatOpenAI(model="gpt-4o-mini")
   qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
   ```

6. **User Query** – ask a question over your documents.

   ```python
   result = qa_chain.run("What are the key points of the policy?")
   print(result)
   ```

---

## Using Embeddings for Semantic Search

RAG relies on **embeddings** – numerical vector representations of text.

* **Why embeddings?**
  Unlike keyword search, embeddings allow **semantic similarity**:

  * Query: *“How to register a company?”*
  * Retrieved text: *“Steps for business incorporation”*

* **Popular Embedding Models**:

  * `text-embedding-3-small` and `text-embedding-3-large` (OpenAI).
  * `all-MiniLM-L6-v2` (SentenceTransformers).
  * Domain-specific embeddings (e.g., BioBERT for biomedical text).

LangChain lets you swap embedding models easily.

```python
from langchain_openai import OpenAIEmbeddings
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
```

You can also fine-tune or choose domain-specific embeddings if accuracy is critical (e.g., healthcare, finance, law).

---

## Implementing Conversational RAG Agents

A **Conversational RAG Agent** extends RAG with **memory and tool use**. It doesn’t just answer standalone queries but also:

* Maintains dialogue context.
* Uses retrieval only when needed.
* Routes queries intelligently.

Example:

```python
from langchain.agents import initialize_agent, Tool
from langchain.chains import ConversationalRetrievalChain

retriever = vectordb.as_retriever()
qa_chain = ConversationalRetrievalChain.from_llm(llm, retriever=retriever)

tools = [
    Tool(
        name="Policy QA System",
        func=qa_chain.run,
        description="Answers questions about policy documents"
    )
]

agent = initialize_agent(tools, llm, agent="chat-conversational-react-description", verbose=True)

# Conversation loop
response = agent.run("Summarize the policy on tax benefits.")
print(response)

response = agent.run("And what exemptions are given to small businesses?")
print(response)
```

Here, the agent **remembers previous context**, so the second query (“what exemptions…”) is linked to the first query about tax benefits.

---

## Evaluating Retrieval Quality

A RAG system is only as good as its retrieval. Evaluating quality ensures users get **accurate, relevant, and complete** answers.

**Key metrics for evaluation**:

1. **Precision** – proportion of retrieved documents that are relevant.
2. **Recall** – proportion of relevant documents that were retrieved.
3. **F1-score** – balance between precision and recall.
4. **Faithfulness** – whether the LLM sticks to retrieved facts or hallucinates.
5. **Contextual relevance** – how well the retrieved context supports the user’s question.

**Evaluation approaches**:

* **Ground-truth evaluation**: Compare retrieved answers with a gold standard dataset.
* **LLM-as-a-judge**: Use a separate LLM to rate relevance/faithfulness.
* **Human evaluation**: Experts validate correctness in high-stakes domains (e.g., healthcare, law).

LangChain includes evaluation utilities like `langchain.evaluation` to streamline testing.

```python
from langchain.evaluation import load_evaluator

evaluator = load_evaluator("context_relevance", llm=llm)
score = evaluator.evaluate_strings(prediction="Tax benefits include ...", input="What are the tax benefits?", reference="Tax benefits include income exemptions...")
print(score)
```

---

✅ **Summary**:

* RAG grounds LLMs with external knowledge.
* LangChain provides loaders, splitters, vector stores, retrievers, and chains to build RAG pipelines.
* Embeddings enable semantic search beyond keywords.
* Conversational RAG agents maintain memory for multi-turn dialogue.
* Evaluation ensures high-quality retrieval and reliable answers.

---

