# **Chapter 12. Deployment and Scaling**

Designing and prototyping an agent in a Jupyter notebook is only the beginning. To make your **Agentic AI app** useful to others, you need to **deploy it** in a stable environment, give it an interface, and ensure it can **scale** under load. This chapter walks through common deployment strategies, UI options, containerization, cloud hosting, and monitoring practices.

---

## Deploying LangChain Apps with FastAPI / Flask

LangChain applications are Python-first, which makes them easy to deploy with **web frameworks** like **FastAPI** or **Flask**. These frameworks expose your agent as an API endpoint that clients can call.

### Example: FastAPI Deployment

```python
from fastapi import FastAPI
from pydantic import BaseModel
from langchain.chains import LLMChain
from langchain_openai import ChatOpenAI

app = FastAPI()

# Define request schema
class Query(BaseModel):
    question: str

# Simple LLM chain
llm = ChatOpenAI(model="gpt-4o-mini")
chain = LLMChain.from_string(llm, "Answer the following question: {question}")

@app.post("/ask")
async def ask(query: Query):
    result = chain.run(query.question)
    return {"answer": result}
```

* Run with:

  ```bash
  uvicorn main:app --reload --host 0.0.0.0 --port 8000
  ```
* Clients (web apps, mobile apps, other services) can now POST requests to `/ask`.

### Flask Alternative

Flask follows a similar pattern:

```python
from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route("/ask", methods=["POST"])
def ask():
    question = request.json["question"]
    answer = chain.run(question)
    return jsonify({"answer": answer})
```

**FastAPI is recommended** for async support and automatic Swagger UI docs.

---

## Streamlit for Interactive UI

For **non-technical users** or demos, [Streamlit](https://streamlit.io/) provides an easy way to create a **web-based interactive UI**.

### Example: Streamlit Chatbot

```python
import streamlit as st
from langchain.chains import ConversationChain
from langchain_openai import ChatOpenAI

st.title("Healthcare Assistant Agent")

if "history" not in st.session_state:
    st.session_state.history = []

llm = ChatOpenAI(model="gpt-4o-mini")
chain = ConversationChain(llm=llm)

user_input = st.text_input("You: ")

if st.button("Ask"):
    response = chain.run(user_input)
    st.session_state.history.append(("You", user_input))
    st.session_state.history.append(("Bot", response))

for role, text in st.session_state.history:
    st.markdown(f"**{role}:** {text}")
```

Run locally:

```bash
streamlit run app.py
```

Streamlit is great for:

* Prototyping quickly
* Internal tools
* Dashboards for analytics + agent results

---

## Dockerizing Your Agentic AI App

**Why Docker?**

* Consistent environment (no “works on my machine” problems)
* Easy to ship to servers or cloud
* Supports scaling with Kubernetes or Docker Compose

### Example: Dockerfile

```dockerfile
# Base image
FROM python:3.11-slim

# Set working directory
WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy app code
COPY . .

# Expose port
EXPOSE 8000

# Run FastAPI app
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

Build and run:

```bash
docker build -t agentic-ai .
docker run -p 8000:8000 agentic-ai
```

Now your agent runs in an isolated container, ready for deployment.

---

## Running on Cloud (AWS, GCP, Azure, Hostinger VPS)

### 1. **AWS**

* Use **ECS** or **EKS (Kubernetes)** for container orchestration
* Use **S3** for storing documents, **DynamoDB** for metadata
* Deploy with **Elastic Beanstalk** for simplicity

### 2. **GCP**

* Use **Cloud Run** (serverless containers) for auto-scaling
* Use **Vertex AI** if you want to integrate with Google’s models
* BigQuery for data analysis integration

### 3. **Azure**

* Azure Container Apps (serverless containers)
* Azure OpenAI Service if you want enterprise-level GPT access
* Azure Monitor for observability

### 4. **Hostinger VPS**

* Budget-friendly VPS option
* Install Docker + Nginx reverse proxy
* Deploy FastAPI/Streamlit apps for small-to-medium projects
* Great for startups and individual projects

> Tip: For small-scale apps (10–20 users), a **VPS (Hostinger, DigitalOcean, Linode)** is cost-effective. For enterprise-scale, consider **AWS/GCP/Azure** with auto-scaling.

---

## Monitoring & Logging Agent Behavior

An agent is dynamic—it makes decisions, calls tools, and interacts with external APIs. Monitoring is crucial.

### Basic Logging

```python
import logging
logging.basicConfig(level=logging.INFO)

logger = logging.getLogger(__name__)
logger.info("User asked a question")
logger.error("Tool execution failed")
```

### Advanced Monitoring Options

* **LangSmith**: LangChain’s observability platform (traces, debugging)
* **Prometheus + Grafana**: For metrics (latency, memory, API calls)
* **Sentry**: Error tracking & alerts
* **Elasticsearch + Kibana**: Centralized log analysis
* **Custom Dashboards**: Monitor agent tool usage, costs, and performance

### What to Monitor?

* API response times
* Token usage (OpenAI costs)
* Tool errors / retries
* User session data
* Latency bottlenecks (e.g., embedding vs LLM calls)

---

## Key Takeaways

* **FastAPI/Flask** make your agent accessible as an API.
* **Streamlit** is perfect for quick interactive UIs.
* **Docker** ensures consistency and portability.
* **Cloud platforms** (AWS, GCP, Azure, Hostinger VPS) give you options depending on scale and budget.
* **Monitoring/logging** ensures your agent stays reliable and cost-efficient.

---

✅ With this, you’re ready to take your **Agentic AI project** from prototype → production → scale.

---
