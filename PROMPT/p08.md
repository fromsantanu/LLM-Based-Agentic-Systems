# 8. **Memory and Context in Prompts**

Large Language Models (LLMs) excel at generating text, but they lack human-like memory. By default, they only “remember” the text inside their active **context window**. To build more natural and useful applications, prompt engineers must design strategies for handling **memory and context** across multiple turns and long sessions.

---

## Conversation Continuation and Context Reuse

In real-world applications, conversations rarely happen in a single turn. Users expect the AI to “remember” past exchanges.
Two primary approaches are used:

* **Inline context reuse**: Reinsert relevant parts of past conversation into each new prompt.
  Example: When building a chatbot, the system message might include a summary of earlier exchanges.

* **Session identifiers**: Store prior interactions under a session ID, and retrieve relevant history for reuse later.

This allows continuity, like remembering a user’s preference (“Call me Santanu”) or ongoing tasks (“Continue from step 3 of yesterday’s lesson”).

---

## Summarization Prompts for Long Sessions

When conversations grow beyond the model’s context window, **summarization** becomes essential.

* **Rolling summaries**: Condense each dialogue segment into short summaries and pass them back into prompts.
* **Hierarchical summaries**: Create layered summaries — detailed notes for short-term use, and higher-level abstracts for long-term context.
* **Thematic summaries**: Focus summaries on specific aspects, such as “Summarize the user’s health goals” or “Summarize coding requests.”

This prevents memory overload while keeping key details available.

---

## Techniques for Multi-Turn Dialogue Consistency

Consistency is crucial in multi-turn interactions, especially in tutoring, healthcare, or role-playing scenarios. Useful techniques include:

* **Role reinforcement**: Reiterate the role in each prompt (e.g., “You are acting as a medical assistant…”).
* **Anchor facts**: Explicitly restate known truths to prevent drift (e.g., “The user’s daughter is pursuing her PhD in AI.”).
* **State tracking**: Maintain structured state (like a JSON object of user preferences) and re-inject it as needed.
* **Error correction loops**: If the model forgets or contradicts earlier context, prompt it to re-check summaries or regenerate.

---

## Integrating Vector Databases for Long-Term Memory

For applications that require long-term memory across sessions (e.g., personal assistants, research bots), **vector databases** play a key role.

* **Embedding storage**: Convert conversation chunks into vector embeddings and store them.
* **Semantic retrieval**: On a new query, retrieve the most relevant past embeddings using similarity search.
* **Hybrid approaches**: Combine keyword search (for exact facts like dates) with semantic search (for meaning).

Popular tools include **ChromaDB, Pinecone, Weaviate, Qdrant**, and vector search features in relational databases.

This architecture enables the assistant to recall facts from weeks or months earlier, building a more personalized and human-like memory.

---

✅ **Key Takeaway**:
Effective memory and context handling is the backbone of natural, consistent, and scalable AI systems. Whether through simple summarization, structured state, or vector database integration, these strategies ensure that prompts stay grounded in prior knowledge, making interactions feel coherent and intelligent over time.

---

