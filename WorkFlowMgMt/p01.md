# **Lesson 1: Understanding Workflow APIs**

### **1. What are Workflow APIs?**

Workflow APIs are special types of APIs designed to **handle multi-step or long-running processes** that happen behind the scenes.
Instead of doing all the heavy work during a single request, a workflow API breaks the task into parts, assigns it to background workers, and lets the user check progress or get results later.

**Example:**
A doctor uploads patient data ‚Üí the API triggers an LLM-based analysis (takes 1‚Äì2 minutes) ‚Üí the workflow API immediately returns a ‚ÄúJob Submitted‚Äù message ‚Üí the background worker completes the task ‚Üí the doctor later checks the ‚ÄúJob Status‚Äù or retrieves the ‚ÄúFinal Report‚Äù.

So, Workflow APIs:

* Accept jobs through an HTTP POST request.
* Store them temporarily in a **queue**.
* A **worker process** runs the actual job in the background.
* The client (like Streamlit or frontend app) **polls** or **waits** for the result.

---

### **2. Synchronous vs. Asynchronous Processing**

Let‚Äôs break this down simply:

| Type             | Description                                                                                                             | Example                                                            | Pros                                      | Cons                                 |
| ---------------- | ----------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------ | ----------------------------------------- | ------------------------------------ |
| **Synchronous**  | The client waits for the server to finish the task before getting a response.                                           | Waiting for a long report to be generated after clicking ‚ÄúSubmit‚Äù. | Simple to code and debug.                 | Bad for long tasks; causes timeouts. |
| **Asynchronous** | The task runs in the background. The client gets a quick response like ‚ÄúJob Submitted‚Äù and checks later for the result. | Uploading a file ‚Üí getting a job ID ‚Üí checking progress later.     | Handles long tasks smoothly, no timeouts. | Needs extra setup (queue + worker).  |

In workflow APIs, **asynchronous processing** is preferred because tasks like:

* LLM query chains (e.g., LangChain)
* Large file uploads
* Complex medical data analysis
  take longer to complete.

---

### **3. Why Use Background Workers for LLM or Data-Heavy Tasks**

When your API runs an **LLM chain** (for diagnosis, summarization, or data cleaning), the process can take a long time ‚Äî sometimes 30 seconds to several minutes.

If you keep the process inside FastAPI‚Äôs request cycle:

* The client may **timeout**.
* The server may **crash or block** other requests.
* The overall experience becomes **slow**.

To avoid this, we **offload** the task to a **background worker**, which runs independently.

So the process becomes:

1. **POST Request** ‚Üí Task received ‚Üí Generate **Job ID**
2. **Redis Queue** ‚Üí Job added
3. **RQ or ARQ Worker** ‚Üí Picks and executes the task in background
4. **GET /status** ‚Üí Client polls for status
5. **GET /result** ‚Üí Retrieve final data/output

This pattern is scalable, reliable, and perfect for AI/ML workloads.

---

### **4. Overview of Redis, RQ, and ARQ**

#### **Redis**

* A super-fast, in-memory database.
* Acts as a **message broker** (like a mailroom between API and worker).
* Used for queues, caching, and job tracking.
* Example: Store job data like `{ job_id: "123", status: "submitted" }`.

#### **RQ (Redis Queue)**

* A **simple task queue** built on Redis.
* Lets you define jobs as Python functions.
* You start **RQ Workers** that keep listening for jobs in Redis.
* Great for **synchronous functions** (regular `def` functions).

**Example:**

```python
from rq import Queue
from redis import Redis
from my_tasks import process_data

redis_conn = Redis()
queue = Queue(connection=redis_conn)
job = queue.enqueue(process_data, "input.json")
```

#### **ARQ (Async Redis Queue)**

* A **modern alternative to RQ** built for **async** Python (using `async def`).
* Integrates very well with **FastAPI**, which is async-based.
* Perfect when your code uses async I/O (like LLM calls, APIs, or DB queries).

**Example:**

```python
from arq import create_pool
from arq.connections import RedisSettings

async def main():
    redis = await create_pool(RedisSettings())
    await redis.enqueue_job('process_data', 'input.json')
```

---

### **5. How They Fit into FastAPI**

FastAPI + Workflow API = High-performance architecture:

| Component                           | Role                                                                        |
| ----------------------------------- | --------------------------------------------------------------------------- |
| **FastAPI App**                     | Accepts job requests, returns job IDs, exposes status/result routes.        |
| **Redis**                           | Acts as the job queue and stores status updates.                            |
| **RQ/ARQ Worker**                   | Executes long-running or AI-related jobs.                                   |
| **Frontend (Streamlit or Web App)** | Submits jobs and periodically checks their status using `/status/<job_id>`. |

---

### **üß© In Short**

Workflow APIs help you:

* Prevent timeouts.
* Scale AI or data tasks.
* Separate fast API responses from long background processing.
* Build reliable systems with **FastAPI + Redis + RQ/ARQ**.

---
