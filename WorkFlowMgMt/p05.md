# Lesson 5: Connecting FastAPI to Redis RQ

## Goals

* Configure a reusable Redis connection (`redis_conn.py`)
* Define a safe job-submission helper around RQ
* Enqueue tasks with **metadata** (job_id, timestamp, payload) you can later query

---

## 1) Minimal folder context

```
fastapi-app/
  app/
    main.py
    settings.py
    redis_conn.py      <-- new
    tasks.py
    jobs.py            <-- new (enqueue helpers)
    schemas.py
    deps.py
```

---

## 2) Settings

`app/settings.py`

```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    REDIS_URL: str = "redis://localhost:6379/0"
    RQ_QUEUE_NAME: str = "default"
    RQ_DEFAULT_TIMEOUT: int = 900  # seconds

    class Config:
        env_file = ".env"

settings = Settings()
```

`.env.example`

```
REDIS_URL=redis://localhost:6379/0
RQ_QUEUE_NAME=default
RQ_DEFAULT_TIMEOUT=900
```

---

## 3) Reusable Redis + RQ objects

`app/redis_conn.py`

```python
from redis import Redis
from rq import Queue
from .settings import settings

_redis_client: Redis | None = None
_default_queue: Queue | None = None

def get_redis() -> Redis:
    global _redis_client
    if _redis_client is None:
        # decode_responses=True if you want str instead of bytes
        _redis_client = Redis.from_url(settings.REDIS_URL)
    return _redis_client

def get_queue(name: str | None = None) -> Queue:
    global _default_queue
    if name is None or name == settings.RQ_QUEUE_NAME:
        if _default_queue is None:
            _default_queue = Queue(settings.RQ_QUEUE_NAME, connection=get_redis(), default_timeout=settings.RQ_DEFAULT_TIMEOUT)
        return _default_queue
    # ad-hoc named queue
    return Queue(name, connection=get_redis(), default_timeout=settings.RQ_DEFAULT_TIMEOUT)
```

---

## 4) Define a task the worker will run

`app/tasks.py`

```python
import time

def heavy_compute(payload: dict) -> dict:
    # Simulate expensive work
    time.sleep(2)
    # Return any structured result you want to persist later
    return {
        "echo": payload,
        "summary": f"Processed keys: {list(payload.keys())}",
    }
```

> Your **RQ worker** (separate process) must import this module so it can find `heavy_compute`.
> Example worker command:
>
> ```bash
> rq worker -u $REDIS_URL $RQ_QUEUE_NAME
> ```
>
> Or with full path imports:
>
> ```bash
> PYTHONPATH=. rq worker -u redis://localhost:6379/0 default
> ```

---

## 5) Pydantic schema for incoming job payload

`app/schemas.py`

```python
from pydantic import BaseModel, Field
from typing import Any, Dict

class JobPayload(BaseModel):
    task: str = Field(examples=["heavy_compute"])
    payload: Dict[str, Any] = Field(default_factory=dict)
```

---

## 6) Enqueue helper that stamps metadata

* Weâ€™ll generate a **stable job_id** (UUIDv4 by default) and attach metadata:

  * `job_id` (string)
  * `submitted_at` (ISO timestamp)
  * `payload` (original sanitized payload)
  * any `tags` if you want to filter later

`app/jobs.py`

```python
from datetime import datetime, timezone
from uuid import uuid4
from rq.job import Job
from typing import Any, Dict
from .redis_conn import get_queue
from . import tasks

# Map task names to callables the worker knows
TASK_REGISTRY = {
    "heavy_compute": tasks.heavy_compute,
}

def submit_job(task_name: str, payload: Dict[str, Any]) -> dict:
    if task_name not in TASK_REGISTRY:
        raise ValueError(f"Unknown task: {task_name}")

    job_id = f"JOB-{uuid4()}"
    submitted_at = datetime.now(timezone.utc).isoformat()

    q = get_queue()
    job: Job = q.enqueue(
        TASK_REGISTRY[task_name],
        payload,
        job_id=job_id,
        meta={
            "job_id": job_id,
            "submitted_at": submitted_at,
            "payload": payload,   # keep lightweight; avoid huge blobs
            "task_name": task_name,
            "tags": ["api", "v1"],
        },
        description=f"{task_name} submitted at {submitted_at}",
        retry=None,  # or rq.retry.Retry(max=3, interval=[10,30,60])
        ttl=None,    # how long job can live in queue before execution
        result_ttl=3600,  # keep result for an hour
        failure_ttl=86400,  # keep failures for a day
    )

    return {
        "job_id": job.get_id(),
        "status": job.get_status(),  # 'queued'
        "submitted_at": submitted_at,
    }
```

---

## 7) Hook it into FastAPI endpoints

`app/main.py`

```python
from fastapi import FastAPI, HTTPException
from rq.job import Job
from .schemas import JobPayload
from .jobs import submit_job
from .redis_conn import get_redis
from rq import Queue

app = FastAPI(title="Workflow API")

@app.post("/submit-job")
def submit_job_endpoint(body: JobPayload):
    try:
        return submit_job(body.task, body.payload)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get("/job-status/{job_id}")
def job_status(job_id: str):
    # Look up the job directly via RQ Job class
    from rq.job import Job
    conn = get_redis()
    try:
        job = Job.fetch(job_id, connection=conn)
    except Exception:
        raise HTTPException(status_code=404, detail="Job not found")

    return {
        "job_id": job.id,
        "status": job.get_status(),   # queued | started | finished | failed | deferred | scheduled
        "meta": job.meta,             # contains our stamped metadata
        "enqueued_at": getattr(job, "enqueued_at", None),
        "started_at": getattr(job, "started_at", None),
        "ended_at": getattr(job, "ended_at", None),
        "result": job.result if job.is_finished else None,
        "exc_info": job.exc_info if job.is_failed else None,
    }
```

---

## 8) Quick test

**Run Redis**

```bash
redis-server
```

**Run worker (separate terminal)**

```bash
export REDIS_URL=redis://localhost:6379/0
export RQ_QUEUE_NAME=default
PYTHONPATH=. rq worker -u $REDIS_URL $RQ_QUEUE_NAME
```

**Run API**

```bash
uvicorn app.main:app --reload
```

**Submit a job**

```bash
curl -X POST http://127.0.0.1:8000/submit-job \
  -H "Content-Type: application/json" \
  -d '{"task":"heavy_compute","payload":{"user_id":101,"message":"hello"}}'
```

Response (example):

```json
{
  "job_id": "JOB-7e5f0b1a-6a25-4d3b-bc7e-0f0bb0b8a5b0",
  "status": "queued",
  "submitted_at": "2025-10-28T03:40:21.331454+00:00"
}
```

**Check status**

```bash
curl http://127.0.0.1:8000/job-status/JOB-7e5f0b1a-6a25-4d3b-bc7e-0f0bb0b8a5b0
```

---

## 9) Notes & good practices

* **Metadata size**: keep `job.meta["payload"]` small; store large inputs in DB/object storage and reference by ID.
* **Idempotency**: if clients might retry `/submit-job`, let them pass a client-side `request_id` and de-dupe on your side.
* **Result retention**: tune `result_ttl`/`failure_ttl` to control Redis memory use.
* **Observability**: consider `rq-dashboard` for a quick web UI.
* **Security**: validate/whitelist `task_name`; never `eval` or import dynamic callables from user input.

---

## 10) (Optional) Docker Compose snippet

```yaml
services:
  redis:
    image: redis:7
    ports: ["6379:6379"]
  worker:
    build: .
    command: bash -lc "PYTHONPATH=. rq worker -u ${REDIS_URL} ${RQ_QUEUE_NAME}"
    environment:
      - REDIS_URL=redis://redis:6379/0
      - RQ_QUEUE_NAME=default
    depends_on: [redis]
  api:
    build: .
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000
    environment:
      - REDIS_URL=redis://redis:6379/0
      - RQ_QUEUE_NAME=default
    ports: ["8000:8000"]
    depends_on: [redis]
```

---

### You now have:

* A **single place** to get Redis/RQ (`redis_conn.py`)
* A **safe submission layer** (`jobs.py`)
* Endpoints that **enqueue with metadata** and **report status/results**.

