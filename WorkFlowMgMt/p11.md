# Lesson 11: Creating an Example Workflow API

## What you’ll build

A `/workflow/run` endpoint that accepts a **multi-step task** (e.g., `text → LLM → result → store`), submits it to your **ARQ** worker, and streams data between steps with clear dependencies.

## Learning goals

* Design a **clean submission contract** for workflows.
* Implement **step dependencies** and a simple **data-flow** mechanism.
* Persist **progress + results** in Redis for later polling (Lesson 8).

---

## 1) API contract

### Request (example)

```json
{
  "run_name": "summarize-then-store",
  "inputs": {
    "text": "India's TB elimination program aims to reduce incidence rapidly."
  },
  "steps": [
    {"id": "pre", "op": "preprocess", "params": {"field": "text"}},
    {"id": "llm", "op": "llm_summarize", "needs": ["pre"], "params": {"max_tokens": 100}},
    {"id": "store", "op": "store_result", "needs": ["llm"], "params": {"namespace": "summaries"}}
  ]
}
```

### Response

```json
{
  "job_id": "b7f935c2-6f95-4a8e-9c1a-3c9c7a21e5a2",
  "status_url": "/status/b7f935c2-6f95-4a8e-9c1a-3c9c7a21e5a2"
}
```

---

## 2) Pydantic schemas

Create `app/schemas.py` (additions):

```python
from typing import Dict, List, Optional, Literal
from pydantic import BaseModel, Field

OpName = Literal["preprocess", "llm_summarize", "store_result"]

class WorkflowStep(BaseModel):
    id: str = Field(..., description="Unique step id")
    op: OpName
    needs: List[str] = Field(default_factory=list, description="Step ids this depends on")
    params: Dict = Field(default_factory=dict)

class WorkflowRunRequest(BaseModel):
    run_name: str
    inputs: Dict = Field(default_factory=dict)
    steps: List[WorkflowStep]

class WorkflowSubmissionResponse(BaseModel):
    job_id: str
    status_url: str
```

---

## 3) FastAPI route: `/workflow/run`

Add to `app/routing.py` (or a new `app/workflows.py`) and include in `main.py`.

```python
import uuid
import json
import time
import redis.asyncio as redis
from fastapi import APIRouter, Depends, HTTPException
from .schemas import WorkflowRunRequest, WorkflowSubmissionResponse
from .settings import settings

router = APIRouter(prefix="/workflow", tags=["workflow"])

# Redis keys
def k_job(job_id: str) -> str: return f"wf:job:{job_id}"        # hash: status, run_name, created
def k_prog(job_id: str) -> str: return f"wf:progress:{job_id}"   # list (RPUSH): progress logs
def k_res(job_id: str) -> str: return f"wf:result:{job_id}"      # string (JSON)

async def get_redis():
    r = redis.from_url(settings.REDIS_URL, decode_responses=True)
    try:
        yield r
    finally:
        await r.aclose()

@router.post("/run", response_model=WorkflowSubmissionResponse, status_code=202)
async def run_workflow(payload: WorkflowRunRequest, r=Depends(get_redis)):
    job_id = str(uuid.uuid4())
    # seed job metadata and initial progress
    await r.hset(k_job(job_id), mapping={
        "status": "queued",
        "run_name": payload.run_name,
        "created_ts": str(time.time())
    })
    await r.rpush(k_prog(job_id), json.dumps({"ts": time.time(), "msg": "enqueued"}))

    # enqueue ARQ task
    # Tip: import here to avoid ARQ import cost on API boot in minimal setups
    from arq.connections import ArqRedis
    ar = await ArqRedis.create_redis_pool(settings.REDIS_URL)
    await ar.enqueue_job("workflow_run", job_id=job_id, payload=payload.model_dump())
    await ar.close(close_connection_pool=True)

    return WorkflowSubmissionResponse(job_id=job_id, status_url=f"/status/{job_id}")
```

> This assumes you already have `/status/{job_id}` from Lesson 8 to fetch **status**, **progress logs**, and **result**.

---

## 4) ARQ worker: orchestrating steps

Create/extend `app/worker_arq.py` with a **small orchestrator** that resolves dependencies, runs ops, and passes a **shared context**:

```python
import json
import time
from typing import Dict, Any, List
import redis.asyncio as redis
from arq import cron
from arq.connections import ArqRedis
from .settings import settings

# -------- Redis key helpers ----------
def k_job(job_id: str) -> str: return f"wf:job:{job_id}"
def k_prog(job_id: str) -> str: return f"wf:progress:{job_id}"
def k_res(job_id: str) -> str: return f"wf:result:{job_id}"

# -------- Utility: progress logging ---
async def log(r, job_id: str, msg: str, **extra):
    entry = {"ts": time.time(), "msg": msg, **extra}
    await r.rpush(k_prog(job_id), json.dumps(entry))

# --------- Step implementations -------
async def op_preprocess(ctx: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:
    # Simple text cleanup / selection
    field = params.get("field", "text")
    raw = ctx["inputs"].get(field, "")
    text = " ".join(str(raw).split())
    return {"clean_text": text}

async def op_llm_summarize(ctx: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:
    # Placeholder LLM call — replace with LangChain/OpenAI later
    source = resolve_ref(ctx, params.get("source", "@pre.clean_text"))  # default: from preprocess
    max_tokens = params.get("max_tokens", 120)
    summarized = f"Summary({min(len(source), max_tokens)}): {source[:max_tokens]}"
    # You could call your real chain here:
    # result = await my_langchain_summarize(source, max_tokens)
    return {"summary": summarized}

async def op_store_result(ctx: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:
    ns = params.get("namespace", "default")
    data = resolve_ref(ctx, params.get("data", "@llm.summary"))
    # Simulate storage: write into ctx.stores[ns]
    ctx.setdefault("stores", {}).setdefault(ns, []).append(data)
    return {"stored_under": ns, "items": len(ctx["stores"][ns])}

OPS = {
    "preprocess": op_preprocess,
    "llm_summarize": op_llm_summarize,
    "store_result": op_store_result
}

# ------- Reference resolver ----------
def resolve_ref(ctx: Dict[str, Any], ref_or_value: Any):
    """
    Tiny resolver:
      - If value is a string starting with "@step.output_key" (e.g., "@pre.clean_text"),
        fetch from ctx["steps"][step_id][output_key].
      - Else return the value as-is.
    """
    if isinstance(ref_or_value, str) and ref_or_value.startswith("@"):
        path = ref_or_value[1:].split(".", 1)
        step_id = path[0]
        key = path[1] if len(path) > 1 else None
        data = ctx["steps"].get(step_id, {})
        return data.get(key) if key else data
    return ref_or_value

# ------- Topological execution -------
def topo_order(steps: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    # Kahn’s algorithm (minimal)
    indeg = {s["id"]: 0 for s in steps}
    graph = {s["id"]: set(s.get("needs", [])) for s in steps}
    for s in steps:
        for dep in s.get("needs", []):
            indeg[s["id"]] += 1
    ready = [s for s in steps if indeg[s["id"]] == 0]
    order = []
    visited = set()
    while ready:
        s = ready.pop()
        order.append(s)
        visited.add(s["id"])
        for t in steps:
            if s["id"] in graph[t["id"]] and t["id"] not in visited:
                indeg[t["id"]] -= 1
                if indeg[t["id"]] == 0:
                    ready.append(t)
    if len(order) != len(steps):
        raise ValueError("Cyclic or missing dependencies in steps.")
    return order

# -------- Worker job entry ----------
async def workflow_run(ctx, job_id: str, payload: Dict[str, Any]):
    r = redis.from_url(settings.REDIS_URL, decode_responses=True)
    await r.hset(k_job(job_id), "status", "started")
    await log(r, job_id, "started", run=payload.get("run_name"))
    try:
        steps = payload["steps"]
        ordered = topo_order(steps)
        wf_ctx: Dict[str, Any] = {
            "inputs": payload.get("inputs", {}),
            "steps": {}  # step_id -> outputs
        }

        for s in ordered:
            op = s["op"]
            step_id = s["id"]
            fn = OPS.get(op)
            if not fn:
                raise ValueError(f"Unknown op: {op}")
            await log(r, job_id, "step_start", step=step_id, op=op)
            out = await fn(wf_ctx, s.get("params", {}))
            # Save outputs for downstream steps
            wf_ctx["steps"][step_id] = out
            await log(r, job_id, "step_done", step=step_id, outputs=out)

        # Final result: merge all step outputs + stores
        final = {
            "run_name": payload.get("run_name"),
            "inputs": wf_ctx["inputs"],
            "outputs": wf_ctx["steps"],
            "stores": wf_ctx.get("stores", {})
        }
        await r.set(k_res(job_id), json.dumps(final))
        await r.hset(k_job(job_id), "status", "finished")
        await log(r, job_id, "finished")
    except Exception as e:
        await r.hset(k_job(job_id), mapping={"status": "failed", "error": str(e)})
        await log(r, job_id, "failed", error=str(e))
        raise
    finally:
        await r.aclose()

class WorkerSettings:
    functions = [workflow_run]
    redis_settings = settings.REDIS_URL
    # Optional: periodic cleanup task
    cron_jobs = [
        cron(func="noop", minute={0,15,30,45})
    ]

async def noop(ctx):
    return "ok"
```

> Replace the placeholder LLM step with your **LangChain/OpenAI** integration when ready.

---

## 5) Wiring ARQ (recap from Lesson 10)

Your `arq` worker CLI:

```bash
arq app.worker_arq.WorkerSettings
```

Env (e.g., `.env`):

```
REDIS_URL=redis://localhost:6379/0
```

---

## 6) Trying it out

### Submit a workflow

```bash
curl -X POST http://localhost:8000/workflow/run \
  -H "Content-Type: application/json" \
  -d '{
    "run_name":"summarize-then-store",
    "inputs":{"text":"India'\''s TB elimination program aims to reduce incidence rapidly."},
    "steps":[
      {"id":"pre","op":"preprocess","params":{"field":"text"}},
      {"id":"llm","op":"llm_summarize","needs":["pre"],"params":{"max_tokens":80}},
      {"id":"store","op":"store_result","needs":["llm"],"params":{"namespace":"summaries","data":"@llm.summary"}}
    ]
  }'
```

### Poll status (from Lesson 8)

```bash
curl http://localhost:8000/status/<JOB_ID>
```

Expect fields like:

* `status`: `queued | started | finished | failed`
* `progress`: array of logs (each with `ts`, `msg`, optional details)
* `result`: final JSON when finished

---

## 7) Notes on dependencies & data flow

* **Topological order** ensures `needs` run first; cycles raise a clear error.
* **Context passing**: every step writes outputs into `ctx["steps"][<id>]`.
* **Reference syntax**: `"@pre.clean_text"` fetches output `clean_text` from step `pre`.
* **Idempotency**: make ops pure where possible; if you add external writes, put **all** final writes in the last step to reduce partial side-effects.
* **Retries**: ARQ supports retries per job; you can add custom retry logic or wrap step calls with try/except and re-raise.

---

## 8) (Optional) RQ variant (one-liner enqueue)

If you chose **RQ** instead of ARQ, your API route body swaps the enqueue:

```python
from rq import Queue
from redis import Redis
from .worker_rq import workflow_run  # same signature, but sync def

q = Queue("default", connection=Redis.from_url(settings.REDIS_URL))
q.enqueue(workflow_run, job_id=job_id, payload=payload.model_dump())
```

…and the worker is started with `rq worker`.

---

## 9) Where to plug your LLM

Replace `op_llm_summarize` with a real call, e.g., a LangChain Runnable:

```python
# pseudo
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2)

async def op_llm_summarize(ctx, params):
    source = resolve_ref(ctx, params.get("source", "@pre.clean_text"))
    prompt = f"Summarize in 2 sentences:\n\n{source}"
    resp = await llm.ainvoke(prompt)
    return {"summary": resp.content}
```

---

## 10) What you now have

* A **production-shaped** `/workflow/run` endpoint.
* A flexible **step graph** with dependency management.
* **Observable** progress and persistent **results** (via Redis).
* A clean seam to swap mock LLMs with **LangChain/OpenAI**.


