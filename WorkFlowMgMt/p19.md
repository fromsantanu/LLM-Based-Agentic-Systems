# Lesson 19: LLM Workflow Example (Optional)

## What you’ll build

A small FastAPI + Redis RQ app with:

* `POST /llm/submit`: enqueue a job that asks an LLM to structure user text.
* `GET  /llm/status/{job_id}`: check job state.
* `GET  /llm/result/{job_id}`: fetch the validated, structured JSON.

Flow: **Submit text → Queue → Worker → LLM → Validate → Store in Redis → Fetch result**

---

## Architecture (at a glance)

```
Client
  | POST /llm/submit
  v
FastAPI  ----> Redis (queue: "llm")
  ^               |
  |               v
  |<----- RQ Worker runs task: call LLM -> validate -> save result
  |
  |--- GET /llm/status/{job_id}
  |--- GET /llm/result/{job_id}
```

---

## Folder structure

```
llm-workflow/
├─ app/
│  ├─ main.py
│  ├─ settings.py
│  ├─ queues.py
│  ├─ schemas.py
│  ├─ llm.py
│  └─ tasks.py
├─ worker.py
├─ requirements.txt
└─ .env.example
```

---

## Dependencies

`requirements.txt`

```txt
fastapi
uvicorn[standard]
pydantic>=2
redis>=5
rq>=1.15
python-dotenv
openai>=1.40
```

`.env.example`

```env
REDIS_URL=redis://localhost:6379/0
RQ_QUEUE=llm
OPENAI_API_KEY=sk-your-key
OPENAI_MODEL=gpt-4o-mini
```

Copy to `.env` and fill in your values.

---

## Data contracts

`app/schemas.py`

```python
from pydantic import BaseModel, Field, ValidationError
from typing import List, Optional

class LLMInput(BaseModel):
    text: str = Field(..., min_length=1, description="Free-form user text")

class ExtractedEntity(BaseModel):
    type: str
    value: str
    confidence: float = Field(..., ge=0.0, le=1.0)

class LLMOutput(BaseModel):
    summary: str
    sentiment: str   # e.g., "positive" | "neutral" | "negative"
    tags: List[str]
    entities: List[ExtractedEntity]
```

---

## Centralized settings & queue handle

`app/settings.py`

```python
from pydantic import BaseModel
from dotenv import load_dotenv
import os

load_dotenv()

class Settings(BaseModel):
    redis_url: str = os.environ.get("REDIS_URL", "redis://localhost:6379/0")
    rq_queue: str = os.environ.get("RQ_QUEUE", "llm")
    openai_api_key: str = os.environ.get("OPENAI_API_KEY", "")
    openai_model: str = os.environ.get("OPENAI_MODEL", "gpt-4o-mini")

settings = Settings()
```

`app/queues.py`

```python
from redis import Redis
from rq import Queue
from .settings import settings

redis_conn = Redis.from_url(settings.redis_url)
queue = Queue(settings.rq_queue, connection=redis_conn)
```

---

## LLM helper (single responsibility)

`app/llm.py`

```python
import json
from typing import Any, Dict
from openai import OpenAI
from .settings import settings

client = OpenAI(api_key=settings.openai_api_key)

SYSTEM_PROMPT = (
    "You are a precise information extraction assistant. "
    "Return ONLY a compact JSON object following the schema provided, no prose."
)

# We’ll ask the model to produce strict JSON and then validate with Pydantic.
def call_llm_for_structure(user_text: str) -> Dict[str, Any]:
    # Prefer models that support JSON mode. Fallback: instruct strongly.
    completion = client.chat.completions.create(
        model=settings.openai_model,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": f"""
Read the text and produce JSON with keys:
summary: brief summary (<=40 words)
sentiment: one of ["positive","neutral","negative"]
tags: array of 1-5 short tags
entities: array of objects {{type, value, confidence (0..1)}}

Text:
\"\"\"{user_text}\"\"\"        
            """}
        ],
        response_format={"type": "json_object"}  # enables JSON output on newer models
    )
    raw = completion.choices[0].message.content
    return json.loads(raw)
```

---

## The queued task

`app/tasks.py`

```python
from typing import Any, Dict
from rq import get_current_job
from .llm import call_llm_for_structure
from .schemas import LLMOutput, ValidationError
from .queues import redis_conn

RESULT_KEY_PREFIX = "result:"

def process_text_to_structured(user_text: str) -> Dict[str, Any]:
    job = get_current_job()
    job.meta["stage"] = "calling_llm"
    job.save_meta()

    # 1) Call LLM
    raw = call_llm_for_structure(user_text)

    job.meta["stage"] = "validating"
    job.save_meta()

    # 2) Validate (raise if shape is wrong)
    data = LLMOutput.model_validate(raw)

    # 3) Persist result in Redis keyed by job_id
    result_key = f"{RESULT_KEY_PREFIX}{job.id}"
    redis_conn.set(result_key, data.model_dump_json(), ex=60 * 60)  # keep 1h

    job.meta["stage"] = "done"
    job.save_meta()

    return data.model_dump()
```

---

## FastAPI app (submit, status, result)

`app/main.py`

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from rq.job import Job
from typing import Any, Dict, Optional
from .queues import queue, redis_conn
from .schemas import LLMInput
from .tasks import process_text_to_structured, RESULT_KEY_PREFIX

app = FastAPI(title="LLM Workflow API", version="1.0")

class SubmitResponse(BaseModel):
    job_id: str

class StatusResponse(BaseModel):
    job_id: str
    status: str
    stage: Optional[str] = None

@app.post("/llm/submit", response_model=SubmitResponse)
def submit_job(payload: LLMInput):
    job = queue.enqueue(process_text_to_structured, payload.text, job_timeout=180)
    return SubmitResponse(job_id=job.id)

@app.get("/llm/status/{job_id}", response_model=StatusResponse)
def job_status(job_id: str):
    try:
        job = Job.fetch(job_id, connection=queue.connection)
    except Exception:
        raise HTTPException(status_code=404, detail="Job not found")

    stage = job.meta.get("stage")
    return StatusResponse(job_id=job.id, status=job.get_status(), stage=stage)

@app.get("/llm/result/{job_id}")
def job_result(job_id: str) -> Dict[str, Any]:
    key = f"{RESULT_KEY_PREFIX}{job_id}"
    blob = redis_conn.get(key)
    if not blob:
        # Check job state for clearer error
        try:
            job = Job.fetch(job_id, connection=queue.connection)
            status = job.get_status()
        except Exception:
            status = "unknown"
        raise HTTPException(status_code=404, detail=f"No result yet. Status={status}")
    return {"job_id": job_id, "result": blob.decode("utf-8")}
```

---

## RQ worker entrypoint

`worker.py`

```python
import os
from rq import Worker, Queue, Connection
from app.queues import redis_conn
from app.settings import settings

listen = [settings.rq_queue]

if __name__ == "__main__":
    with Connection(redis_conn):
        worker = Worker([Queue(name) for name in listen])
        worker.work(with_scheduler=True)
```

---

## Run it

1. Start Redis:

```bash
redis-server
```

2. Install deps:

```bash
python -m venv .venv && source .venv/bin/activate  # (Windows: .venv\Scripts\activate)
pip install -r requirements.txt
cp .env.example .env  # fill values
```

3. Run API:

```bash
uvicorn app.main:app --reload --port 8000
```

4. Run worker (separate shell):

```bash
source .venv/bin/activate
python worker.py
```

---

## Try it out

**Submit**

```bash
curl -X POST http://localhost:8000/llm/submit \
  -H "Content-Type: application/json" \
  -d '{"text": "I loved the clinic. Dr. Rao was kind. Waiting time was 10 minutes."}'
```

→ `{"job_id":"<uuid>"}`

**Poll status**

```bash
curl http://localhost:8000/llm/status/<job_id>
```

→ `{"job_id":"...","status":"started","stage":"validating"}`

**Fetch result**

```bash
curl http://localhost:8000/llm/result/<job_id>
```

→

```json
{
  "job_id": "abc123",
  "result": "{\"summary\":\"...\",\"sentiment\":\"positive\",\"tags\":[\"clinic\",\"experience\"],\"entities\":[{\"type\":\"person\",\"value\":\"Dr. Rao\",\"confidence\":0.94}]}"
}
```

*(Note: `result` is a JSON string for simplicity; you can `json.loads` on the client.)*

---

## Notes, pitfalls & options

* **Strict JSON**: We enabled `response_format={"type": "json_object"}` and then **validated** with Pydantic. Always validate LLM outputs.
* **Timeouts**: Set `job_timeout=180` and consider OpenAI client timeouts/retries if needed.
* **Idempotency**: You can hash `LLMInput` and de-dupe repeated requests by storing a `hash → job_id` map in Redis.
* **Persistence**: For production, persist results in a DB (Postgres) and store only a reference in Redis.
* **Auth**: Protect endpoints with JWT if needed (see your JWT lessons).
* **ARQ variant** (if you prefer async):

  * Replace RQ with ARQ and make `process_text_to_structured` `async`.
  * Use ARQ’s `ArqRedis.enqueue_job`.
* **Streaming**: You can stream logs via WebSockets to update `stage` in real time (see your Lesson 15).

---

## Exercises

1. **Extend schema**: Add `language` detection and `toxicity_score` (0–1). Validate and return.
2. **Batch mode**: Accept an array of texts, enqueue one job per text, return all job_ids.
3. **Retries**: Configure RQ retry (e.g., `Retry(max=3, interval=[5,10,20])`) on network errors.
4. **Dashboard**: Run `rq-dashboard` to visualize the queue.
5. **Frontend**: Make a simple React or Streamlit UI that submits and polls.

---

## Optional: small embellishments

* Add `X-Request-ID` header passthrough and log it in `job.meta`.
* Save token usage from the OpenAI response (if available) for analytics.
* Add `/llm/cancel/{job_id}` to demonstrate cancellation semantics.

---

