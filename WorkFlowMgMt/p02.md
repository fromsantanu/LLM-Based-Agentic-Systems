# Lesson 2: Environment & Project Setup

## 1) Install dependencies

**Create and activate a virtualenv**

```bash
# Linux/macOS
python -m venv .venv && source .venv/bin/activate

# Windows (PowerShell)
python -m venv .venv; .\.venv\Scripts\Activate.ps1
```

**Base web stack**

```bash
pip install fastapi uvicorn[standard] pydantic-settings python-dotenv
```

**Choose ONE queue runtime (start with RQ; switch to ARQ later if you prefer async):**

* **Option A — RQ (sync tasks, simple & battle-tested)**

```bash
pip install redis rq
```

* **Option B — ARQ (async tasks on asyncio)**

```bash
pip install arq redis
```

**(Optional but recommended)**

```bash
pip install httpx loguru
```

If you plan to publish as a package:

```bash
pip install hatchling
```

---

## 2) Project folder structure (modular FastAPI + workers)

```
workflow-apis/
├─ app/
│  ├─ __init__.py
│  ├─ main.py                 # FastAPI app factory + routes include
│  ├─ settings.py             # Pydantic Settings (REDIS_URL, etc.)
│  ├─ deps.py                 # Shared dependencies (e.g., Redis conn)
│  ├─ routers/
│  │  ├─ __init__.py
│  │  ├─ health.py            # /health, /ready endpoints
│  │  └─ jobs.py              # submit_job, get_status, etc.
│  ├─ services/
│  │  ├─ __init__.py
│  │  ├─ tasks_rq.py          # RQ task definitions
│  │  └─ tasks_arq.py         # ARQ task definitions
│  ├─ queues/
│  │  ├─ __init__.py
│  │  ├─ rq_conn.py           # RQ Queue + Redis connection
│  │  └─ arq_settings.py      # ARQ worker settings
│  └─ models/
│     ├─ __init__.py
│     └─ dto.py               # Pydantic DTOs for requests/responses
├─ workers/
│  ├─ rq_worker.py            # `rq worker` entry (if using RQ)
│  └─ arq_worker.py           # `arq` entry (if using ARQ)
├─ .env.example
├─ pyproject.toml             # or requirements.txt
├─ README.md
└─ Makefile                   # handy dev commands (optional)
```

---

## 3) `.env` configuration (copy to `.env`)

```env
# Server
APP_ENV=dev
APP_NAME=workflow-apis
HOST=0.0.0.0
PORT=8000

# Queue/Redis
REDIS_URL=redis://localhost:6379/0

# RQ
RQ_QUEUE_NAME=default
WORKER_NAME=worker-1

# ARQ
ARQ_QUEUE_NAME=arq:jobs
ARQ_BURST=False
```

> Tip: keep both RQ and ARQ keys; you’ll enable only the one you use.

---

## 4) Pydantic settings

**app/settings.py**

```python
from pydantic_settings import BaseSettings, SettingsConfigDict

class Settings(BaseSettings):
    app_env: str = "dev"
    app_name: str = "workflow-apis"
    host: str = "0.0.0.0"
    port: int = 8000

    redis_url: str = "redis://localhost:6379/0"

    # RQ
    rq_queue_name: str = "default"
    worker_name: str = "worker-1"

    # ARQ
    arq_queue_name: str = "arq:jobs"
    arq_burst: bool = False

    model_config = SettingsConfigDict(env_file=".env", env_prefix="", extra="ignore")

settings = Settings()
```

---

## 5) Minimal FastAPI app + routers

**app/main.py**

```python
from fastapi import FastAPI
from .settings import settings
from .routers import health, jobs

def create_app() -> FastAPI:
    app = FastAPI(title=settings.app_name)
    app.include_router(health.router, prefix="/health", tags=["health"])
    app.include_router(jobs.router, prefix="/jobs", tags=["jobs"])
    return app

app = create_app()
```

**app/routers/health.py**

```python
from fastapi import APIRouter

router = APIRouter()

@router.get("/")
def health():
    return {"status": "ok"}

@router.get("/ready")
def ready():
    return {"ready": True}
```

**app/models/dto.py**

```python
from pydantic import BaseModel

class SubmitJobRequest(BaseModel):
    payload: dict

class SubmitJobResponse(BaseModel):
    job_id: str
    status: str = "submitted"

class JobStatusResponse(BaseModel):
    job_id: str
    status: str
    result: dict | None = None
```

---

## 6) RQ setup (Option A)

**app/queues/rq_conn.py**

```python
import redis
from rq import Queue
from ..settings import settings

_redis = redis.from_url(settings.redis_url)
rq_queue = Queue(settings.rq_queue_name, connection=_redis)
```

**app/services/tasks_rq.py**

```python
import time

def heavy_task(data: dict) -> dict:
    # Simulate expensive work
    time.sleep(3)
    return {"echo": data, "meta": {"finish_reason": "stop"}}
```

**app/routers/jobs.py (RQ path)**

```python
from fastapi import APIRouter
from ..models.dto import SubmitJobRequest, SubmitJobResponse, JobStatusResponse
from ..queues.rq_conn import rq_queue
from ..services.tasks_rq import heavy_task

router = APIRouter()

@router.post("/submit", response_model=SubmitJobResponse)
def submit_job(req: SubmitJobRequest):
    job = rq_queue.enqueue(heavy_task, req.payload)
    return SubmitJobResponse(job_id=job.get_id())

@router.get("/status/{job_id}", response_model=JobStatusResponse)
def job_status(job_id: str):
    job = rq_queue.fetch_job(job_id)
    if not job:
        return JobStatusResponse(job_id=job_id, status="not_found")
    if job.is_finished:
        return JobStatusResponse(job_id=job_id, status="finished", result=job.result)
    if job.is_failed:
        return JobStatusResponse(job_id=job_id, status="failed")
    return JobStatusResponse(job_id=job_id, status="queued" if job.enqueued_at and not job.started_at else "started")
```

**workers/rq_worker.py**

```python
# Run worker with:  rq worker -u $REDIS_URL $RQ_QUEUE_NAME
# (This file exists mainly for clarity; RQ uses the CLI entrypoint.)
```

**Run:**

```bash
# terminal 1
redis-server   # or use Docker: docker run -p 6379:6379 redis:7

# terminal 2 (web)
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# terminal 3 (worker)
rq worker -u "$REDIS_URL" "$RQ_QUEUE_NAME"
# Windows PowerShell: rq worker -u $env:REDIS_URL $env:RQ_QUEUE_NAME
```

---

## 7) ARQ setup (Option B)

**app/queues/arq_settings.py**

```python
from arq import cron
from arq.connections import RedisSettings
from ..settings import settings

async def heavy_task(ctx, data: dict) -> dict:
    # async work here (await httpx, DB, etc.)
    return {"echo": data, "meta": {"finish_reason": "stop"}}

class WorkerSettings:
    functions = [heavy_task]
    queue_name = settings.arq_queue_name
    redis_settings = RedisSettings.from_dsn(settings.redis_url)
    # example of a periodic job:
    cron_jobs = [cron(heavy_task, minute={0}, kwargs={"data": {"ping": "startup"}})]
```

**app/services/tasks_arq.py**

```python
from arq import create_pool
from arq.connections import RedisSettings
from ..settings import settings

async def enqueue_arq_job(data: dict):
    redis = await create_pool(RedisSettings.from_dsn(settings.redis_url))
    return await redis.enqueue_job("heavy_task", data)
```

**app/routers/jobs.py (ARQ path, alternate endpoints)**

```python
# If you prefer ARQ, replace the RQ endpoints with this async variant:
from fastapi import APIRouter
from ..models.dto import SubmitJobRequest, SubmitJobResponse, JobStatusResponse
from ..services.tasks_arq import enqueue_arq_job

router = APIRouter()

@router.post("/submit", response_model=SubmitJobResponse)
async def submit_job(req: SubmitJobRequest):
    job = await enqueue_arq_job(req.payload)
    return SubmitJobResponse(job_id=job.job_id)

@router.get("/status/{job_id}", response_model=JobStatusResponse)
async def job_status(job_id: str):
    # ARQ doesn't keep a built-in “fetch by id” like RQ; typically you
    # push results to a store (Redis key/DB). For the lesson, return stub:
    return JobStatusResponse(job_id=job_id, status="unknown")
```

**workers/arq_worker.py**

```python
# Run worker with:
# arq app.queues.arq_settings.WorkerSettings
```

**Run:**

```bash
# terminal 1
redis-server

# terminal 2
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# terminal 3
arq app.queues.arq_settings.WorkerSettings
```

---

## 8) pyproject.toml (optional, for `pip install -e .`)

```toml
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "workflow-apis"
version = "0.1.0"
description = "Workflow APIs with FastAPI + Redis (RQ/ARQ)"
dependencies = [
  "fastapi",
  "uvicorn[standard]",
  "pydantic-settings",
  "python-dotenv",
  "redis",
  # choose one:
  # "rq",
  # "arq",
  "httpx",
  "loguru",
]

[tool.hatch.build.targets.wheel]
packages = ["app"]
```

Install in editable mode:

```bash
pip install -e .
```

---

## 9) Quick smoke test

```bash
# submit a job (RQ example)
curl -X POST http://localhost:8000/jobs/submit \
  -H "Content-Type: application/json" \
  -d '{"payload": {"message":"hello"}}'

# check status
curl http://localhost:8000/jobs/status/<JOB_ID_FROM_RESPONSE>
```

---

## 10) (Optional) Makefile for DX

```makefile
run:
\tuvicorn app.main:app --reload --host 0.0.0.0 --port 8000

worker-rq:
\trq worker -u $$(grep ^REDIS_URL .env | cut -d= -f2) $$(grep ^RQ_QUEUE_NAME .env | cut -d= -f2)

worker-arq:
\tarq app.queues.arq_settings.WorkerSettings
```

---

### Recap

* Install core web packages, then **pick one** queue: RQ (simple, sync) **or** ARQ (async).
* Use a clean folder split: `routers/`, `services/`, `queues/`, `models/`.
* Centralize config in `settings.py` (+ `.env`).
* Keep workers as separate processes (terminal 3).
* You now have a working baseline to add Lesson 3’s **job lifecycle** and **status polling** details.

