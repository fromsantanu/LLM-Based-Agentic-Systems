# **Lesson 12: Job Chaining and Dependencies**

---

### 🎯 **Objective**

Learn how to design workflows where one job triggers another — either sequentially or in parallel — and manage dependencies, retries, and fallbacks effectively in Redis RQ or ARQ.

---

### 🧩 **1. What is Job Chaining?**

Job chaining means connecting multiple background tasks so that the output of one becomes the input for the next.
It ensures **ordered execution** of dependent tasks.

Example:

```
Task A → Task B → Task C
```

Each subsequent job runs **only after** the previous one completes successfully.

---

### ⚙️ **2. Why Use Job Chaining?**

* To **divide complex workflows** into smaller, modular tasks.
* To **reuse task results** (e.g., preprocess → analyze → store).
* To ensure **fault isolation** — if one step fails, you can retry or skip selectively.

---

### 🧠 **3. Implementing Chained Jobs in Redis RQ**

#### Example Code:

```python
from rq import Queue
from redis import Redis
from time import sleep

redis_conn = Redis()
q = Queue(connection=redis_conn)

def task_a():
    print("Running Task A")
    return "Data from A"

def task_b(data):
    print(f"Running Task B with {data}")
    return "Result from B"

def task_c(result):
    print(f"Running Task C with {result}")
    return "Workflow Complete"

# Chaining jobs manually
job_a = q.enqueue(task_a)
job_b = q.enqueue(task_b, depends_on=job_a)
job_c = q.enqueue(task_c, depends_on=job_b)
```

🧩 **Explanation:**

* `depends_on` ensures a job waits until its dependency finishes successfully.
* If `job_a` fails, `job_b` and `job_c` won’t run.

---

### ⚡ **4. Sequential vs Parallel Jobs**

#### **Sequential Pattern**

Ideal for ordered pipelines:

```
Data Extraction → Processing → Reporting
```

Each task starts only after the previous completes.

#### **Parallel Pattern**

Used when independent tasks can run together:

```
   → Task B
A → Task C
   → Task D
```

Then a **final aggregation task** combines results from all.

Example:

```python
job_b = q.enqueue(task_b)
job_c = q.enqueue(task_c)
job_d = q.enqueue(task_d)

job_final = q.enqueue(aggregate_results, depends_on=[job_b, job_c, job_d])
```

---

### 🔁 **5. Retry Logic and Error Fallbacks**

#### Adding Retries:

```python
q.enqueue(task_a, retry=3)
```

This automatically retries a failed job 3 times before marking it as failed.

#### Handling Fallbacks:

You can enqueue a **fallback task** if a job fails:

```python
from rq.job import Job

def fallback_job():
    print("Fallback: handling failed job gracefully")

job_main = q.enqueue(task_a)
job_fallback = q.enqueue(fallback_job, depends_on=job_main)
```

Or, in custom logic:

```python
try:
    result = task_a()
except Exception:
    fallback_job()
```

---

### 🔄 **6. Using ARQ for Async Job Dependencies**

ARQ supports chaining asynchronously using `await` inside coroutines.

Example:

```python
from arq import create_pool
from arq.connections import RedisSettings

async def task_a(ctx):
    print("Task A done")
    return "A"

async def task_b(ctx, data):
    print(f"Task B received {data}")
    return "B done"

async def main(ctx):
    result_a = await ctx['redis'].enqueue_job('task_a')
    await ctx['redis'].enqueue_job('task_b', result_a.result())

class WorkerSettings:
    functions = [task_a, task_b, main]
    redis_settings = RedisSettings()
```

---

### 🧮 **7. Best Practices**

* Keep tasks **idempotent** (safe to re-run without side effects).
* Use **depends_on** to ensure proper sequencing.
* Always design **fallbacks** for critical workflows.
* For long chains, store partial results in Redis or a database.

---

### ✅ **Summary**

| Concept           | Description                                        |
| ----------------- | -------------------------------------------------- |
| **Job chaining**  | Linking dependent tasks sequentially               |
| **Parallel jobs** | Running multiple tasks at once                     |
| **Retry logic**   | Automatic re-attempt on failure                    |
| **Fallback**      | Graceful handling when job fails                   |
| **Best practice** | Keep tasks modular, idempotent, and fault-tolerant |

---

