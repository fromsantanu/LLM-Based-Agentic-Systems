# Lesson 6: Writing Worker Functions

## What you’ll learn

* How to write **task functions** (e.g., `process_message`, `generate_report`, `run_chain`)
* How an **RQ Worker** runs these jobs asynchronously
* **Logging**, **progress updates**, and **error handling** (including retries)

---

## 1) Project files (new/updated)

```
fastapi-app/
  app/
    tasks/
      __init__.py
      worker.py            # Worker entrypoint (RQ Worker process)
      jobs.py              # Your task functions live here
      logging_config.py    # Centralized logging config (optional)
```

---

## 2) Logging setup

**app/tasks/logging_config.py**

```python
import logging
import sys

def setup_logging(level: str = "INFO") -> None:
    fmt = "%(asctime)s | %(levelname)s | %(name)s | %(message)s"
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(logging.Formatter(fmt))
    root = logging.getLogger()
    root.setLevel(level)
    # avoid duplicate handlers if reloaded
    if not any(isinstance(h, logging.StreamHandler) for h in root.handlers):
        root.addHandler(handler)
```

---

## 3) Writing task functions

Key guidelines:

* **Pure, deterministic** functions where possible. Fetch external resources *inside* the task.
* Use `rq.get_current_job()` to **update progress** and attach **metadata**.
* Catch **expected exceptions**; let unexpected ones raise to leverage RQ’s retry.

**app/tasks/jobs.py**

```python
import time
import json
import logging
from typing import Any, Dict
from rq import get_current_job

logger = logging.getLogger(__name__)

def _update_progress(job, percent: int, note: str = ""):
    if not job: 
        return
    job.meta["progress"] = percent
    if note:
        job.meta["note"] = note
    job.save_meta()

def process_message(message: str) -> Dict[str, Any]:
    """
    Cleans and analyzes a message. Demonstrates progress updates.
    """
    job = get_current_job()
    logger.info("process_message started", extra={"job_id": job.id if job else None})

    _update_progress(job, 5, "starting")

    # 1) normalize
    normalized = " ".join(message.strip().split())
    time.sleep(0.2)
    _update_progress(job, 25, "normalized")

    # 2) naive sentiment (stub)
    sentiment = "positive" if any(w in normalized.lower() for w in ["good","great","thanks"]) else "neutral"
    time.sleep(0.2)
    _update_progress(job, 60, "sentiment_done")

    # 3) produce result
    result = {"original": message, "normalized": normalized, "sentiment": sentiment}
    _update_progress(job, 100, "done")
    logger.info("process_message finished", extra={"job_id": job.id if job else None})
    return result


def generate_report(payload: Dict[str, Any]) -> str:
    """
    Simulates a longer-running report build.
    Returns a 'report id' or path. Use logs + progress.
    """
    job = get_current_job()
    logger.info("generate_report started", extra={"job_id": job.id if job else None})
    _update_progress(job, 10, "validating")

    # validate payload
    if "data" not in payload or not isinstance(payload["data"], list):
        logger.error("Invalid payload schema")
        raise ValueError("payload.data must be a list")

    batches = max(1, len(payload["data"]))
    for i, item in enumerate(payload["data"], start=1):
        # do something slow
        time.sleep(0.1)
        _update_progress(job, int(10 + (i / batches) * 80), f"processed {i}/{batches}")

    # pretend to write file
    report_id = f"rpt-{job.id if job else int(time.time())}"
    logger.info("generate_report finished", extra={"report_id": report_id})
    _update_progress(job, 100, "done")
    return report_id


def run_chain(chain_input: Dict[str, Any]) -> Dict[str, Any]:
    """
    Placeholder for a LangChain (or similar) LLM pipeline.
    Keep I/O small; don’t pass open DB sessions or large objects.
    """
    job = get_current_job()
    logger.info("run_chain started", extra={"job_id": job.id if job else None})
    _update_progress(job, 5, "init")

    # Example: simulate LLM call
    try:
        prompt = chain_input.get("prompt", "")
        if not prompt:
            raise ValueError("prompt is required")

        # --- call your LLM client here (pseudo) ---
        # response = llm.invoke(prompt)
        time.sleep(0.5)
        response_text = f"[LLM] Reply to: {prompt[:40]}..."
        # ------------------------------------------

        result = {"prompt": prompt, "response": response_text}
        _update_progress(job, 100, "done")
        logger.info("run_chain finished")
        return result

    except Exception as e:
        logger.exception("run_chain failed")
        # re-raise to let RQ mark as FAILED (and possibly retry)
        raise
```

---

## 4) RQ Worker entrypoint

**app/tasks/worker.py**

```python
import os
import logging
import redis
from rq import Worker, Queue, Connection
from app.tasks.logging_config import setup_logging

# Import tasks so the worker process can find them by name
from app.tasks import jobs  # noqa: F401

REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
QUEUES = os.getenv("RQ_QUEUES", "default,high,low").split(",")

def main():
    setup_logging(os.getenv("LOG_LEVEL", "INFO"))
    logger = logging.getLogger(__name__)
    logger.info("Starting RQ worker", extra={"queues": QUEUES, "redis": REDIS_URL})

    conn = redis.from_url(REDIS_URL)
    with Connection(conn):
        worker = Worker([Queue(q) for q in QUEUES])
        worker.work(with_scheduler=True)  # enables scheduled jobs if rq-scheduler is running

if __name__ == "__main__":
    main()
```

**Run the worker (separate process):**

```bash
# Activate your venv first
export REDIS_URL="redis://localhost:6379/0"
python -m app.tasks.worker
```

---

## 5) Enqueueing with retries & timeouts (from your API code)

When you submit jobs (e.g., in `/submit-job`), you can specify **timeouts** and **retries**:

```python
from datetime import timedelta
from rq import Queue
from rq.retry import Retry
import redis
from app.tasks import jobs

conn = redis.from_url("redis://localhost:6379/0")
q = Queue("default", connection=conn)

# Example: process_message
job = q.enqueue(
    jobs.process_message,
    "Hello   world!!",
    job_timeout=60,                      # seconds
    retry=Retry(max=3, interval=[5,15,30]),
    meta={"user_id": 42, "purpose": "demo"}
)

# Example: run_chain
job2 = q.enqueue(
    jobs.run_chain,
    {"prompt": "Summarize effects of vitamin D."},
    job_timeout=180,
    retry=Retry(max=2, interval=10)
)
```

---

## 6) Error handling patterns

* **Let unexpected errors raise** → RQ marks job **FAILED** and applies **Retry** if configured.
* Use **structured logs** (`logger.exception(...)`) so failures are visible in logs.
* For **expected** issues (validation, external API 4xx), raise a **specific exception** (e.g., `ValueError`) and don’t retry.
* Use **`job.meta`** to record extra info (progress, warnings, artifact paths) and call `job.save_meta()`.

**Example: defensive validation**

```python
def safe_divide(a: float, b: float) -> float:
    if b == 0:
        # expected error: don't retry
        raise ValueError("Division by zero")
    return a / b
```

---

## 7) Progress & Job Meta (best practices)

* Call `get_current_job()` **inside** the task.
* Update `job.meta["progress"] = <int>` at logical steps (10/25/60/100).
* Keep meta small (short notes, IDs, URLs/paths), not large blobs.
* **Idempotency:** If the task writes a file, check if it exists and **skip rework** on retries.

---

## 8) Observability checklist

* **Logs**: Level, message, context (`job_id`, `user_id`).
* **Progress**: `%` + a short note (e.g., “normalized”, “sentiment_done”).
* **Artifacts**: Write outputs to a known location, store path in `job.meta["artifact_path"]`.
* **Dead letter**: Monitor **Failed Jobs** (RQ dashboard) and requeue if appropriate.

---

## 9) Quick test (interactive)

In a Python shell:

```python
import redis
from rq import Queue
from app.tasks import jobs

conn = redis.from_url("redis://localhost:6379/0")
q = Queue(connection=conn)

j = q.enqueue(jobs.process_message, "Thanks! This is great.")
print(j.id)
# Check status later via your /job-status endpoint or RQ CLI/UI
```

---

## 10) Recap

* Put worker-safe code in `app/tasks/jobs.py`.
* Run `app/tasks/worker.py` as a **separate process**.
* Use **`job_timeout`** and **`Retry`** at enqueue time.
* Log *liberally* and **update progress** in `job.meta`.

