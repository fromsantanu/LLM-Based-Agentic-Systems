# **Lesson 16: Error Handling and Retry Strategies**

---

### üéØ **Objective**

In this lesson, you'll learn how to make your workflow APIs resilient by handling errors gracefully, retrying failed jobs automatically, and defining custom failure handlers to capture and analyze issues in production.

---

### üß© **1. Understanding the Importance of Error Handling**

Even well-written background jobs can fail due to:

* Network issues (e.g., API timeouts)
* External dependency errors (like a database or LLM API failure)
* Invalid input or missing parameters
* Unexpected exceptions in worker code

Good error handling ensures:

* The system doesn‚Äôt crash on individual job failure.
* Jobs can be retried automatically.
* Developers can monitor failure patterns.

---

### ‚öôÔ∏è **2. Handling Exceptions Gracefully**

Wrap your task logic in try/except blocks:

```python
# worker_tasks.py
import time
from rq import get_current_job

def process_message(message: str):
    job = get_current_job()
    try:
        if not message:
            raise ValueError("Message cannot be empty.")
        # Simulate a transient error
        if "retry" in message:
            raise ConnectionError("Temporary connection issue.")
        
        time.sleep(2)
        return f"Processed: {message}"
    except Exception as e:
        job.meta['error'] = str(e)
        job.save_meta()
        raise e  # Reraise for RQ retry mechanism
```

‚úÖ **Tip:** Always re-raise exceptions after logging them so that retry logic can work properly.

---

### üîÅ **3. Implementing Automatic Retries**

Redis RQ supports automatic retries using the `Retry` class.

```python
from rq import Retry
from redis import Redis
from rq import Queue
from worker_tasks import process_message

redis_conn = Redis()
queue = Queue('default', connection=redis_conn)

# Retry failed jobs up to 3 times with exponential backoff
job = queue.enqueue(
    process_message,
    "retry this message",
    retry=Retry(max=3, interval=[10, 30, 60])
)
```

üïí **Explanation:**

* `max=3`: maximum retries.
* `interval=[10, 30, 60]`: wait 10s before first retry, 30s before second, and 60s before third.

---

### üß† **4. Adding Custom Failure Handlers**

You can define a custom handler to perform specific actions when a job fails.

```python
# failure_handler.py
from rq import get_current_job
from datetime import datetime

def custom_failure_handler(job, exc_type, exc_value, traceback):
    error_info = {
        "job_id": job.id,
        "failed_at": datetime.utcnow().isoformat(),
        "error_type": str(exc_type),
        "error_message": str(exc_value)
    }
    print(f"‚ö†Ô∏è Job failed: {error_info}")

    # Optionally save to a database or send an alert
    with open("failed_jobs.log", "a") as f:
        f.write(str(error_info) + "\n")
```

Then attach it when enqueuing:

```python
job = queue.enqueue(
    process_message,
    "retry this message",
    on_failure=custom_failure_handler
)
```

---

### üßæ **5. Retry + Logging Example (ARQ)**

If using **ARQ (async worker)**:

```python
# arq_tasks.py
import asyncio
from arq import Retry

async def process_task(ctx, message: str):
    try:
        if "error" in message:
            raise ValueError("Bad input detected.")
        await asyncio.sleep(1)
        return f"Processed message: {message}"
    except Exception as e:
        print(f"Error occurred: {e}")
        raise Retry(defer=5)  # Retry after 5 seconds
```

---

### üìä **6. Monitoring Failures**

You can use:

* `rq info` CLI to check failed jobs.
* A custom `/failed-jobs` API endpoint to fetch failed job metadata.
* Logs for analysis and alerting (Slack/email integration).

---

### üß± **7. Best Practices**

* Keep retry counts reasonable (3‚Äì5 times).
* Use exponential backoff to avoid overloading servers.
* Always log both success and failure outcomes.
* Store error metadata in Redis or a database for post-mortem analysis.
* Use circuit breakers (optional) to stop retries when a service is down.

---

### ‚úÖ **Summary**

| Feature            | Description                               | Example                               |
| ------------------ | ----------------------------------------- | ------------------------------------- |
| Graceful Exception | Catch and re-raise exceptions for logging | `try/except`                          |
| Auto Retry         | Automatically re-queue failed jobs        | `Retry(max=3, interval=[10, 30, 60])` |
| Custom Handler     | Execute custom code when a job fails      | `on_failure=custom_failure_handler`   |

---

