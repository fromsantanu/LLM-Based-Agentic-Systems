# Lesson 8: Fetching Job Status

## What you’ll build

* A `GET /status/{job_id}` endpoint that:

  * looks up an RQ job by `job_id`
  * returns its **status**, **progress**, **result** (if finished), or **error** (if failed)
* A simple client-side **polling** pattern to track progress until completion.

---

## RQ job lifecycle (recap)

`queued → started → finished → failed` (and sometimes `deferred` if dependencies).
Each job can store progress in `job.meta["progress"] = 0..100` (you must call `job.save_meta()`).

---

## 1) Server code: FastAPI `/status/{job_id}`

Create (or update) `app/rq_utils.py`:

```python
# app/rq_utils.py
from redis import Redis
from rq import Queue
from rq.job import Job

def get_redis() -> Redis:
    # Or read REDIS_URL from env if you prefer
    return Redis.from_url("redis://localhost:6379/0", decode_responses=True)

def get_queue(name: str = "default") -> Queue:
    return Queue(name, connection=get_redis())

def get_job(job_id: str) -> Job | None:
    try:
        return Job.fetch(job_id, connection=get_redis())
    except Exception:
        return None
```

Add response models in `app/schemas.py`:

```python
# app/schemas.py
from pydantic import BaseModel
from typing import Any, Optional, Literal

JobStatusLiteral = Literal["queued","started","finished","failed","deferred","canceled","stopped","unknown"]

class JobStatusResponse(BaseModel):
    job_id: str
    status: JobStatusLiteral
    progress: int | None = None               # 0..100 if provided by worker
    result: Optional[Any] = None              # present when finished (if result_ttl not expired)
    error: Optional[str] = None               # present when failed
    enqueued_at: Optional[str] = None
    started_at: Optional[str] = None
    ended_at: Optional[str] = None
```

Add the route in `app/main.py` (or `routing.py` if you split):

```python
# app/main.py
from fastapi import FastAPI, HTTPException
from rq.exceptions import NoSuchJobError
from app.schemas import JobStatusResponse
from app.rq_utils import get_job

app = FastAPI(title="Workflow APIs")

def _status_from_rq(job) -> str:
    # RQ returns strings like 'queued','started','finished','failed','deferred'
    return job.get_status() if job else "unknown"

def _safe_str(x):
    return x.isoformat() if x else None

@app.get("/status/{job_id}", response_model=JobStatusResponse)
def get_status(job_id: str):
    job = get_job(job_id)
    if job is None:
        # could be nonexistent or expired from Redis (result_ttl passed)
        return JobStatusResponse(job_id=job_id, status="unknown")

    status = _status_from_rq(job)
    meta = job.meta or {}
    progress = meta.get("progress")

    # Try to get result or error
    result = None
    err = None
    if status == "finished":
        # result may be None AND may have been cleaned (result_ttl)
        result = job.result
    elif status == "failed":
        # Best-effort error message
        if job.exc_string:
            # exc_string is the full traceback; trim if you like
            err = job.exc_string.splitlines()[-1].strip()
        else:
            err = "Job failed."

    return JobStatusResponse(
        job_id=job_id,
        status=status,
        progress=progress,
        result=result,
        error=err,
        enqueued_at=_safe_str(job.enqueued_at),
        started_at=_safe_str(job.started_at),
        ended_at=_safe_str(job.ended_at),
    )
```

### Recommended HTTP semantics

* **200 OK** for all lookups (even unknown IDs) with `status="unknown"`.
  Alternatively you can `404` unknown — but `200` makes polling simpler.

---

## 2) Worker: reporting progress + errors

In **Lesson 6**, your worker functions run inside an RQ worker. Update them to report progress:

```python
# worker_tasks.py
import time
from rq import get_current_job

def long_task(n: int) -> dict:
    job = get_current_job()
    # optional: initialize
    job.meta["progress"] = 0
    job.save_meta()

    acc = 0
    for i in range(n):
        # ... do work ...
        time.sleep(0.1)
        acc += i

        pct = int((i + 1) * 100 / n)
        job.meta["progress"] = pct
        job.save_meta()

    return {"sum": acc, "n": n}
```

**Note:** Always call `job.save_meta()` after updating `job.meta`.

When you enqueue (Lesson 5), consider:

```python
job = queue.enqueue(long_task, 200, result_ttl=3600, job_timeout=600)
```

* `result_ttl`: how long the result is kept (seconds) after finishing
* `job_timeout`: max seconds before worker kills the job

---

## 3) Client-side polling patterns

### A) Minimal JavaScript (browser/app)

```html
<script>
async function pollStatus(jobId, onUpdate) {
  const maxAttempts = 60;     // ~60s if interval=1000ms
  const interval = 1000;

  for (let attempt = 1; attempt <= maxAttempts; attempt++) {
    const res = await fetch(`/status/${jobId}`);
    const data = await res.json();
    onUpdate?.(data); // {status, progress, result, error,...}

    if (data.status === "finished" || data.status === "failed") {
      return data;
    }
    await new Promise(r => setTimeout(r, interval));
  }
  throw new Error("Polling timed out");
}

// Example usage:
pollStatus("abcd1234", (s) => {
  if (s.progress != null) {
    console.log(`Progress: ${s.progress}%`);
  } else {
    console.log(`Status: ${s.status}`);
  }
}).then(final => {
  if (final.status === "finished") console.log("Result:", final.result);
  else console.error("Failed:", final.error);
});
</script>
```

### B) JS with exponential backoff (reduces server load)

```js
async function pollWithBackoff(jobId) {
  let delay = 500; // start at 0.5s
  const maxDelay = 5000;

  while (true) {
    const res = await fetch(`/status/${jobId}`);
    const s = await res.json();

    if (s.status === "finished" || s.status === "failed") return s;

    await new Promise(r => setTimeout(r, delay));
    delay = Math.min(delay * 1.5, maxDelay);
  }
}
```

### C) Python client (requests)

```python
import time, requests

def poll(job_id: str, base="http://localhost:8000", timeout=60):
    start = time.time()
    delay = 0.5
    while True:
        r = requests.get(f"{base}/status/{job_id}")
        s = r.json()
        if s["status"] in ("finished","failed"):
            return s
        if time.time() - start > timeout:
            raise TimeoutError("Polling timed out")
        time.sleep(delay)
        delay = min(delay * 1.5, 5.0)
```

---

## 4) CURL for quick tests

```bash
# Look up a job
curl -s http://localhost:8000/status/abcd1234 | jq

# Typical finished payload
# {
#   "job_id":"abcd1234",
#   "status":"finished",
#   "progress":100,
#   "result":{"sum":19900,"n":200},
#   "error":null,
#   "enqueued_at":"2025-10-28T09:12:30.000000",
#   "started_at":"2025-10-28T09:12:31.100000",
#   "ended_at":"2025-10-28T09:12:51.400000"
# }
```

---

## 5) Edge cases & production tips

* **Unknown/expired job_id**: return `status="unknown"` (or 404). Document result TTL so clients know how long they can fetch results.
* **No progress**: If your worker never sets `job.meta["progress"]`, clients should fall back to status text.
* **Large results**: Prefer storing the output elsewhere (DB/object storage) and return a reference/URL in `result`.
* **Security**: If job results contain sensitive data, protect `/status` with auth and ensure `job_id`s aren’t guessable (RQ uses UUID-like IDs, which is fine).
* **Backoff**: Clients should ramp up intervals (e.g., 0.5s → 1s → 2s → 5s) to reduce load.
* **CORS**: If polled from a browser app on a different origin, enable CORS middleware.

---

## 6) Optional: unify status payloads

If you also use ARQ/Dramatiq later, keep the response schema **identical** and translate engine-specific states to your canonical set (`queued/started/finished/failed/...`). This lets the front-end poll a single shape regardless of backend.

---

### You’re done!

You now have:

* a robust `/status/{job_id}` endpoint
* workers reporting progress
* client polling patterns with backoff


