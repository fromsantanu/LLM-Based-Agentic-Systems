# Lesson 3: Creating the Base FastAPI Application

**Goal:** Stand up a tiny FastAPI service with two workflow endpoints:

* `POST /submit-job` → accepts JSON, returns a `job_id` and `"Submitted"`
* `POST /job-status` → accepts a `job_id`, returns the latest status

We’ll keep storage **in-memory** for now (so it’s easy to run). In later lessons, you can swap this with **Redis + RQ/ARQ**.

---

## 1) Minimal project structure

```
fastapi-app/
  app/
    __init__.py
    main.py
    settings.py
    schemas.py
    services/
      __init__.py
      jobs.py
  .env.example
  pyproject.toml  (or requirements.txt)
```

> If you already created this in Lesson 2, just add the new files below.

---

## 2) Pydantic schemas (request/response models)

`app/schemas.py`

```python
from typing import Literal, Optional
from pydantic import BaseModel, Field

# ---------- Submit Job ----------
class SubmitJobRequest(BaseModel):
    payload: dict = Field(default_factory=dict, description="Arbitrary input for the workflow")

class SubmitJobResponse(BaseModel):
    job_id: str
    status: Literal["Submitted"]

# ---------- Job Status ----------
class JobStatusRequest(BaseModel):
    job_id: str

class JobStatusResponse(BaseModel):
    job_id: str
    status: Literal["Submitted", "Running", "Completed", "Failed"]
    result: Optional[dict] = None
    error: Optional[str] = None
```

---

## 3) A tiny job “service” (in-memory for now)

`app/services/jobs.py`

```python
import time
import uuid
from typing import Dict, Literal, Optional

JobStatus = Literal["Submitted", "Running", "Completed", "Failed"]

class InMemoryJobStore:
    """
    Simple demo store. Replace with Redis later.
    """
    def __init__(self) -> None:
        self._jobs: Dict[str, dict] = {}

    def create_job(self, payload: dict) -> str:
        job_id = f"JOB-{int(time.time()*1000)}-{uuid.uuid4().hex[:6].upper()}"
        self._jobs[job_id] = {
            "status": "Submitted",
            "payload": payload,
            "result": None,
            "error": None,
            "created_at": time.time(),
        }
        return job_id

    def get_job(self, job_id: str) -> Optional[dict]:
        return self._jobs.get(job_id)

    def set_status(
        self,
        job_id: str,
        status: JobStatus,
        *,
        result: Optional[dict] = None,
        error: Optional[str] = None
    ) -> None:
        job = self._jobs.get(job_id)
        if not job:
            return
        job["status"] = status
        job["result"] = result
        job["error"] = error


# Singleton-ish store for now (process local)
job_store = InMemoryJobStore()
```

> Later, your worker will `set_status(job_id, "Running"/"Completed"/"Failed", ...)` from a background process.

---

## 4) Settings (optional, but good habit)

`app/settings.py`

```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    APP_NAME: str = "Workflow API"
    # Placeholder for future lessons:
    # REDIS_URL: str = "redis://localhost:6379/0"

    class Config:
        env_file = ".env"

settings = Settings()
```

`.env.example`

```
APP_NAME="Workflow API"
# REDIS_URL="redis://localhost:6379/0"
```

---

## 5) FastAPI app with the two endpoints

`app/main.py`

```python
from fastapi import FastAPI, HTTPException
from app.settings import settings
from app.schemas import (
    SubmitJobRequest, SubmitJobResponse,
    JobStatusRequest, JobStatusResponse
)
from app.services.jobs import job_store

app = FastAPI(title=settings.APP_NAME)

@app.get("/health")
def health():
    return {"ok": True}

@app.post("/submit-job", response_model=SubmitJobResponse)
def submit_job(req: SubmitJobRequest):
    """
    Accepts a JSON payload, returns a job_id with 'Submitted' status.
    """
    job_id = job_store.create_job(req.payload)
    # In later lessons, enqueue the job to RQ/ARQ here.
    return {"job_id": job_id, "status": "Submitted"}

@app.post("/job-status", response_model=JobStatusResponse)
def job_status(req: JobStatusRequest):
    """
    Accepts a job_id and returns the latest status/result.
    """
    job = job_store.get_job(req.job_id)
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")

    return {
        "job_id": req.job_id,
        "status": job["status"],
        "result": job["result"],
        "error": job["error"],
    }
```

---

## 6) Run it

```bash
uvicorn app.main:app --reload
# Server: http://127.0.0.1:8000
```

Open the interactive docs at `http://127.0.0.1:8000/docs`.

---

## 7) Quick tests (curl)

**Submit a job**

```bash
curl -s -X POST http://127.0.0.1:8000/submit-job \
  -H "Content-Type: application/json" \
  -d '{"payload":{"patient_id":"101","note":"gap-fill inputs"}}'
```

**Response**

```json
{"job_id":"JOB-173...-AB12CD","status":"Submitted"}
```

**Check job status**

```bash
curl -s -X POST http://127.0.0.1:8000/job-status \
  -H "Content-Type: application/json" \
  -d '{"job_id":"JOB-173...-AB12CD"}'
```

**Response (initially)**

```json
{"job_id":"JOB-173...-AB12CD","status":"Submitted","result":null,"error":null}
```

---

## 8) What you have now (and what’s next)

* Clean **request/response** validation using Pydantic.
* Two core workflow endpoints that your **Streamlit UI** or any client can call.
* A replaceable **job store** (in-memory today → **Redis** + RQ/ARQ next).


