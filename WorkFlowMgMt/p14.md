# Lesson 14: Storing Job Metadata

**Goal:** Persist every job’s lifecycle (enqueue ➜ start ➜ progress ➜ finish/fail/cancel) so you can inspect history, resume work, and compute analytics.

---

## 1) What to store (minimum viable metadata)

* `job_id` (str, pk)
* `user_id` (str | UUID | nullable)
* `queue` (str)
* `status` (`queued|started|paused|finished|failed|canceled`)
* `params` (JSON, redacted of secrets)
* `result` (JSON | nullable)
* `error` (text | nullable)
* Timestamps:

  * `created_at`, `enqueued_at`, `started_at`, `finished_at`, `failed_at`, `canceled_at`
* Durations (derived):

  * `wait_ms` = started_at − enqueued_at
  * `run_ms` = finished_at − started_at (or failed_at − started_at)
* `retries` (int), `max_retries` (int)
* `progress_pct` (0–100), `tags` (array), `parent_id` (for chaining), `trace_id` (for tracing)

> Tip: Keep *params* and *result* small; for bulky payloads store a pointer (e.g., S3 key) rather than the blob.

---

## 2) Storage choices: Redis vs SQL (or both)

* **Redis only**

  * Pros: blazing-fast writes, simple to use with RQ/ARQ, ephemeral.
  * Cons: limited querying/joins; analytics harder unless you roll your own indexes.
* **SQL only**

  * Pros: durability, rich queries, analytics-friendly, FK to `users`.
  * Cons: slightly more plumbing from workers.
* **Hybrid (recommended)**

  * Redis = live runtime state (progress, heartbeats).
  * SQL = durable history and analytics.

---

## 3) SQL schema (PostgreSQL example)

```sql
-- jobs table
CREATE TABLE jobs (
  job_id        TEXT PRIMARY KEY,
  user_id       TEXT,
  queue         TEXT NOT NULL,
  status        TEXT NOT NULL CHECK (status IN
                ('queued','started','paused','finished','failed','canceled')),
  params        JSONB,
  result        JSONB,
  error         TEXT,
  retries       INT NOT NULL DEFAULT 0,
  max_retries   INT NOT NULL DEFAULT 0,
  progress_pct  INT NOT NULL DEFAULT 0,
  parent_id     TEXT REFERENCES jobs(job_id) ON DELETE SET NULL,
  trace_id      TEXT,
  created_at    TIMESTAMPTZ NOT NULL DEFAULT now(),
  enqueued_at   TIMESTAMPTZ,
  started_at    TIMESTAMPTZ,
  finished_at   TIMESTAMPTZ,
  failed_at     TIMESTAMPTZ,
  canceled_at   TIMESTAMPTZ
);

-- Optional append-only event log for auditing and analytics
CREATE TABLE job_events (
  id           BIGSERIAL PRIMARY KEY,
  job_id       TEXT NOT NULL REFERENCES jobs(job_id) ON DELETE CASCADE,
  event_type   TEXT NOT NULL,           -- enqueued|started|progress|finished|failed|canceled|retry
  payload      JSONB,                   -- e.g., {"progress_pct": 35}
  created_at   TIMESTAMPTZ NOT NULL DEFAULT now()
);

-- Helpful indexes
CREATE INDEX idx_jobs_status ON jobs(status);
CREATE INDEX idx_jobs_user   ON jobs(user_id);
CREATE INDEX idx_events_job  ON job_events(job_id);
CREATE INDEX idx_jobs_created ON jobs(created_at);
```

**Derived metrics view (optional):**

```sql
CREATE VIEW job_metrics AS
SELECT
  job_id,
  EXTRACT(EPOCH FROM (started_at - enqueued_at))*1000 AS wait_ms,
  EXTRACT(EPOCH FROM (
    COALESCE(finished_at, failed_at, canceled_at) - started_at))*1000 AS run_ms
FROM jobs;
```

---

## 4) Redis structures (if you keep a live mirror)

Use a **Redis Hash** per job and a **Sorted Set** per queue for quick scans.

```
HSET job:{job_id} status started_at 2025-10-28T05:31:00Z progress_pct 40 ...
ZADD queue:{queue}:updated {epoch_ms} {job_id}
```

Progress updates from workers:

```
HSET job:{job_id} progress_pct 65 updated_at 2025-10-28T05:33:12Z
```

Optional TTL for ephemeral keys:

```
EXPIRE job:{job_id} 604800  # 7 days (history still in SQL)
```

---

## 5) FastAPI models & CRUD

**Pydantic schemas**

```python
# app/schemas.py
from pydantic import BaseModel, Field
from typing import Optional, Any, Literal, List
from datetime import datetime

Status = Literal['queued','started','paused','finished','failed','canceled']

class JobCreate(BaseModel):
    user_id: Optional[str] = None
    queue: str = "default"
    params: dict = Field(default_factory=dict)
    tags: List[str] = Field(default_factory=list)
    max_retries: int = 0

class JobOut(BaseModel):
    job_id: str
    user_id: Optional[str]
    queue: str
    status: Status
    progress_pct: int
    retries: int
    max_retries: int
    params: dict
    result: Optional[dict]
    error: Optional[str]
    created_at: datetime
    enqueued_at: Optional[datetime]
    started_at: Optional[datetime]
    finished_at: Optional[datetime]
    failed_at: Optional[datetime]
    canceled_at: Optional[datetime]
```

**SQLAlchemy model**

```python
# app/models.py
from sqlalchemy.orm import Mapped, mapped_column
from sqlalchemy import String, Integer, Text, JSON, ForeignKey, CheckConstraint
from sqlalchemy.sql import func
from sqlalchemy.dialects.postgresql import JSONB
from datetime import datetime

class Job(Base):
    __tablename__ = "jobs"
    job_id: Mapped[str] = mapped_column(String, primary_key=True)
    user_id: Mapped[str | None] = mapped_column(String, nullable=True)
    queue: Mapped[str] = mapped_column(String, nullable=False)
    status: Mapped[str] = mapped_column(String, nullable=False)
    params: Mapped[dict | None] = mapped_column(JSONB, nullable=True)
    result: Mapped[dict | None] = mapped_column(JSONB, nullable=True)
    error: Mapped[str | None] = mapped_column(Text, nullable=True)
    retries: Mapped[int] = mapped_column(Integer, default=0)
    max_retries: Mapped[int] = mapped_column(Integer, default=0)
    progress_pct: Mapped[int] = mapped_column(Integer, default=0)
    parent_id: Mapped[str | None] = mapped_column(String, ForeignKey("jobs.job_id"))
    trace_id: Mapped[str | None] = mapped_column(String, nullable=True)
    created_at: Mapped[datetime] = mapped_column(server_default=func.now())
    enqueued_at: Mapped[datetime | None]
    started_at: Mapped[datetime | None]
    finished_at: Mapped[datetime | None]
    failed_at: Mapped[datetime | None]
    canceled_at: Mapped[datetime | None]

    __table_args__ = (
        CheckConstraint("status IN ('queued','started','paused','finished','failed','canceled')"),
    )
```

---

## 6) Wiring it in: enqueue ➜ update ➜ finish

**On submission (FastAPI route)**

```python
# app/routes.py
from fastapi import APIRouter, Depends
from uuid import uuid4
from datetime import datetime, timezone
from .schemas import JobCreate, JobOut
from .models import Job
from .db import SessionDep
from .workers import enqueue_job  # your RQ/ARQ wrapper

router = APIRouter()

@router.post("/jobs", response_model=JobOut)
def submit_job(payload: JobCreate, db: SessionDep):
    job_id = str(uuid4())
    now = datetime.now(timezone.utc)
    # 1) Persist in SQL
    job = Job(
        job_id=job_id, user_id=payload.user_id, queue=payload.queue,
        status="queued", params=payload.params, max_retries=payload.max_retries,
        created_at=now, enqueued_at=now
    )
    db.add(job); db.commit(); db.refresh(job)

    # 2) Enqueue to background system
    enqueue_job(job_id=job_id, queue=payload.queue, params=payload.params)

    return job
```

**Worker hooks (RQ example)**

```python
# app/workers.py
from rq import Queue
from redis import Redis
from sqlalchemy.orm import Session
from contextlib import contextmanager
from datetime import datetime, timezone
from .db import SessionLocal
from .models import Job

redis = Redis.from_url("redis://localhost:6379")
q = Queue("default", connection=redis)

def enqueue_job(job_id: str, queue: str, params: dict):
    queue_obj = Queue(queue, connection=redis)
    queue_obj.enqueue(process_task, job_id, params, job_id=job_id, on_success=on_success, on_failure=on_failure)

@contextmanager
def db_session():
    db: Session = SessionLocal()
    try:
        yield db
    finally:
        db.close()

def mark_started(job_id: str):
    with db_session() as db:
        job = db.get(Job, job_id)
        job.status = "started"
        job.started_at = datetime.now(timezone.utc)
        db.commit()

def mark_progress(job_id: str, pct: int):
    with db_session() as db:
        job = db.get(Job, job_id)
        job.progress_pct = pct
        db.commit()

def mark_finished(job_id: str, result: dict):
    with db_session() as db:
        job = db.get(Job, job_id)
        job.status = "finished"
        job.result = result
        job.finished_at = datetime.now(timezone.utc)
        db.commit()

def mark_failed(job_id: str, error: str):
    with db_session() as db:
        job = db.get(Job, job_id)
        job.status = "failed"
        job.error = error[:4000]
        job.failed_at = datetime.now(timezone.utc)
        db.commit()

# your actual job
def process_task(job_id: str, params: dict):
    mark_started(job_id)
    # ... do work, optionally call mark_progress(job_id, x)
    result = {"ok": True, "summary": "done"}
    mark_finished(job_id, result)
    return result

def on_success(job, connection, result, *args, **kwargs):
    # already marked; noop or double-check
    return

def on_failure(job, connection, type, value, traceback):
    mark_failed(job.id, f"{type.__name__}: {value}")
```

**ARQ (async) sketch:** call similar `mark_*` helpers inside the coroutine; use `ctx['db']` or session factory.

---

## 7) Reading history & status

```python
@router.get("/jobs/{job_id}", response_model=JobOut)
def get_job(job_id: str, db: SessionDep):
    job = db.get(Job, job_id)
    if not job: raise HTTPException(404, "job not found")
    return job

@router.get("/jobs", response_model=list[JobOut])
def list_jobs(
    user_id: str | None = None,
    status: str | None = None,
    limit: int = 50,
    db: SessionDep = Depends(...)
):
    q = db.query(Job).order_by(Job.created_at.desc())
    if user_id: q = q.filter(Job.user_id == user_id)
    if status:  q = q.filter(Job.status == status)
    return q.limit(limit).all()
```

---

## 8) Analytics you can compute

**Key metrics**

* Throughput: jobs/day by `status='finished'`
* Success rate: finished / (finished + failed) over a window
* P95 run time: percentile of `run_ms`
* Queue wait time: avg/percentile of `wait_ms`
* Retry rate: avg `retries` per job
* Error hotspots: top N error messages (normalized)
* Per-queue or per-user breakdowns

**Example SQL snippets**

Jobs per day:

```sql
SELECT date_trunc('day', finished_at) AS day, COUNT(*) AS jobs_done
FROM jobs
WHERE finished_at IS NOT NULL
GROUP BY 1 ORDER BY 1 DESC LIMIT 30;
```

Success rate (last 7 days):

```sql
WITH base AS (
  SELECT status FROM jobs
  WHERE created_at >= now() - interval '7 days'
)
SELECT
  sum(CASE WHEN status='finished' THEN 1 ELSE 0 END)::float / NULLIF(count(*),0) AS success_rate
FROM base;
```

P95 run time:

```sql
SELECT
  PERCENTILE_CONT(0.95) WITHIN GROUP (
    ORDER BY EXTRACT(EPOCH FROM (COALESCE(finished_at, failed_at) - started_at))*1000
  ) AS p95_run_ms
FROM jobs
WHERE started_at IS NOT NULL AND (finished_at IS NOT NULL OR failed_at IS NOT NULL);
```

Average wait by queue:

```sql
SELECT queue,
  AVG(EXTRACT(EPOCH FROM (started_at - enqueued_at))*1000) AS avg_wait_ms
FROM jobs
WHERE started_at IS NOT NULL
GROUP BY queue ORDER BY avg_wait_ms DESC;
```

Top error signatures:

```sql
SELECT LEFT(error, 120) AS error_head, COUNT(*) AS n
FROM jobs
WHERE status='failed' AND error IS NOT NULL
GROUP BY 1 ORDER BY n DESC LIMIT 10;
```

---

## 9) API surface for analytics (FastAPI)

```python
class AnalyticsOut(BaseModel):
    from_ts: datetime
    to_ts: datetime
    success_rate: float
    total_finished: int
    total_failed: int
    p95_run_ms: float
    avg_wait_ms_by_queue: dict[str, float]

@router.get("/analytics/summary", response_model=AnalyticsOut)
def analytics_summary(days: int = 7, db: SessionDep = Depends(...)):
    # run the SQL above (or ORM equivalents), assemble into the schema
    ...
```

---

## 10) Operational tips & best practices

* **Idempotent writes:** workers should safely re-run status updates (e.g., if retried).
* **Atomic transitions:** enforce valid transitions (e.g., cannot go `finished ➜ started`).
* **Redaction:** strip API keys / PHI from `params` and `result` before persisting.
* **Retention policy:** e.g., keep 90 days in SQL, compact events > 30 days, TTL Redis keys after 7–14 days.
* **Multi-tenant:** always filter by `tenant_id`/`owner_id` (add column if you offer subscriptions).
* **Indexes:** add `(status, created_at)` and `(user_id, created_at)` for common queries.
* **Backfill script:** create a one-off tool to migrate legacy Redis-only history into SQL.
* **Tracing:** propagate `trace_id` (OpenTelemetry) from request ➜ queue ➜ worker.
* **Alerts:** trigger on **spikes in failures**, **P95 run time**, **queue backlog length**.

---

## 11) Minimal Alembic migration stub

```python
# alembic/versions/20251028_141400_create_jobs.py
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

revision = "20251028_141400"
down_revision = None

def upgrade():
    op.create_table(
        "jobs",
        sa.Column("job_id", sa.String(), primary_key=True),
        sa.Column("user_id", sa.String(), nullable=True),
        sa.Column("queue", sa.String(), nullable=False),
        sa.Column("status", sa.String(), nullable=False),
        sa.Column("params", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column("result", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column("error", sa.Text(), nullable=True),
        sa.Column("retries", sa.Integer(), server_default="0", nullable=False),
        sa.Column("max_retries", sa.Integer(), server_default="0", nullable=False),
        sa.Column("progress_pct", sa.Integer(), server_default="0", nullable=False),
        sa.Column("parent_id", sa.String(), sa.ForeignKey("jobs.job_id")),
        sa.Column("trace_id", sa.String(), nullable=True),
        sa.Column("created_at", sa.DateTime(timezone=True), server_default=sa.text("now()")),
        sa.Column("enqueued_at", sa.DateTime(timezone=True)),
        sa.Column("started_at", sa.DateTime(timezone=True)),
        sa.Column("finished_at", sa.DateTime(timezone=True)),
        sa.Column("failed_at", sa.DateTime(timezone=True)),
        sa.Column("canceled_at", sa.DateTime(timezone=True)),
        sa.CheckConstraint("status IN ('queued','started','paused','finished','failed','canceled')"),
    )
    op.create_table(
        "job_events",
        sa.Column("id", sa.BigInteger(), primary_key=True, autoincrement=True),
        sa.Column("job_id", sa.String(), sa.ForeignKey("jobs.job_id", ondelete="CASCADE")),
        sa.Column("event_type", sa.String(), nullable=False),
        sa.Column("payload", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column("created_at", sa.DateTime(timezone=True), server_default=sa.text("now()")),
    )
    op.create_index("idx_jobs_status", "jobs", ["status"])
    op.create_index("idx_jobs_user", "jobs", ["user_id"])
    op.create_index("idx_jobs_created", "jobs", ["created_at"])
    op.create_index("idx_events_job", "job_events", ["job_id"])

def downgrade():
    op.drop_index("idx_events_job", table_name="job_events")
    op.drop_index("idx_jobs_created", table_name="jobs")
    op.drop_index("idx_jobs_user", table_name="jobs")
    op.drop_index("idx_jobs_status", table_name="jobs")
    op.drop_table("job_events")
    op.drop_table("jobs")
```

---

## 12) Testing checklist

* Submit job ➜ verify row in `jobs` (`status=queued`, timestamps set).
* Worker starts ➜ `status=started`, `started_at` set, progress updates persist.
* Success path ➜ `status=finished`, `result` saved, `finished_at` set.
* Failure path ➜ `status=failed`, `error` saved, `failed_at` set.
* Retry path ➜ `retries` increments; events logged.
* Analytics endpoints return correct aggregates for seeded fixtures.

---

## 13) Takeaway

* Use **Redis** for **live state**; **SQL** for **history + analytics**.
* Normalize your **status transitions** and **timestamps**.
* Build a small **analytics API** early—your ops (and users) will thank you later.

