# Lesson 13: Pause, Resume, and Cancel Logic

## Learning goals

* Design **cooperative pause/resume** for long-running jobs.
* **Cancel** queued or running jobs safely.
* Model and enforce **job state transitions** to avoid race conditions.

---

## Key ideas

1. **Cooperative control (recommended)**

   * Store control flags in Redis (e.g., `pause`, `cancel`).
   * Tasks periodically **checkpoint** and check flags (every N items or seconds).
   * On `pause`, the task saves progress and returns a **PAUSED** state (or loops in a sleep until `resume`).
   * On `cancel`, the task stops quickly and marks **CANCELED**.

2. **Queue-level vs job-level**

   * **Queued job cancel** is straightforward (remove from queue).
   * **Running job** must cooperate (you can’t “pause a thread” safely).
   * For global maintenance, RQ supports **worker suspension** (pauses *fetching* new jobs), but that’s different from per-job pause.

3. **State machine (idempotent)**

   ```
   QUEUED -> STARTED -> [PAUSE_REQUESTED] -> PAUSED -> RESUME_REQUESTED -> STARTED
                         \-> CANCEL_REQUESTED -> CANCELED
   STARTED -> FINISHED | FAILED
   ```

   * Only allow valid transitions. Reject or no-op invalid ones.

---

## Minimal Redis schema (keys)

For each job `job_id`:

* `job:{id}:ctrl` (HASH):

  * `pause`: `"0" | "1"`
  * `cancel`: `"0" | "1"`
* `job:{id}:meta` (HASH):

  * `state`: `"QUEUED" | "STARTED" | "PAUSED" | "CANCELED" | "FINISHED" | "FAILED"`
  * `progress`: `0..100`
  * `last_checkpoint`: ISO timestamp
  * `payload_version`: optional for schema evolution

You can store these in **RQ `job.meta`** or in your own Redis keys. Using your own keys works for both **RQ** and **ARQ**.

---

## REST API (FastAPI)

Routes:

* `POST /jobs/{job_id}/pause`   → set `pause=1`, transition to `PAUSE_REQUESTED`
* `POST /jobs/{job_id}/resume`  → set `pause=0`, transition to `RESUME_REQUESTED`
* `POST /jobs/{job_id}/cancel`  → set `cancel=1`, transition to `CANCEL_REQUESTED`
* `GET  /jobs/{job_id}/status`  → read `state`, `progress`, etc.

### Pydantic models

```python
# app/schemas.py
from pydantic import BaseModel
from typing import Optional

class JobStatus(BaseModel):
    job_id: str
    state: str
    progress: int
    detail: Optional[str] = None
```

### State helpers

```python
# app/workflows/state.py
from enum import Enum
import time

class JobState(str, Enum):
    QUEUED="QUEUED"
    STARTED="STARTED"
    PAUSE_REQUESTED="PAUSE_REQUESTED"
    PAUSED="PAUSED"
    RESUME_REQUESTED="RESUME_REQUESTED"
    CANCEL_REQUESTED="CANCEL_REQUESTED"
    CANCELED="CANCELED"
    FINISHED="FINISHED"
    FAILED="FAILED"

VALID_TRANSITIONS = {
    "QUEUED": { "STARTED", "CANCEL_REQUESTED" },
    "STARTED": { "PAUSE_REQUESTED", "CANCEL_REQUESTED", "FINISHED", "FAILED" },
    "PAUSE_REQUESTED": { "PAUSED", "CANCEL_REQUESTED" },
    "PAUSED": { "RESUME_REQUESTED", "CANCEL_REQUESTED" },
    "RESUME_REQUESTED": { "STARTED", "CANCEL_REQUESTED" },
    "CANCEL_REQUESTED": { "CANCELED" },
    "CANCELED": set(),
    "FINISHED": set(),
    "FAILED": set(),
}

def can_transition(old: JobState, new: JobState) -> bool:
    return new in VALID_TRANSITIONS[old]
```

---

## Worker-side cooperative pattern (RQ, sync)

```python
# app/workers/rq_tasks.py
import time
from redis import Redis
from rq import get_current_job
from app.workflows.state import JobState, can_transition

r = Redis.from_url("redis://localhost:6379")

def _ctrl_key(job_id): return f"job:{job_id}:ctrl"
def _meta_key(job_id): return f"job:{job_id}:meta"

def _set_state(job_id, new_state):
    r.hset(_meta_key(job_id), "state", new_state)

def _set_progress(job_id, pct):
    r.hset(_meta_key(job_id), "progress", int(pct))
    r.hset(_meta_key(job_id), "last_checkpoint", str(time.time()))

def _get_flag(job_id, flag):
    return r.hget(_ctrl_key(job_id), flag) == b"1"

def long_job(payload: dict):
    job = get_current_job()
    job_id = job.id

    # START
    _set_state(job_id, JobState.STARTED)

    items = payload.get("items", list(range(1000)))
    total = len(items)
    for idx, item in enumerate(items, start=1):
        # periodic checkpoint (every 25 items)
        if idx % 25 == 0:
            # check cancel
            if _get_flag(job_id, "cancel"):
                _set_state(job_id, JobState.CANCEL_REQUESTED)
                # do any cleanup here...
                _set_state(job_id, JobState.CANCELED)
                return {"status": "CANCELED", "processed": idx-1}

            # check pause
            if _get_flag(job_id, "pause"):
                _set_state(job_id, JobState.PAUSE_REQUESTED)
                # Option A: exit early as PAUSED (persist progress; client can re-enqueue with resume)
                _set_state(job_id, JobState.PAUSED)
                return {"status": "PAUSED", "processed": idx-1}

                # Option B (alternate): loop/sleep until pause cleared (comment Option A and use below)
                # while _get_flag(job_id, "pause") and not _get_flag(job_id, "cancel"):
                #     time.sleep(1)
                # _set_state(job_id, JobState.RESUME_REQUESTED)
                # _set_state(job_id, JobState.STARTED)

        # simulate work on item
        time.sleep(0.02)  # replace with real processing
        _set_progress(job_id, int(idx * 100 / total))

    _set_state(job_id, JobState.FINISHED)
    return {"status": "FINISHED", "processed": total}
```

**Two pause styles:**

* **Exit-as-PAUSED**: task returns early; you later **resume by re-enqueueing** with saved progress.
* **Sleep-until-resume**: task stays alive and sleeps in a loop until `pause` clears. (Simpler UX, but holds a worker slot.)

---

## Worker-side pattern (ARQ, async)

```python
# app/workers/arq_tasks.py
import asyncio, time
from aioredis import from_url
from app.workflows.state import JobState

async def long_job_arq(ctx, job_id: str, items: list[int]):
    r = await from_url("redis://localhost:6379", encoding="utf-8", decode_responses=True)
    meta_key = f"job:{job_id}:meta"; ctrl_key = f"job:{job_id}:ctrl"
    await r.hset(meta_key, mapping={"state": JobState.STARTED})

    total = len(items)
    for idx, item in enumerate(items, 1):
        if idx % 25 == 0:
            cancel = await r.hget(ctrl_key, "cancel")
            pause  = await r.hget(ctrl_key, "pause")
            if cancel == "1":
                await r.hset(meta_key, mapping={"state": JobState.CANCEL_REQUESTED})
                await r.hset(meta_key, mapping={"state": JobState.CANCELED})
                return {"status": "CANCELED", "processed": idx-1}
            if pause == "1":
                await r.hset(meta_key, mapping={"state": JobState.PAUSE_REQUESTED})
                await r.hset(meta_key, mapping={"state": JobState.PAUSED})
                return {"status": "PAUSED", "processed": idx-1}

        # simulate async work
        await asyncio.sleep(0.02)
        await r.hset(meta_key, mapping={"progress": int(idx * 100 / total), "last_checkpoint": str(time.time())})

    await r.hset(meta_key, mapping={"state": JobState.FINISHED})
    return {"status": "FINISHED", "processed": total}
```

---

## FastAPI control endpoints

```python
# app/routes/jobs.py
from fastapi import APIRouter, HTTPException
from redis import Redis
from app.schemas import JobStatus
from app.workflows.state import JobState

r = Redis.from_url("redis://localhost:6379")

router = APIRouter(prefix="/jobs", tags=["jobs"])

def _ctrl_key(job_id): return f"job:{job_id}:ctrl"
def _meta_key(job_id): return f"job:{job_id}:meta"

@router.post("/{job_id}/pause")
def pause_job(job_id: str):
    # set pause=1
    r.hset(_ctrl_key(job_id), "pause", 1)
    # mark transition request if currently STARTED
    r.hset(_meta_key(job_id), "state", JobState.PAUSE_REQUESTED)
    return {"job_id": job_id, "action": "pause_requested"}

@router.post("/{job_id}/resume")
def resume_job(job_id: str):
    r.hset(_ctrl_key(job_id), "pause", 0)
    r.hset(_meta_key(job_id), "state", JobState.RESUME_REQUESTED)
    return {"job_id": job_id, "action": "resume_requested"}

@router.post("/{job_id}/cancel")
def cancel_job(job_id: str):
    # If job is still QUEUED in RQ, you can dequeue/cancel directly too.
    r.hset(_ctrl_key(job_id), "cancel", 1)
    r.hset(_meta_key(job_id), "state", JobState.CANCEL_REQUESTED)
    return {"job_id": job_id, "action": "cancel_requested"}

@router.get("/{job_id}/status", response_model=JobStatus)
def job_status(job_id: str):
    meta = r.hgetall(_meta_key(job_id)) or {}
    if not meta:
        raise HTTPException(404, "Job not found")
    return JobStatus(
        job_id=job_id,
        state=meta.get("state", JobState.QUEUED),
        progress=int(meta.get("progress", 0)),
        detail=meta.get("detail")
    )
```

---

## Canceling a **QUEUED** job (RQ)

```python
# app/workers/rq_admin.py
from rq import Queue
from redis import Redis

r = Redis.from_url("redis://localhost:6379")
q = Queue("default", connection=r)

def cancel_if_queued(job_id: str) -> bool:
    job = q.fetch_job(job_id)
    if job is None:
        return False
    if job.get_status() == "queued":
        job.cancel()               # removes from queue
        r.hset(f"job:{job_id}:meta", "state", "CANCELED")
        return True
    return False
```

> If already **STARTED**, the cooperative flag method above is the safe route.

---

## Resuming a paused job

Two strategies:

1. **Re-enqueue** with resume context (recommended)

   * You saved `processed` count and any partial artifacts in `meta` (or external store).
   * On resume, enqueue the same task with `start_index` parameter.

   ```python
   # Enqueue
   q.enqueue(long_job, {"items": big_list, "start_index": saved_idx})
   # Worker starts at saved_idx instead of 0
   ```

2. **Sleep-until-resume** (holds worker slot)

   * The task stays alive, sleeping until `pause=0`. Simple UX, but reduces throughput.

---

## Handling races & idempotency

* **Edge cases**:

  * User calls `/cancel` twice → should be **idempotent** and safe.
  * `/pause` then immediately `/cancel` → task should honor **cancel** first on next checkpoint.
  * `/resume` on a job that already finished → return 409 or no-op with a helpful message.

* **On worker startup/restart**:

  * If you store `progress` + artifacts consistently, resuming is robust after crashes.

---

## Observability & UX

* Ensure `/status/{job_id}` returns:

  * `state`, `progress`, and **reason** (`detail`) for PAUSED/CANCELED/FAILED.
* Consider a small **websocket** or **Server-Sent Events** to push status to UI.
* Log state transitions with structured logs, e.g.:

  ```json
  {"job_id":"...","from":"STARTED","to":"PAUSE_REQUESTED","ts":"..."}
  ```

---

## Quick test plan

1. **Happy path**: run a long job; see progress rise to 100%; state → FINISHED.
2. **Pause during run**: call `/pause` mid-way; state → PAUSED; result returns with `processed=k`.
3. **Resume (re-enqueue)**: enqueue with `start_index=k`; watch it finish.
4. **Cancel queued**: enqueue; cancel immediately; verify it never starts; state → CANCELED.
5. **Cancel running**: enqueue; wait for STARTED; call `/cancel`; job exits soon; artifacts cleaned up.

---

## Takeaways

* True pause of a Python function/thread is unsafe; **cooperative** pause/resume/cancel is the practical pattern.
* Make **frequent checkpoints** and keep transitions **idempotent**.
* Prefer **exit-as-PAUSED + re-enqueue with resume context** for scale and simplicity.

---

## Optional: tiny state diagram (for your docs)

```
[QUEUED] --start--> [STARTED]
[STARTED] --pause--> [PAUSE_REQUESTED] --> [PAUSED]
[PAUSED]  --resume-> [RESUME_REQUESTED] -> [STARTED]
[STARTED] --cancel-> [CANCEL_REQUESTED] -> [CANCELED]
[STARTED] ----------> [FINISHED | FAILED]
```

---

