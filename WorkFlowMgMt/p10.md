# Lesson 10: Configuring ARQ Worker

## What you’ll learn

* Install and wire up **ARQ** with Redis
* Create an **ARQWorker** configuration (WorkerSettings)
* Write **async** task functions
* Enqueue and check jobs from **FastAPI async routes**

---

## 1) Install & basic project layout

```bash
pip install arq redis fastapi uvicorn python-dotenv pydantic-settings
```

```
workflow-api/
  app/
    __init__.py
    main.py              # FastAPI app with async routes
    settings.py          # loads env (REDIS_URL, etc.)
    arq_client.py        # create_pool() helper for enqueue/status
  worker/
    __init__.py
    tasks.py             # async def process_task(...)
    settings.py          # class WorkerSettings for ARQ worker
  .env.example
  README.md
```

`.env.example`

```
REDIS_URL=redis://localhost:6379/0
ARQ_QUEUE=arq:workflow
```

---

## 2) ARQ Worker configuration (the “ARQWorker class”)

Create `worker/tasks.py`:

```python
# worker/tasks.py
import asyncio
from datetime import datetime

# Each ARQ task is async and receives a context dict (ctx) as first arg
async def process_task(ctx, payload: dict) -> dict:
    # Example: simulate compute
    await asyncio.sleep(0.5)
    # You can access ctx["redis"] if you added it in startup (see settings.py)
    return {
        "ok": True,
        "received": payload,
        "processed_at": datetime.utcnow().isoformat() + "Z",
    }

# Another example task
async def generate_report(ctx, report_id: str) -> str:
    await asyncio.sleep(1.0)
    return f"report:{report_id}:ready"
```

Create `worker/settings.py`:

```python
# worker/settings.py
from arq.connections import RedisSettings
from worker.tasks import process_task, generate_report

# This class is discovered by the `arq` CLI
class WorkerSettings:
    redis_settings = RedisSettings()  # uses env vars or defaults
    queue_name = "arq:workflow"       # override via env if you like
    functions = [process_task, generate_report]
    max_jobs = 8                      # parallelism per worker process
    job_timeout = 60                  # seconds
    keep_result = 600                 # seconds to keep results in Redis

    # optional: run code once when worker starts, build shared ctx
    async def startup(ctx):
        # put shared resources into ctx if needed, e.g., a Redis pool/db client
        ctx["service_name"] = "workflow-worker"

    async def shutdown(ctx):
        # close clients if you opened any in startup
        pass
```

### Run the worker

From repo root:

```bash
export PYTHONPATH=.
arq worker.settings.WorkerSettings
```

(Windows PowerShell: `set PYTHONPATH=.`)

> Tip: run multiple workers for higher throughput; each uses `max_jobs` concurrency.

---

## 3) FastAPI: enqueue & query from async routes

`app/settings.py`:

```python
# app/settings.py
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    REDIS_URL: str = "redis://localhost:6379/0"
    ARQ_QUEUE: str = "arq:workflow"

settings = Settings()  # auto-reads .env if present
```

`app/arq_client.py`:

```python
# app/arq_client.py
from arq import create_pool
from arq.connections import RedisSettings
from app.settings import settings

async def get_arq_pool():
    # Build a connection pool for enqueueing and job queries
    return await create_pool(RedisSettings.from_url(settings.REDIS_URL))
```

`app/main.py`:

```python
# app/main.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from arq.jobs import Job
from app.arq_client import get_arq_pool

app = FastAPI(title="Workflow API with ARQ")

class Payload(BaseModel):
    message: str
    meta: dict | None = None

@app.on_event("startup")
async def on_startup():
    # Create and share ARQ pool
    app.state.arq = await get_arq_pool()

@app.on_event("shutdown")
async def on_shutdown():
    # ARQ pool cleans up on GC, nothing special required
    pass

@app.post("/submit", status_code=202)
async def submit_job(payload: Payload):
    """
    Enqueue an async ARQ job by function name.
    """
    arq = app.state.arq
    job: Job = await arq.enqueue_job("process_task", payload.model_dump())  # returns Job
    return {"job_id": job.job_id}

@app.get("/status/{job_id}")
async def job_status(job_id: str):
    """
    Non-blocking status check. Use job.result() with timeout for blocking.
    """
    arq = app.state.arq
    job = Job(job_id, arq)
    info = await job.info()  # {'status': 'queued'|'in_progress'|'complete'|'not_found'|'deferred', ...}
    if not info:
        raise HTTPException(404, detail="Job not found")

    # If complete, fetch result (non-blocking)
    if info.get("status") == "complete":
        result = await job.result()   # returns the dict from process_task()
        return {"status": "complete", "result": result}

    return {"status": info.get("status")}
```

Run the API:

```bash
uvicorn app.main:app --reload
```

Test quickly:

```bash
# 1) Submit:
curl -X POST http://127.0.0.1:8000/submit -H "Content-Type: application/json" \
  -d '{"message":"hello","meta":{"user":"santanu"}}'

# => {"job_id":"<uuid>"}

# 2) Poll status:
curl http://127.0.0.1:8000/status/<job_id>
# => {"status":"queued" | "in_progress" | "complete", "result": {...}}
```

---

## 4) Writing robust async tasks

**Signature:** `async def my_task(ctx, *args, **kwargs) -> Any`

* **Context (`ctx`)** is a dict from `WorkerSettings.startup()`. Use it to share clients (DB, HTTP, vector store).
* **Timeout & retries**

  * `job_timeout` in `WorkerSettings` cancels long runners.
  * Implement your own retry logic inside tasks, or enqueue a follow-up job if needed.
* **Idempotency**

  * Pass a deterministic key in payload and make tasks safe to rerun (e.g., check “already processed” flags in DB/Redis).
* **Serialization**

  * Keep payloads JSON-serializable (ARQ uses msgpack). Avoid large blobs; store them in S3/DB and pass references.

Example with a quick retry pattern:

```python
# worker/tasks.py (snippet)
async def process_task(ctx, payload: dict) -> dict:
    for attempt in range(3):
        try:
            # do work...
            return {"ok": True, "attempt": attempt + 1}
        except Exception as e:
            if attempt == 2:
                raise
            await asyncio.sleep(0.5 * (attempt + 1))
```

---

## 5) Using ARQ cleanly with FastAPI (patterns)

* **App pool reuse:** create one ARQ pool on startup (`app.state.arq`).
* **Function names:** enqueue by function name string (**must** match the function registered in `WorkerSettings.functions`).
* **Queues:** isolate concerns by `queue_name` (e.g., `"arq:reports"`, `"arq:triage"`); run separate workers per queue if needed.
* **Backpressure:** tune `max_jobs` and run more worker processes rather than raising timeouts.

---

## 6) Bonus: Cron & scheduled jobs (optional)

Add to `WorkerSettings`:

```python
from arq import cron

class WorkerSettings:
    # ...
    cron_jobs = [
        cron(generate_report, minute={0,15,30,45}, kwargs={"report_id": "quarter-hour"})
    ]
```

Workers will invoke `generate_report` on schedule.

---

## 7) Common commands

```bash
# Start redis locally (example using docker):
docker run --rm -p 6379:6379 redis:7

# Run worker:
arq worker.settings.WorkerSettings

# Start API:
uvicorn app.main:app --reload
```

---

## 8) Quick checklist

* [ ] `arq` installed and Redis running
* [ ] `worker/settings.py` has `WorkerSettings` and lists your `functions`
* [ ] Tasks are `async def task(ctx, ...)`
* [ ] FastAPI creates a single ARQ pool on startup
* [ ] Enqueue with `enqueue_job("task_name", ...)`
* [ ] Poll with `/status/{job_id}` using `Job.info()` / `Job.result()`

---

