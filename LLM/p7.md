# 7. Infrastructure and Deployment

Building and deploying Large Language Models (LLMs) requires careful consideration of **where** and **how** the models will run. Infrastructure decisions affect performance, cost, scalability, and even user experience. This chapter outlines key approaches and trade-offs in running LLMs.

---

## Running LLMs Locally vs via API

* **Locally hosted models**

  * *Pros*: Full control over data (privacy & compliance), offline use, no dependency on external APIs, potential long-term cost savings.
  * *Cons*: High upfront hardware costs, ongoing maintenance, limited scalability unless distributed.
  * *Typical tools*: Ollama, LM Studio, vLLM, Hugging Face `transformers`.

* **API-based models**

  * *Pros*: Easy integration, access to state-of-the-art models (e.g., GPT-4, Claude, Gemini), minimal setup, scalable by provider.
  * *Cons*: Recurring costs, potential latency, limited transparency in model behavior, reliance on provider uptime.
  * *Best for*: Prototypes, small teams, or when the goal is to use the latest frontier models without managing infrastructure.

---

## Cloud Platforms

Several major platforms provide APIs or model hosting:

* **OpenAI** – GPT-4o, fine-tuning, embeddings API.
* **Anthropic** – Claude models with a strong focus on safety and reasoning.
* **Google** – Gemini family of models via Vertex AI.
* **Hugging Face** – Model hub with Inference Endpoints, AutoTrain, and Spaces for demos.
* **Others** – Cohere (Rerank, Command R+), Together.ai, and AWS Bedrock for multi-provider access.

**Key decision factors:**

* SLA (service level agreement) guarantees.
* Compliance requirements (e.g., HIPAA, GDPR).
* Cost structure: per-token pricing vs compute hours.
* Ecosystem integration (vector DBs, monitoring, orchestration frameworks).

---

## Model Serving with FastAPI, Flask, or Streamlit

When hosting models yourself, you need a serving layer:

* **FastAPI**

  * High performance, async support.
  * Ideal for production APIs where speed and reliability matter.

* **Flask**

  * Lightweight, easy to set up.
  * Good for prototypes or small internal services.

* **Streamlit**

  * Interactive, UI-first approach.
  * Best suited for demos, dashboards, or educational tools.
  * Less efficient for high-volume production APIs.

Other options: **Gradio** (quick demos), **Ray Serve** (scalable serving), **vLLM** (optimized inference server).

---

## Hardware Needs: GPU vs CPU vs TPU

* **CPU**

  * Works for small models (<3B parameters).
  * Low inference speed on larger models.
  * Best for lightweight apps or non-critical workloads.

* **GPU**

  * Primary choice for most LLM workloads.
  * Strong parallelism for matrix operations.
  * Popular options: NVIDIA A100, H100, RTX 4090 for local experimentation.

* **TPU** (Tensor Processing Unit)

  * Specialized hardware from Google.
  * Optimized for training and inference at scale.
  * Available primarily through Google Cloud.

**Memory requirements:**

* Rule of thumb: \~2x model parameter size in VRAM.

  * e.g., a 7B parameter model (\~14 GB) fits comfortably on a 24 GB GPU.
  * Larger models (30B+) require multi-GPU setups or quantization.

---

## Cost Optimization and Batching Strategies

* **Token cost management (API models)**

  * Use smaller models when possible.
  * Apply summarization or compression before sending context.
  * Track usage with monitoring dashboards.

* **Compute cost management (self-hosted)**

  * Use quantization (4-bit, 8-bit) to fit models into smaller GPUs.
  * Batch multiple requests into one forward pass for efficiency.
  * Use CPU offloading or mixed precision (FP16/BF16).

* **Hybrid strategies**

  * Route easy queries to smaller models, send complex tasks to larger ones.
  * Cache frequent responses.
  * Deploy models closer to users (edge servers) to reduce latency.

---

✅ **Key Takeaway:**
The choice between running LLMs locally or via API depends on **control, cost, and scale**. Cloud APIs are fast to start but expensive at scale, while local deployments demand hardware investment but provide privacy and long-term savings. Serving frameworks (FastAPI, Streamlit, etc.), hardware choices (CPU, GPU, TPU), and cost strategies all play a role in building sustainable, production-ready LLM applications.

---

