# Chapter 20: Performance & Optimization

Optimizing FastAPI applications is crucial when you start moving from small prototypes to production systems. Performance can be improved at several levels â€” from how you write async code to how you scale the deployment.

---

## 1. Using `async` Properly

FastAPI is designed to take advantage of **Pythonâ€™s async/await syntax**, but misuse can lead to **blocking I/O**, reducing performance.

* **When to use `async def`:**
  Use for routes that make external I/O calls (database queries, API requests, file reading).

* **When to use `def`:**
  Use for CPU-bound tasks (complex calculations, image processing).

Example:

```python
from fastapi import FastAPI
import httpx

app = FastAPI()

# Good use of async
@app.get("/async-data")
async def get_data():
    async with httpx.AsyncClient() as client:
        response = await client.get("https://jsonplaceholder.typicode.com/posts/1")
    return response.json()

# For CPU-heavy tasks, keep it sync
@app.get("/cpu-task")
def cpu_task():
    total = sum([i * i for i in range(10_000_000)])
    return {"result": total}
```

ðŸ‘‰ Rule of thumb: Use `async` when waiting on I/O, `def` when crunching CPU.

---

## 2. Connection Pooling for Databases

Creating a new database connection for each request is **expensive**. Instead, use **connection pooling**.

Example with **SQLAlchemy + async engine**:

```python
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from fastapi import Depends

DATABASE_URL = "postgresql+asyncpg://user:password@localhost/dbname"

engine = create_async_engine(DATABASE_URL, pool_size=5, max_overflow=10)
SessionLocal = sessionmaker(bind=engine, class_=AsyncSession, expire_on_commit=False)

async def get_db():
    async with SessionLocal() as session:
        yield session
```

* `pool_size`: Number of connections kept open.
* `max_overflow`: Extra temporary connections allowed.

ðŸ‘‰ Always use pooled sessions instead of opening/closing fresh connections each request.

---

## 3. Caching with Redis

For frequently requested or slow-to-compute data, caching saves time and reduces database load.

### Example: Caching with Redis

```python
from fastapi import FastAPI, Depends
import aioredis
import json

app = FastAPI()
redis = None

@app.on_event("startup")
async def startup():
    global redis
    redis = await aioredis.from_url("redis://localhost")

@app.get("/items/{item_id}")
async def get_item(item_id: int):
    # Try cache
    cached = await redis.get(f"item:{item_id}")
    if cached:
        return json.loads(cached)

    # Simulate DB fetch
    data = {"item_id": item_id, "name": f"Item {item_id}"}

    # Save in cache
    await redis.set(f"item:{item_id}", json.dumps(data), ex=60)  # expire in 60s
    return data
```

ðŸ‘‰ Caching works best for:

* Expensive DB queries
* External API calls
* Common lookups (user profiles, product catalogs)

---

## 4. Gunicorn + Uvicorn Workers

For production, use **Gunicorn** as a process manager with **Uvicorn workers**.

### Example Run Command:

```bash
gunicorn -k uvicorn.workers.UvicornWorker main:app --workers 4 --bind 0.0.0.0:8000
```

* `--workers 4`: Number of worker processes (set ~ # of CPU cores).
* `--bind`: Host/port binding.

ðŸ‘‰ With multiple workers:

* Gunicorn manages worker processes.
* Each worker runs Uvicorn (ASGI server).
* Better utilization of CPU + fault isolation.

---

## 5. Example: Caching Frequently Requested Data

Imagine a public API that fetches COVID-19 case stats. Instead of hitting the database every time, cache the results.

```python
from fastapi import FastAPI
import aioredis, json
import httpx

app = FastAPI()
redis = None

@app.on_event("startup")
async def startup():
    global redis
    redis = await aioredis.from_url("redis://localhost")

@app.get("/covid-stats")
async def covid_stats():
    cached = await redis.get("covid:latest")
    if cached:
        return {"source": "cache", "data": json.loads(cached)}

    # Simulate external API call
    async with httpx.AsyncClient() as client:
        response = await client.get("https://api.covidtracking.com/v1/us/current.json")
        data = response.json()

    # Store in cache for 5 minutes
    await redis.set("covid:latest", json.dumps(data), ex=300)
    return {"source": "API", "data": data}
```

* First call â†’ Fetch from API, save to Redis.
* Next calls â†’ Return cached data instantly.
* After 5 minutes â†’ Refresh.

---

## âœ… Key Takeaways

* Use `async` wisely â€” donâ€™t block the event loop.
* Connection pooling improves DB efficiency.
* Cache repetitive queries with Redis.
* Use **Gunicorn + Uvicorn workers** in production.
* Optimize early for scalability, not just correctness.

---
