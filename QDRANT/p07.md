# **Chapter 7. Persistence & Deployment**

Once you understand the basics of working with collections, embeddings, and queries, the next important step is **persistence and deployment**. Qdrant is designed to be not just a local playground for experimentation but a scalable production-ready vector database. This chapter will walk you through how to persist data, import/export collections, deploy Qdrant using Docker, and manage large datasets efficiently.

---

## Persistent Storage (SQLite, RocksDB)

Qdrant supports multiple storage backends to ensure that your data is not lost when the server restarts.

### SQLite

* **Lightweight option** for metadata and collection management.
* Useful for small to medium projects, development, or embedded use cases.
* Stores payloads and indexing data on disk with less overhead.

```bash
qdrant \
  --storage "sqlite" \
  --storage-path "./qdrant_data"
```

### RocksDB

* **High-performance option** for large-scale vector storage.
* Designed for production deployments with heavy read/write operations.
* Provides better durability and indexing efficiency.

```bash
qdrant \
  --storage "rocksdb" \
  --storage-path "./qdrant_rocks_data"
```

**Tip:** Choose **SQLite** for simplicity, **RocksDB** for performance at scale.

---

## Importing & Exporting Collections

Qdrant provides built-in functionality to move collections between environments or back them up.

### Export a Collection

```bash
curl -X POST "http://localhost:6333/collections/my_collection/snapshots"
```

This creates a **snapshot file** containing all points, payloads, and indexes.

### Import a Collection

Snapshots can be restored in another Qdrant instance:

```bash
curl -X POST \
  "http://localhost:6333/collections/my_collection/restore" \
  -H "Content-Type: application/json" \
  -d '{"snapshot": "path/to/snapshot"}'
```

This is particularly useful when:

* Migrating data between test and production environments.
* Creating backups for disaster recovery.

---

## Running Qdrant with Docker

The simplest way to deploy Qdrant is via **Docker**.

### Basic Run Command

```bash
docker run -p 6333:6333 \
  -v $(pwd)/qdrant_storage:/qdrant/storage:z \
  qdrant/qdrant
```

* **Port 6333**: REST API & gRPC.
* **Mounted volume**: Persists data locally.

### Docker Compose Example

```yaml
version: "3.8"
services:
  qdrant:
    image: qdrant/qdrant:v1.7.0
    ports:
      - "6333:6333"
    volumes:
      - ./qdrant_storage:/qdrant/storage
```

Then start with:

```bash
docker-compose up -d
```

This setup ensures data persistence and easy restart management.

---

## Managing Large Datasets

When scaling up to millions of vectors, careful planning is needed.

1. **Batch Inserts**
   Insert vectors in batches (e.g., 1,000â€“10,000 at a time) instead of single inserts for efficiency.

   ```python
   client.upsert(
       collection_name="docs",
       points=[
           {"id": 1, "vector": vec1, "payload": {"title": "doc1"}},
           {"id": 2, "vector": vec2, "payload": {"title": "doc2"}},
           # ...
       ]
   )
   ```

2. **Sharding**
   Qdrant automatically splits large collections into **shards** for parallel processing. Adjust the number of shards when creating collections if you expect very large datasets.

   ```json
   {
     "vectors": { "size": 768, "distance": "Cosine" },
     "shard_number": 8
   }
   ```

3. **Indexing**

   * Use **HNSW index** for approximate nearest neighbor (default).
   * Adjust `ef_construct` and `m` parameters to balance speed vs. accuracy.

4. **Monitoring**
   Use the `/collections/{name}` endpoint to check memory usage and optimize shard distribution.

---

## Key Takeaways

* Qdrant ensures persistence with **SQLite** (lightweight) or **RocksDB** (scalable).
* Snapshots allow you to **import/export collections** easily for migration or backup.
* **Docker/Docker Compose** provides a quick and reliable way to run Qdrant in production.
* For **large datasets**, rely on batch inserts, sharding, and careful index tuning.

---

