# **Chapter 9. Performance & Scaling**

When working with large-scale vector search applications, performance and scalability become crucial. Qdrant provides several features and configurations to optimize indexing, querying, and deployment across multiple nodes. This chapter explores strategies to ensure high throughput and low latency in real-world scenarios.

---

## 9.1 Indexing Strategies

Efficient indexing is the foundation of high-performance search.
Qdrant supports several distance metrics (Cosine, Dot Product, Euclidean) and indexing approaches.

* **Default HNSW index**

  * Uses Hierarchical Navigable Small World (HNSW) graphs for approximate nearest neighbor (ANN) search.
  * Provides sub-millisecond query times even for millions of vectors.
  * Tunable parameters:

    * `m` (graph connectivity factor)
    * `ef_construct` (construction time vs accuracy tradeoff)
    * `ef_search` (query-time accuracy vs speed tradeoff)

* **Optimizing index parameters**

  * Use higher `m` for better recall at the cost of more memory.
  * Adjust `ef_search` dynamically depending on latency requirements.
  * For large datasets, build indexes offline before deploying into production.

* **Index maintenance**

  * Re-indexing may be required when changing metric or tuning parameters.
  * Qdrant can rebuild indexes without downtime using background processes.

---

## 9.2 Batch Inserts and Queries

When handling millions of vectors, inserting or querying one point at a time is inefficient.

* **Batch Inserts**

  * Use `upsert_points` or `scroll` API with batches of 1k–10k vectors for best throughput.
  * Parallelize inserts across multiple workers.
  * Compress embeddings (e.g., float16) to reduce storage and network overhead.

* **Batch Queries**

  * Query multiple vectors in a single API call.
  * Leverage `search_batch` to reduce request overhead.
  * Useful for applications like re-ranking, multi-modal retrieval, and recommendation engines.

---

## 9.3 Sharding and Distributed Setups

For very large collections, distributing data across multiple nodes ensures both scalability and fault tolerance.

* **Sharding**

  * Split collections into shards distributed across nodes.
  * Qdrant automatically routes queries to the appropriate shard.
  * Recommended when single-node memory is insufficient.

* **Replication**

  * Replicate shards across multiple nodes for high availability.
  * Ensures resilience against node failures.
  * Useful for mission-critical applications where uptime is non-negotiable.

* **Cluster Scaling**

  * Add nodes dynamically to handle increased workloads.
  * Horizontal scaling allows linear growth of dataset size and query capacity.
  * Best practice: monitor query load and scale before hitting memory/CPU bottlenecks.

---

## 9.4 Monitoring Qdrant Performance

To maintain smooth operations at scale, monitoring is essential.

* **Built-in Metrics**

  * Qdrant exposes Prometheus-compatible metrics.
  * Track latency, query throughput, memory usage, and indexing times.

* **Key Metrics to Watch**

  * **Query latency** – should remain under target SLA (e.g., <50ms).
  * **Recall rate** – balance accuracy vs speed by tuning ANN parameters.
  * **Memory usage** – ensure RAM is sufficient for HNSW index + payloads.
  * **Disk I/O** – avoid bottlenecks in persistence-heavy workloads.

* **Alerting & Logging**

  * Set alerts for memory thresholds and error spikes.
  * Enable structured logs for debugging slow queries.

* **Scaling Decisions**

  * Increase `ef_search` for accuracy if latency is acceptable.
  * Add replicas if query load increases.
  * Add shards/nodes when dataset size grows beyond single-node memory.

---

✅ **Summary**:
Scaling Qdrant effectively involves careful indexing strategy, efficient batch operations, distributed sharding, and proactive monitoring. By tuning parameters and designing for horizontal scalability, Qdrant can handle billions of vectors with high availability and performance.

---

