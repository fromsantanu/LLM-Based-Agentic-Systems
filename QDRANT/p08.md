# **Chapter 8. Integration with Python Workflows**

Qdrant is designed with developer-friendly APIs, making it straightforward to integrate into Python-based workflows. Whether you are building a standalone Retrieval-Augmented Generation (RAG) pipeline, integrating with frameworks like LangChain or LlamaIndex, or exposing vector search through a web API, Qdrant offers seamless compatibility.

---

## Qdrant Client in Python

Qdrant provides an official Python client that allows you to interact with the database programmatically. You can connect to a local instance, a Docker deployment, or Qdrant Cloud.

```python
# Install the client
# pip install qdrant-client

from qdrant_client import QdrantClient
from qdrant_client.http import models

# Connect to local Qdrant
client = QdrantClient(host="localhost", port=6333)

# Or connect to Qdrant Cloud
# client = QdrantClient(url="https://<your-cluster>.qdrant.io", api_key="<API_KEY>")

# Create a collection
client.recreate_collection(
    collection_name="docs",
    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),
)
```

This client supports CRUD operations, similarity queries, and advanced filtering, which makes it ideal for machine learning pipelines.

---

## Building a RAG Pipeline with Qdrant + LLMs

Retrieval-Augmented Generation (RAG) enhances language models by providing them with factual grounding from a vector database.

### Workflow

1. **Embed documents** using a transformer model.
2. **Store embeddings** in Qdrant along with metadata.
3. **Query Qdrant** with user input to retrieve relevant documents.
4. **Feed retrieved context** into an LLM for response generation.

```python
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient

# Load embedding model
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Example documents
docs = ["Qdrant is a vector database.", "LangChain helps build LLM apps."]

# Create embeddings
vectors = embedder.encode(docs).tolist()

# Upload to Qdrant
client.upsert(
    collection_name="docs",
    points=[
        models.PointStruct(id=i, vector=vectors[i], payload={"text": docs[i]})
        for i in range(len(docs))
    ],
)

# User query
query = "What is Qdrant?"
query_vector = embedder.encode(query).tolist()

# Search in Qdrant
results = client.search(
    collection_name="docs",
    query_vector=query_vector,
    limit=2
)

for r in results:
    print(r.payload["text"], r.score)
```

The retrieved texts can then be inserted into the prompt for your LLM (OpenAI, HuggingFace, etc.).

---

## Using Qdrant with LangChain

LangChain provides a `Qdrant` vector store wrapper, making integration simple.

```python
from langchain.vectorstores import Qdrant
from langchain.embeddings import OpenAIEmbeddings
from qdrant_client import QdrantClient

# Initialize embedding model
embeddings = OpenAIEmbeddings()

# Connect to Qdrant
client = QdrantClient(host="localhost", port=6333)

# Create LangChain vectorstore
vectorstore = Qdrant(client, "docs", embeddings)

# Add documents
texts = ["Qdrant integrates with LangChain.", "It supports semantic search."]
vectorstore.add_texts(texts)

# Perform similarity search
query = "How does Qdrant work with LangChain?"
docs = vectorstore.similarity_search(query, k=2)
for d in docs:
    print(d.page_content)
```

LangChain abstracts the database logic, letting you focus on building agents, chains, and memory systems.

---

## Using Qdrant with LlamaIndex

LlamaIndex (formerly GPT Index) provides another powerful integration with Qdrant.

```python
from llama_index import VectorStoreIndex, Document
from llama_index.vector_stores import QdrantVectorStore
from qdrant_client import QdrantClient

# Connect to Qdrant
client = QdrantClient(host="localhost", port=6333)
vector_store = QdrantVectorStore(client=client, collection_name="docs")

# Create documents
documents = [Document(text="Qdrant works well with LlamaIndex.")]

# Build index
index = VectorStoreIndex.from_documents(documents, vector_store=vector_store)

# Query the index
query_engine = index.as_query_engine()
response = query_engine.query("Which DB works with LlamaIndex?")
print(response)
```

This setup makes Qdrant a scalable backend for knowledge-intensive applications.

---

## Connecting to FastAPI/Flask

To expose your Qdrant-powered search as an API, you can use **FastAPI** or **Flask**.

### Example with FastAPI:

```python
from fastapi import FastAPI
from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer

app = FastAPI()
client = QdrantClient(host="localhost", port=6333)
embedder = SentenceTransformer("all-MiniLM-L6-v2")

@app.get("/search/")
def search(query: str, limit: int = 3):
    vector = embedder.encode(query).tolist()
    results = client.search(
        collection_name="docs",
        query_vector=vector,
        limit=limit
    )
    return [{"text": r.payload["text"], "score": r.score} for r in results]
```

Now, you can send requests like:

```
GET http://127.0.0.1:8000/search/?query=What+is+Qdrant
```

---

## Key Takeaways

* **Qdrant client** enables seamless CRUD and search operations in Python.
* **RAG pipelines** become more accurate and explainable by grounding LLMs with Qdrant search.
* **LangChain and LlamaIndex** provide high-level integrations for quick prototyping.
* **FastAPI/Flask** makes it easy to expose Qdrant-powered search services as web APIs.

---

