# **Chapter 4. Working with Embeddings**

Embeddings are at the heart of vector databases like **Qdrant**. They convert raw data (text, images, audio, etc.) into high-dimensional numerical vectors that capture semantic meaning. These vectors are then stored in Qdrant and used for similarity search, clustering, and recommendation tasks.

In this chapter, we will cover:

* Generating embeddings with **OpenAI**, **HuggingFace**, and **SentenceTransformers**
* Storing vectors in **Qdrant**
* Comparing results across different embedding models

---

## 4.1 Generating Embeddings

### 4.1.1 Using OpenAI Embeddings

OpenAI provides API endpoints for embedding generation. For text, the `text-embedding-3-small` and `text-embedding-3-large` models are widely used.

```python
from openai import OpenAI

client = OpenAI(api_key="YOUR_API_KEY")

text = "Qdrant is an open-source vector database."
response = client.embeddings.create(
    model="text-embedding-3-small",
    input=text
)

embedding = response.data[0].embedding
print(len(embedding))  # e.g., 1536 dimensions
```

ðŸ”¹ **Pros:** High-quality, production-ready embeddings.
ðŸ”¹ **Cons:** Requires API key and paid usage.

---

### 4.1.2 Using HuggingFace Transformers

HuggingFace provides pretrained models via the `transformers` library.

```python
from transformers import AutoTokenizer, AutoModel
import torch

model_name = "sentence-transformers/all-MiniLM-L6-v2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

text = "Qdrant is an open-source vector database."
inputs = tokenizer(text, return_tensors="pt")
with torch.no_grad():
    embeddings = model(**inputs).last_hidden_state.mean(dim=1)

embedding = embeddings[0].numpy()
print(embedding.shape)  # e.g., (384,)
```

ðŸ”¹ **Pros:** Free, flexible, offline.
ðŸ”¹ **Cons:** Requires local computation (GPU recommended).

---

### 4.1.3 Using SentenceTransformers

`sentence-transformers` is a high-level wrapper around HuggingFace, optimized for sentence embeddings.

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("all-MiniLM-L6-v2")

sentences = [
    "Qdrant is an open-source vector database.",
    "Pinecone is a managed vector database."
]

embeddings = model.encode(sentences)
print(embeddings.shape)  # (2, 384)
```

ðŸ”¹ **Pros:** Easy to use, well-optimized for semantic search.
ðŸ”¹ **Cons:** Limited to supported models.

---

## 4.2 Storing Vectors in Qdrant

Once embeddings are generated, they can be stored in Qdrant for similarity search.

```python
from qdrant_client import QdrantClient
from qdrant_client.http import models as rest

# Connect to Qdrant (local or cloud)
client = QdrantClient(host="localhost", port=6333)

# Create a collection (if not exists)
client.recreate_collection(
    collection_name="documents",
    vectors_config=rest.VectorParams(size=384, distance=rest.Distance.COSINE),
)

# Insert embeddings
points = [
    rest.PointStruct(id=1, vector=embeddings[0].tolist(), payload={"text": sentences[0]}),
    rest.PointStruct(id=2, vector=embeddings[1].tolist(), payload={"text": sentences[1]})
]

client.upsert(collection_name="documents", points=points)
```

---

## 4.3 Comparing Different Embedding Models

Not all embeddings perform equally well. Comparison is usually based on:

1. **Vector Size (Dimensionality)**

   * OpenAI: 1536
   * all-MiniLM-L6-v2: 384
   * Others: 768, 1024, etc.

2. **Performance (Semantic Search Quality)**

   * Higher dimensional embeddings generally capture more nuance.
   * SentenceTransformers are optimized for sentence similarity tasks.
   * OpenAI embeddings often excel in multilingual and domain-general contexts.

3. **Cost and Latency**

   * OpenAI â†’ Paid, low-latency cloud API.
   * HuggingFace / SentenceTransformers â†’ Free, requires local compute.

---

## 4.4 Best Practices

* **Benchmark models** on your dataset using retrieval accuracy (e.g., Recall\@K).
* **Choose dimensionality wisely**: smaller embeddings are faster but may capture less semantic detail.
* **Normalize embeddings** when using cosine similarity.
* **Cache embeddings** to avoid recomputation.

---

âœ… **Next Step:** In the following chapters, we will explore **CRUD operations** in Qdrant, allowing us to insert, update, delete, and fetch documents from collections.

---

