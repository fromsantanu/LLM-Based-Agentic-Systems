# **Chapter 5: LangChain Recap**

---

## **1. Introduction**

Before diving deeper into LangServe integration, it’s essential to review the **core LangChain concepts** that underpin the entire ecosystem.
LangChain provides abstractions to build **modular, composable AI pipelines** that interact with LLMs, tools, databases, and APIs.

This chapter summarizes the **fundamental building blocks** — from LLMs and prompts to tools, chains, agents, and the new `Runnable` interface — so you can clearly understand how everything fits together in later chapters.

---

## **2. Core Building Blocks**

### **2.1. LLMs (Large Language Models)**

* These are the engines that generate human-like text.
* LangChain supports many providers (OpenAI, Anthropic, HuggingFace, Ollama, etc.).
* You interact with them through standardized interfaces like `LLM` or `ChatModel`.

**Example:**

```python
from langchain_community.llms import OpenAI

llm = OpenAI(model="gpt-3.5-turbo")
response = llm.invoke("Write a short haiku about FastAPI.")
print(response)
```

---

### **2.2. Prompts**

Prompts are **structured templates** that define how you communicate with the model.

* A **PromptTemplate** uses placeholders for dynamic inputs.
* You can separate *prompt construction* from *execution*.

**Example:**

```python
from langchain.prompts import PromptTemplate

template = PromptTemplate(
    input_variables=["topic"],
    template="Explain {topic} in simple terms."
)
prompt = template.format(topic="LangServe")
```

---

### **2.3. Chains**

A **Chain** is a reusable workflow connecting components — e.g., a prompt → LLM → output parser.

LangChain offers both **prebuilt chains** and **custom pipelines**.

**Example:**

```python
from langchain.chains import LLMChain

chain = LLMChain(llm=llm, prompt=template)
result = chain.invoke({"topic": "FastAPI"})
print(result["text"])
```

---

### **2.4. Tools**

Tools are external functions or APIs that an agent can call to get real-world information.

Each tool is defined with:

* A name
* A function
* A description for the model

**Example:**

```python
from langchain.tools import Tool

def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny."

weather_tool = Tool(
    name="get_weather",
    func=get_weather,
    description="Fetch current weather for a city."
)
```

---

### **2.5. Agents**

Agents are **dynamic decision-makers** that decide which tool or chain to call next, based on the input and intermediate results.

They use an **LLM as a controller**, and tools as **actuators**.

**Example:**

```python
from langchain.agents import initialize_agent, AgentType

agent = initialize_agent(
    tools=[weather_tool],
    llm=llm,
    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

agent.invoke({"input": "What's the weather in Delhi?"})
```

---

## **3. Runnable Interface and Composition**

LangChain introduced the **Runnable Interface** to unify all building blocks under a single, composable pattern.

### **3.1. Runnable Basics**

Every core object now supports `.invoke()`, `.batch()`, `.stream()`, and `.ainvoke()` methods.

**Example:**

```python
from langchain.schema.runnable import RunnableMap, RunnableSequence

# Define a runnable sequence (Prompt → LLM → Output)
chain = RunnableSequence([
    template,
    llm
])
result = chain.invoke({"topic": "LangChain"})
```

---

### **3.2. RunnableMap**

`RunnableMap` allows **parallel execution** of multiple runnables.

**Example:**

```python
multi = RunnableMap({
    "summary": chain,
    "haiku": RunnableSequence([PromptTemplate.from_template("Write a haiku about {topic}"), llm])
})

result = multi.invoke({"topic": "FastAPI"})
print(result)
```

---

### **3.3. RunnableSequence**

`RunnableSequence` executes steps **in sequence**, passing outputs as inputs to the next step — similar to a functional pipeline.

This is the most common structure used when composing larger workflows.

---

## **4. Memory, Callbacks, and Streaming**

### **4.1. Memory**

Memory retains **context across multiple interactions**, allowing the agent to behave conversationally.

**Example:**

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

memory = ConversationBufferMemory()
conversation = ConversationChain(llm=llm, memory=memory)
conversation.invoke({"input": "Hi, my name is Desmond."})
conversation.invoke({"input": "What’s my name?"})
```

Output:

```
You said your name is Desmond.
```

---

### **4.2. Callbacks**

Callbacks provide hooks to observe and react to events during execution — useful for **logging, tracing, or monitoring**.

LangFuse and LangSmith use this system for observability.

**Example:**

```python
from langchain.callbacks import StdOutCallbackHandler

handler = StdOutCallbackHandler()
llm.invoke("Say hello!", callbacks=[handler])
```

---

### **4.3. Streaming**

For real-time applications (e.g., chat interfaces or dashboards), LangChain supports **token streaming** from models.

**Example:**

```python
for chunk in llm.stream("Write a poem about dashboards."):
    print(chunk, end="")
```

---

## **5. Summary**

| Concept              | Description                    | Example                             |
| -------------------- | ------------------------------ | ----------------------------------- |
| **LLM**              | Language model generating text | `OpenAI(model="gpt-4")`             |
| **PromptTemplate**   | Template to structure input    | `PromptTemplate("Explain {topic}")` |
| **Chain**            | Sequence of steps              | `LLMChain(prompt, llm)`             |
| **Tool**             | External callable              | `Tool(func=get_weather)`            |
| **Agent**            | LLM deciding next action       | `initialize_agent([...])`           |
| **RunnableSequence** | Sequential pipeline            | `[prompt → llm → parser]`           |
| **RunnableMap**      | Parallel pipelines             | `{summary, haiku}`                  |
| **Memory**           | Conversation persistence       | `ConversationBufferMemory()`        |
| **Callbacks**        | Execution monitoring           | `StdOutCallbackHandler()`           |
| **Streaming**        | Live output                    | `llm.stream()`                      |

---

## **6. Lab: Compose and Run a Mini Chain**

**Goal:** Build a composable chain using `RunnableSequence` and `RunnableMap`.

```python
from langchain.prompts import PromptTemplate
from langchain_community.llms import OpenAI
from langchain.schema.runnable import RunnableSequence, RunnableMap

llm = OpenAI(model="gpt-3.5-turbo")

summary_prompt = PromptTemplate.from_template("Summarize {topic} in one line.")
haiku_prompt = PromptTemplate.from_template("Write a haiku about {topic}.")

summary_chain = RunnableSequence([summary_prompt, llm])
haiku_chain = RunnableSequence([haiku_prompt, llm])

multi_chain = RunnableMap({
    "summary": summary_chain,
    "haiku": haiku_chain
})

result = multi_chain.invoke({"topic": "LangServe"})
print(result)
```

**Expected Output:**

```python
{
  'summary': 'LangServe exposes LangChain apps as APIs with ease.',
  'haiku': 'Chains that come alive / LangServe routes whisper and hum / APIs breathe with thought.'
}
```

---

## **7. What’s Next**

In the next chapter, we’ll connect these LangChain workflows to **FastAPI routes** using **LangServe**, enabling them to serve as **live, production-ready API endpoints**.

---
