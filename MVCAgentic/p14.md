# **Chapter 14: Logging, Monitoring & Debugging**

In this chapter, weâ€™ll explore how to **track**, **analyze**, and **debug** whatâ€™s happening inside your LangServe + FastAPI application â€” especially when working with LangChain components and external integrations.

---

## **1. Why Logging & Monitoring Matter**

In an AI-driven microservice ecosystem (like FastAPI + LangServe + Database + Streamlit/Dash), visibility is crucial.
Logs help you:

* Track user interactions.
* Debug broken or slow chains.
* Measure system performance.
* Audit requests and responses for compliance.

### **Common challenges without logging**

* Hard to debug chain errors.
* Difficult to trace prompts â†’ responses.
* No visibility into LLM token usage or latency.
* No way to monitor user-level activities.

---

## **2. Adding Structured Logging to FastAPI**

### **2.1 Using Pythonâ€™s built-in `logging` module**

```python
# app/core/logging_config.py
import logging

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
    handlers=[
        logging.StreamHandler(),  # console
        logging.FileHandler("logs/app.log")  # file-based logs
    ]
)

logger = logging.getLogger("app_logger")
```

---

### **2.2 Using in your FastAPI routes**

```python
# app/main.py
from fastapi import FastAPI
from app.core.logging_config import logger

app = FastAPI(title="LangServe Logging Demo")

@app.get("/health")
def health_check():
    logger.info("Health check endpoint accessed")
    return {"status": "ok"}

@app.post("/process")
def process_data(payload: dict):
    logger.info(f"Incoming request: {payload}")
    try:
        # ... call your LangServe chain here ...
        result = {"message": "Success"}
        logger.info(f"Processing result: {result}")
        return result
    except Exception as e:
        logger.exception("Error while processing request")
        return {"error": str(e)}
```

ðŸ’¡ **Tip:** Use `logger.exception()` instead of `logger.error()` when you want the stack trace printed automatically.

---

### **2.3 Structured JSON logs (for modern monitoring tools)**

Structured logs are better for tools like ELK (Elasticsearch + Logstash + Kibana), Datadog, or Grafana Loki.

```python
# app/core/json_logger.py
import json, logging

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_record = {
            "time": self.formatTime(record),
            "level": record.levelname,
            "message": record.getMessage(),
            "module": record.module,
            "line": record.lineno,
        }
        return json.dumps(log_record)

handler = logging.StreamHandler()
handler.setFormatter(JSONFormatter())
json_logger = logging.getLogger("json_logger")
json_logger.addHandler(handler)
json_logger.setLevel(logging.INFO)
```

---

## **3. Integrating Logging with LangServe**

LangServe already provides internal logging for requests, chains, and token usage, but you can extend this.

### **3.1 Add middleware to track all API calls**

```python
# app/core/middleware.py
from fastapi import Request
from starlette.middleware.base import BaseHTTPMiddleware
from app.core.logging_config import logger

class RequestLoggingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        logger.info(f"Request: {request.method} {request.url}")
        response = await call_next(request)
        logger.info(f"Response status: {response.status_code}")
        return response
```

```python
# app/main.py
from app.core.middleware import RequestLoggingMiddleware
app.add_middleware(RequestLoggingMiddleware)
```

This provides **end-to-end request tracking** even for LangServe endpoints.

---

## **4. Tracking LangChain Traces with LangFuse / OpenTelemetry**

### **4.1 Why use tracing tools?**

LangChain workflows can be complex â€” involving multiple chains, tools, retrievers, and callbacks.
Tracing tools like **LangFuse** or **OpenTelemetry** give you visual insights into:

* Token usage per component
* Prompt â†’ Response chains
* Latency and cost metrics
* Errors within nested chains

---

### **4.2 Using LangFuse with LangChain**

Install LangFuse SDK:

```bash
pip install langfuse
```

Initialize in your project:

```python
# app/core/langfuse_setup.py
from langfuse import Langfuse

langfuse = Langfuse(
    public_key="LF_PUBLIC_KEY",
    secret_key="LF_SECRET_KEY",
    host="https://cloud.langfuse.com"  # or self-hosted endpoint
)
```

Use with LangChain:

```python
from langchain_openai import ChatOpenAI
from langchain.callbacks.langfuse_callback import LangfuseCallbackHandler

from app.core.langfuse_setup import langfuse

callback = LangfuseCallbackHandler(langfuse)
llm = ChatOpenAI(model="gpt-4o", temperature=0.3, callbacks=[callback])

result = llm.invoke("Summarize this text about FastAPI logging.")
```

All your **LangServe calls and traces** now appear in the LangFuse dashboard automatically.

---

### **4.3 Using OpenTelemetry**

For enterprise monitoring, you can integrate with **OpenTelemetry (OTEL)**:

```bash
pip install opentelemetry-sdk opentelemetry-instrumentation-fastapi
```

```python
# app/core/otel_setup.py
from opentelemetry import trace
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter

provider = TracerProvider()
processor = BatchSpanProcessor(ConsoleSpanExporter())
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)
```

Then:

```python
from app.core.otel_setup import FastAPIInstrumentor

FastAPIInstrumentor.instrument_app(app)
```

This sends traces to console or external OTEL collector for visualization in **Grafana Tempo** or **Jaeger**.

---

## **5. Debugging LangServe Endpoints Locally**

### **5.1 Run LangServe in local mode**

```bash
uvicorn app.main:app --reload --log-level debug
```

LangServe will show:

* Registered endpoints (e.g. `/api/diagnose`)
* Streaming logs
* Token usage summaries (if LangFuse enabled)

---

### **5.2 Using curl or HTTPie for quick tests**

```bash
http POST :8000/api/summarize text="LangServe connects LangChain with FastAPI."
```

You should see JSON output.
If not, check:

* Console logs
* Stack trace in `app.log`
* Input format validation (Pydantic errors)

---

### **5.3 Debugging common LangServe issues**

| Problem                       | Likely Cause                    | Fix                                                        |
| ----------------------------- | ------------------------------- | ---------------------------------------------------------- |
| 404 on `/api/...`             | `add_routes()` not called       | Ensure `langserve.add_routes(app, chain, path="/api/...")` |
| 422 Unprocessable Entity      | Wrong request model             | Check Pydantic schema or request payload                   |
| Chain not returning output    | Missing `.invoke()` or `.run()` | Verify chain usage                                         |
| Streaming stops midway        | Callback error or timeout       | Wrap chain in `try/except` and log error                   |
| Trace not visible in LangFuse | Missing API keys or callback    | Recheck credentials in `.env`                              |

---

### **5.4 Debugging with VS Code**

Add a `.vscode/launch.json`:

```json
{
  "version": "0.2.0",
  "configurations": [
    {
      "name": "Debug LangServe App",
      "type": "python",
      "request": "launch",
      "module": "uvicorn",
      "args": ["app.main:app", "--reload", "--log-level", "debug"],
      "jinja": true
    }
  ]
}
```

Then press **F5** to start the server in debug mode.

---

## **6. Testing LangServe Endpoints**

Create a `tests/test_api.py` file:

```python
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)

def test_health_check():
    res = client.get("/health")
    assert res.status_code == 200
    assert res.json()["status"] == "ok"

def test_langserve_endpoint():
    res = client.post("/api/summarize", json={"text": "FastAPI integrates well with LangServe"})
    assert res.status_code == 200
```

âœ… Run:

```bash
pytest -v
```

---

## **7. Summary**

| Feature            | Purpose                           | Example Tool                |
| ------------------ | --------------------------------- | --------------------------- |
| Structured Logging | Track all requests and responses  | Python `logging`, JSON logs |
| Tracing            | Monitor LLM token usage, latency  | LangFuse, OpenTelemetry     |
| Debugging          | Find runtime errors quickly       | VS Code Debugger, Logs      |
| Testing            | Verify endpoints work as expected | Pytest, TestClient          |

---


