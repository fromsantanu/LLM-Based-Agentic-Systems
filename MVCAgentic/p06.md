*Chapter 6: Deploying LangChain Components via LangServe**

---

### üéØ **Goal:**

Turn your LangChain workflows into production-ready REST APIs using **LangServe**, complete with **streaming**, **validation**, and **error handling**.

---

## 6.1 What Is LangServe Deployment?

LangServe provides a **FastAPI-compatible interface** for deploying LangChain components‚Äîsuch as chains, agents, and retrievers‚Äîas **API endpoints**.
It allows developers to:

* Expose any LangChain `Runnable` via REST endpoints.
* Handle requests and responses in **JSON**.
* Support **streaming outputs** for long-running chains.
* Integrate with FastAPI‚Äôs middleware, authentication, and dependency injection.

---

## 6.2 Exposing a Chain as a REST API

### ‚úÖ **Basic Setup**

You can expose a LangChain chain using `add_routes`:

```python
# app/main.py
from fastapi import FastAPI
from langserve import add_routes
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# Create FastAPI app
app = FastAPI(title="LangServe Example API")

# Define a simple summarization chain
prompt = PromptTemplate.from_template("Summarize the following text:\n{text}")
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
chain = LLMChain(llm=llm, prompt=prompt)

# Expose the chain under /summarize endpoint
add_routes(app, chain, path="/summarize")
```

---

### üìÇ **Project Layout Example**

```
langserve_app/
‚îÇ
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py           # FastAPI entry point
‚îÇ   ‚îú‚îÄ‚îÄ chains/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ summarize_chain.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ classify_chain.py
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ logging.py
‚îÇ       ‚îî‚îÄ‚îÄ validators.py
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

### üöÄ **Run the App**

```bash
uvicorn app.main:app --reload
```

Then open in browser:
üëâ [http://localhost:8000/summarize/playground](http://localhost:8000/summarize/playground)

You‚Äôll see an **interactive LangServe playground** auto-generated by the library.

---

## 6.3 Example: `/summarize` ‚Üí Summarization Chain

### ‚ú≥Ô∏è **Request Example**

**POST /summarize/invoke**

```json
{
  "input": {
    "text": "LangChain simplifies building LLM-powered applications by modularizing core components."
  }
}
```

**Response:**

```json
{
  "output": "LangChain modularizes LLM app components for easier development."
}
```

---

## 6.4 Streaming Responses from LangServe Endpoints

When working with long responses (e.g., document summaries or conversations), streaming improves user experience.

You can enable streaming by using the `/stream` endpoint:

### Example with `curl`

```bash
curl -N -X POST http://localhost:8000/summarize/stream \
  -H "Content-Type: application/json" \
  -d '{"input": {"text": "This is a long text to summarize..."}}'
```

The response will stream tokens progressively, allowing live updates on the frontend.

---

### üß† **How It Works**

* LangServe converts your chain into a **Runnable** pipeline.
* `/invoke` runs synchronously (returns after completion).
* `/stream` sends partial outputs token by token (Server-Sent Events).

---

## 6.5 Handling Errors and Validation

LangServe integrates naturally with FastAPI‚Äôs validation and exception system.

### ‚úÖ **Custom Input Validation**

You can wrap your chain call with a **Pydantic model** for structured input.

```python
from pydantic import BaseModel, Field

class SummarizeInput(BaseModel):
    text: str = Field(..., min_length=10, description="Text to be summarized")
```

Then validate before invoking the chain:

```python
@app.post("/summarize_custom")
async def summarize_text(data: SummarizeInput):
    try:
        response = await chain.ainvoke({"text": data.text})
        return {"summary": response["text"]}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

## 6.6 Adding Logging and Error Handling

Use standard Python logging for observability.

```python
import logging

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

@app.middleware("http")
async def log_requests(request, call_next):
    logger.info(f"Incoming request: {request.method} {request.url}")
    try:
        response = await call_next(request)
        logger.info(f"Response status: {response.status_code}")
        return response
    except Exception as e:
        logger.error(f"Unhandled exception: {e}")
        raise
```

---

### ‚ö†Ô∏è Common Error Scenarios

| Error Type              | Cause                       | Solution                               |
| ----------------------- | --------------------------- | -------------------------------------- |
| `ValidationError`       | Missing or invalid input    | Use Pydantic models                    |
| `HTTPException`         | Manually raised errors      | Provide detailed `detail` message      |
| `LangChainRuntimeError` | Internal chain failure      | Wrap chain call in try-except          |
| Connection issues       | API key or network problems | Validate API key and LLM configuration |

---

## 6.7 Combining Multiple Chains

You can add multiple routes, each serving different chains:

```python
from app.chains.summarize_chain import summarize_chain
from app.chains.classify_chain import classify_chain

add_routes(app, summarize_chain, path="/summarize")
add_routes(app, classify_chain, path="/classify")
```

Each route automatically provides:

* `/invoke` (sync call)
* `/stream` (live streaming)
* `/batch` (bulk inputs)
* `/config` (metadata)
* `/playground` (web UI)

---

## 6.8 Summary

| Concept        | Description                                         |
| -------------- | --------------------------------------------------- |
| `add_routes()` | Registers a LangChain chain as an API route         |
| `/invoke`      | Synchronous endpoint for final output               |
| `/stream`      | Streaming endpoint for token-by-token output        |
| `/playground`  | Auto UI to test the chain                           |
| Validation     | Handled via Pydantic models                         |
| Logging        | Integrated with FastAPI middleware                  |
| Error Handling | Wrap chain calls in try-except, raise HTTPException |

---


