# **Chapter 11: Calling LangServe APIs from Dash**

This chapter demonstrates how to connect a **Dash web interface** to your **LangServe API endpoints**.
You‚Äôll learn how to build interactive input fields, trigger API calls via callbacks, handle asynchronous responses, and display results dynamically in the dashboard.

---

## **11.1 Overview**

Dash (by Plotly) provides an elegant and reactive web UI for Python apps.
LangServe exposes your LangChain pipelines as REST APIs.
Combining them lets you build **interactive dashboards powered by AI**, where LangServe handles the intelligence and Dash handles the interface.

Typical use cases:

* Text summarization tools
* Question‚Äìanswering dashboards
* Medical/diagnostic AI query interfaces
* Report generators and decision support dashboards

---

## **11.2 Basic Architecture**

```text
[User Input on Dash UI]
        ‚Üì
[dcc.Input + html.Button]
        ‚Üì (callback)
[async API call ‚Üí LangServe endpoint (/api/summarize, /api/ask, etc.)]
        ‚Üì
[Return JSON response]
        ‚Üì
[Update Dash Output Component dynamically]
```

---

## **11.3 Creating the Dash App Skeleton**

```python
# app.py
from dash import Dash, html, dcc, Input, Output, State
import dash_bootstrap_components as dbc
import requests
import asyncio
import aiohttp

app = Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])
server = app.server  # For deployment with FastAPI or Flask
```

---

## **11.4 UI Layout**

We‚Äôll build a simple UI that lets users enter text and get a summarized result from a LangServe endpoint (`/api/summarize`).

```python
app.layout = dbc.Container([
    html.H2("LangServe Summarization Dashboard", className="text-center mt-3"),
    
    dbc.Row([
        dbc.Col([
            dcc.Textarea(
                id='input-text',
                placeholder='Enter text to summarize...',
                style={'width': '100%', 'height': 200}
            ),
            html.Br(),
            dbc.Button("Summarize", id='submit-btn', color='primary', className='mt-2'),
        ], width=6),
        
        dbc.Col([
            html.H5("Result:"),
            html.Div(id='output-text', style={'whiteSpace': 'pre-wrap', 'border': '1px solid #ccc', 'padding': '10px'})
        ], width=6)
    ])
])
```

---

## **11.5 Making API Calls (Synchronous Version)**

If your LangServe endpoint returns results quickly, you can use a simple `requests.post()` inside the callback.

```python
@app.callback(
    Output('output-text', 'children'),
    Input('submit-btn', 'n_clicks'),
    State('input-text', 'value'),
    prevent_initial_call=True
)
def call_langserve_api(n_clicks, text):
    if not text:
        return "Please enter text."
    
    try:
        response = requests.post(
            "http://127.0.0.1:8000/api/summarize/invoke",
            json={"input": {"text": text}}
        )
        data = response.json()
        return data.get("output", "No response received.")
    
    except Exception as e:
        return f"Error: {str(e)}"
```

---

## **11.6 Handling Async API Calls**

For longer LLM tasks (e.g., summarization of long reports or multi-step reasoning), asynchronous calls help prevent UI blocking.

```python
@app.callback(
    Output('output-text', 'children'),
    Input('submit-btn', 'n_clicks'),
    State('input-text', 'value'),
    prevent_initial_call=True
)
def async_callback(n_clicks, text):
    return asyncio.run(call_async_api(text))


async def call_async_api(text):
    if not text:
        return "Please enter text."
    
    async with aiohttp.ClientSession() as session:
        try:
            async with session.post(
                "http://127.0.0.1:8000/api/summarize/invoke",
                json={"input": {"text": text}}
            ) as response:
                data = await response.json()
                return data.get("output", "No response received.")
        except Exception as e:
            return f"Error: {str(e)}"
```

---

## **11.7 Displaying Streaming Results (Optional)**

LangServe supports **streaming outputs** via `/stream` endpoints.
Dash can handle this using a **`dcc.Interval`** or **WebSocket client**.
Simplest approach: simulate incremental updates every few seconds.

```python
@app.callback(
    Output('output-text', 'children'),
    Input('submit-btn', 'n_clicks'),
    State('input-text', 'value'),
    prevent_initial_call=True
)
def stream_results(n_clicks, text):
    import time
    partial_output = ""
    for chunk in simulate_stream(text):  # Replace with actual streaming call
        partial_output += chunk
        time.sleep(0.5)
    return partial_output
```

> **Tip:** You can connect this pattern to `/api/summarize/stream` endpoint using `aiohttp` streaming.

---

## **11.8 Final Code Integration**

```python
if __name__ == "__main__":
    app.run_server(debug=True, port=8050)
```

Now, visit:
üëâ **[http://127.0.0.1:8050](http://127.0.0.1:8050)**
Type your text ‚Üí press ‚ÄúSummarize‚Äù ‚Üí view LangServe response in real-time.

---

## **11.9 Error Handling and UX Tips**

* Show loading spinners: `dbc.Spinner` or `dcc.Loading`
* Handle network issues gracefully with fallback messages
* Cache recent inputs using `dcc.Store` or backend memory
* Limit text input size if needed (to prevent API overload)
* For multi-agent pipelines, use tabs or collapsible sections for clarity

---

## **11.10 Example: Extending to Question‚ÄìAnswering**

Simply change the endpoint and labels:

```python
response = requests.post(
    "http://127.0.0.1:8000/api/qa/invoke",
    json={"input": {"question": text}}
)
```

Rename button ‚Üí ‚ÄúAsk‚Äù
Output ‚Üí model‚Äôs answer.

---

## **Summary**

| Concept              | Description                                                       |
| -------------------- | ----------------------------------------------------------------- |
| **Dash Components**  | Use `dcc.Input`, `dcc.Textarea`, `html.Button`, `html.Div` for UI |
| **Callbacks**        | Trigger API calls and update outputs dynamically                  |
| **Async API Calls**  | Use `aiohttp` to handle long-running LangServe tasks              |
| **Streaming**        | Connect `/stream` endpoints for real-time LLM output              |
| **Integration Goal** | Let Dash act as frontend to your LangServe-powered AI backend     |

---

