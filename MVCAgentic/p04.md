# **Chapter 4: Connecting FastAPI with LangServe**

### **Goal**

Integrate **LangServe** into a running **FastAPI** app so both:

* Standard REST endpoints
* and **LangChain-based endpoints** (served through LangServe)

work seamlessly together under a unified API service.

---

## **1. Why Combine FastAPI and LangServe?**

LangServe lets you **serve LangChain chains and agents as API endpoints** automatically — but in a real system, you also want normal REST routes (e.g., `/health`, `/users`, `/dashboard`).

By embedding LangServe inside a FastAPI app, you can:

* Expose **LangChain-powered endpoints** like `/api/agents/triage`
* Keep your **custom FastAPI routes** like `/api/patients`
* Share config, dependencies, and application state across both.

---

## **2. Basic Structure**

Your folder layout might look like:

```
app/
│
├── main.py
├── config.py
├── controllers/
│   ├── patients.py
│   └── utils.py
├── langserve/
│   ├── __init__.py
│   └── chains.py
└── models/
    └── patient_model.py
```

* `controllers/` → your FastAPI REST controllers
* `langserve/` → defines LangChain chains or agents to serve via LangServe
* `main.py` → mounts both FastAPI and LangServe routes

---

## **3. Mounting LangServe Routes**

LangServe uses the `add_routes()` helper to bind a **LangChain Runnable** or **chain** to a FastAPI router.

### **Example: `main.py`**

```python
from fastapi import FastAPI
from langserve import add_routes
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from langchain.chat_models import ChatOpenAI

# 1️⃣ Create FastAPI app
app = FastAPI(title="FastAPI + LangServe Example")

# 2️⃣ Define a LangChain chain
llm = ChatOpenAI(model="gpt-3.5-turbo")
memory = ConversationBufferMemory()
chain = ConversationChain(llm=llm, memory=memory)

# 3️⃣ Mount LangServe route under /api/agent
add_routes(app, chain, path="/api/agent")

# 4️⃣ Normal FastAPI route
@app.get("/api/health")
async def health_check():
    return {"status": "ok"}

# 5️⃣ Root
@app.get("/")
async def home():
    return {"message": "Welcome to FastAPI + LangServe!"}
```

---

### **Run the App**

```bash
uvicorn app.main:app --reload
```

* Visit `http://127.0.0.1:8000/` → basic FastAPI route
* Visit `http://127.0.0.1:8000/api/agent/playground` → LangServe UI playground

✅ **You now have both FastAPI and LangServe endpoints running together.**

---

## **4. Grouping LangServe Endpoints**

When you have multiple chains or agents, it’s best to **group them** under a common prefix (e.g., `/api/agents`).

### Example: `langserve/chains.py`

```python
from langchain.chains import ConversationChain
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory

def get_conversation_chain():
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    memory = ConversationBufferMemory()
    return ConversationChain(llm=llm, memory=memory)
```

Then in `main.py`:

```python
from fastapi import FastAPI
from langserve import add_routes
from app.langserve.chains import get_conversation_chain

app = FastAPI(title="LangServe Integrated API")

conversation_chain = get_conversation_chain()

# Mount all LangServe routes under /api/agents/
add_routes(app, conversation_chain, path="/api/agents/conversation")
```

This structure makes it easy to add more LangChain agents later.

---

## **5. Managing Shared State**

You might need to share config, environment variables, or database connections between FastAPI routes and LangChain components.

### Example: Using `app.state`

```python
from fastapi import FastAPI
from langserve import add_routes
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI

app = FastAPI()

# Store shared configuration/state
app.state.model_name = "gpt-4o-mini"

# Define chain that reads from app.state
def create_chain(app):
    prompt = PromptTemplate.from_template("Summarize: {text}")
    llm = ChatOpenAI(model=app.state.model_name)
    return LLMChain(prompt=prompt, llm=llm)

# Mount
add_routes(app, create_chain(app), path="/api/agents/summarizer")

@app.get("/api/config")
def show_config():
    return {"model_in_use": app.state.model_name}
```

Now both:

* FastAPI controllers and
* LangChain agents

can **share the same configuration and memory** objects.

---

## **6. Handling Session or Memory**

LangServe manages **stateful memory** internally using LangChain’s memory classes, but sometimes you want per-user memory.

For multi-user apps:

* Use `ConversationBufferMemory()` per user ID.
* Store in Redis or database.
* Reconstruct memory when a user resumes a conversation.

You can pass session keys using FastAPI middleware or query params:

```python
@app.middleware("http")
async def add_user_memory(request, call_next):
    user_id = request.headers.get("x-user-id", "guest")
    request.state.user_id = user_id
    response = await call_next(request)
    return response
```

Then your LangChain builder can fetch user memory based on `request.state.user_id`.

---

## **7. Example Workflow**

| Task               | Handler         | Endpoint                   |
| ------------------ | --------------- | -------------------------- |
| Basic health check | FastAPI route   | `/api/health`              |
| Chat conversation  | LangServe chain | `/api/agents/conversation` |
| Summarization      | LangServe chain | `/api/agents/summarizer`   |
| Config fetch       | FastAPI route   | `/api/config`              |

All served from **one running FastAPI instance**.

---

## **8. Lab Exercise**

**Goal:** Create a working FastAPI app that:

1. Has a `/api/summary` LangServe chain to summarize user text.
2. Has a `/api/ping` route returning `{ "pong": true }`.
3. Shares one global model name via `app.state.model_name`.

✅ **Bonus:** Add another route `/api/playground` where you list all available agent endpoints for testing.

---

## **9. Key Takeaways**

* LangServe routes can be **mounted directly on FastAPI** using `add_routes()`.
* Both systems can **share state** via `app.state` or dependency injection.
* LangServe automatically provides a **web playground** for each chain.
* With proper structuring, FastAPI + LangServe gives you a **complete API layer** — part REST, part LLM-driven intelligence.

---
