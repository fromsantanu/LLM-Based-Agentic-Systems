# **Chapter 1: Introduction to LangServe**

---

## üéØ **Goal**

To understand what **LangServe** is, how it connects with **LangChain**, and how you can expose LangChain workflows as **live API endpoints**‚Äîready for use in dashboards, web apps, or agentic systems.

---

## üß† **1. What is LangServe?**

**LangServe** is a **framework built by LangChain** that makes it **easy to deploy LangChain components (chains, agents, tools)** as **web APIs**.
It provides a FastAPI-based service layer around your LangChain logic.

### üîπ Think of it as:

> ‚ÄúA bridge between your LangChain code and the real world.‚Äù

With LangServe, you can take a working Python chain and serve it over HTTP or WebSocket‚Äîjust like any normal REST API.

---

### üîç **Example Concept**

Without LangServe:

```python
result = chain.invoke({"question": "What is LangServe?"})
```

With LangServe:

```bash
POST /invoke
{
  "input": {"question": "What is LangServe?"}
}
```

You can now call it from **Streamlit, Dash, or even cURL**.

---

## ‚öôÔ∏è **2. How LangServe Relates to LangChain**

LangServe sits **on top of** LangChain‚Äôs runtime interface, called the **Runnable Interface**.
Here‚Äôs the hierarchy:

| Layer                  | Role                                                      | Example                                  |
| ---------------------- | --------------------------------------------------------- | ---------------------------------------- |
| **LangChain Core**     | Defines common building blocks like chains, tools, agents | `LLMChain`, `AgentExecutor`              |
| **Runnable Interface** | Provides a unified way to execute any LangChain component | `.invoke()`, `.batch()`, `.stream()`     |
| **LangServe**          | Wraps these runnables into web services                   | `/invoke`, `/stream`, `/batch` endpoints |

So, **LangServe = LangChain + FastAPI + Deployment layer**

---

## üß© **3. Core Concepts**

### üü£ **Endpoint**

Each LangChain object (chain, retriever, agent) becomes an **endpoint**‚Äîa callable API resource such as:

```
/invoke       ‚Äì for synchronous calls
/stream       ‚Äì for streaming token results
/batch        ‚Äì for processing multiple inputs
```

---

### üü° **Chain**

A **chain** is a pipeline of operations:
Input ‚Üí Processing ‚Üí Output.

Example:

```python
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain

prompt = PromptTemplate.from_template("Translate this English text to French: {text}")
model = ChatOpenAI(model="gpt-3.5-turbo")
chain = LLMChain(llm=model, prompt=prompt)
```

---

### üîµ **Runnable Interface**

All modern LangChain objects (Chains, LLMs, Retrievers, Tools) implement the **Runnable** interface.

Core methods:

* `invoke(input)` ‚Üí run once
* `batch(inputs)` ‚Üí run multiple
* `stream(input)` ‚Üí yield incremental outputs

This makes the components composable and ‚Äúservable‚Äù.

---

## üîÅ **4. LangServe vs FastAPI**

| Aspect        | **LangServe**                                       | **FastAPI**                                 |
| ------------- | --------------------------------------------------- | ------------------------------------------- |
| **Purpose**   | Serve LangChain components as APIs                  | Build generic REST APIs                     |
| **Setup**     | Auto-generates endpoints from `Runnable` objects    | Manual endpoint coding                      |
| **Streaming** | Built-in `/stream` support                          | Manual WebSocket setup                      |
| **Docs UI**   | Auto-generated Swagger (like FastAPI)               | Swagger by default                          |
| **Use Case**  | Ideal for serving LLM workflows, chains, and agents | Ideal for any backend API or business logic |

‚úÖ You can **combine both**:
Use **FastAPI** for user routes and admin logic,
and **LangServe** to serve your AI workflows as sub-apps.

---

## ‚ö° **5. Example ‚Äì Turning a LangChain Chain into a Live API**

Here‚Äôs a minimal working example.

### üìÅ **File: `server.py`**

```python
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from langserve import LangServe

# 1. Define your LangChain chain
prompt = PromptTemplate.from_template("Translate this English text to French: {text}")
model = ChatOpenAI(model="gpt-3.5-turbo")
chain = LLMChain(llm=model, prompt=prompt)

# 2. Create LangServe app
app = LangServe(
    routes={"/translate": chain},  # One route per chain
    title="LangServe Demo",
    description="A simple LangServe app for translation"
)

# 3. Run the app
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)
```

---

### ‚ñ∂Ô∏è **Run it**

```bash
uvicorn server:app --reload
```

Then open your browser:

```
http://127.0.0.1:8000/docs
```

You‚Äôll see the `/translate/invoke` endpoint, ready to test!

---

### üß™ **Test Request**

```bash
curl -X POST "http://127.0.0.1:8000/translate/invoke" \
     -H "Content-Type: application/json" \
     -d '{"input": {"text": "Good morning"}}'
```

**Response**

```json
{"output": "Bonjour"}
```

---

## üß≠ **Summary**

* **LangServe** turns LangChain components into production-ready web APIs.
* Built on **FastAPI**, but specialized for **LLM-based pipelines**.
* Provides endpoints for `invoke`, `batch`, and `stream`.
* Perfectly integrates with **Streamlit**, **Dash**, or **JS clients**.
* You can deploy multiple chains in a single app for complex workflows.

---

## üß© **Mini-Exercise**

1. Install LangServe:

   ```bash
   pip install langserve langchain openai
   ```

2. Modify the example so that it **summarizes a paragraph** instead of translating.

3. Visit `http://127.0.0.1:8000/docs` and test your new API.

---
