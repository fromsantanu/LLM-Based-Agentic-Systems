# **Chapter 8: Authentication and User Context**

### **Goal**

Enable user authentication in your FastAPI + LangServe setup and manage user-specific contexts for your LangChain agents ‚Äî so each user has a personalized memory, history, and responses.

---

## **8.1 Overview**

In a real-world system, multiple users will interact with your LangServe endpoints.
You need:

1. **Authentication** ‚Äî Verify users and issue tokens/sessions.
2. **User Context** ‚Äî Pass the logged-in user's info (e.g., name, role, ID) to the LangChain chain or agent.
3. **Session Memory** ‚Äî Keep each user‚Äôs chat/memory isolated and persistent across interactions.

---

## **8.2 Adding User Login via FastAPI Routes**

### **Step 1: Create an Auth Router**

File: `routes/auth.py`

```python
from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordRequestForm
from pydantic import BaseModel
from datetime import datetime, timedelta
from jose import jwt, JWTError

SECRET_KEY = "secret_demo_key"
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30

router = APIRouter(prefix="/auth", tags=["auth"])

# Fake user data
fake_users_db = {
    "admin@example.com": {"password": "admin123", "full_name": "Admin User"},
    "user@example.com": {"password": "user123", "full_name": "Regular User"}
}

class Token(BaseModel):
    access_token: str
    token_type: str

def create_access_token(data: dict, expires_delta: timedelta | None = None):
    to_encode = data.copy()
    expire = datetime.utcnow() + (expires_delta or timedelta(minutes=15))
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

@router.post("/login", response_model=Token)
def login(form_data: OAuth2PasswordRequestForm = Depends()):
    user = fake_users_db.get(form_data.username)
    if not user or user["password"] != form_data.password:
        raise HTTPException(status_code=400, detail="Invalid credentials")

    token_data = {"sub": form_data.username}
    access_token = create_access_token(data=token_data, expires_delta=timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES))
    return {"access_token": access_token, "token_type": "bearer"}
```

‚úÖ **What this does**

* Implements `/auth/login` endpoint.
* Validates credentials.
* Issues JWT token with expiry time.

---

### **Step 2: Add Token Verification Dependency**

File: `core/security.py`

```python
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from jose import jwt, JWTError

SECRET_KEY = "secret_demo_key"
ALGORITHM = "HS256"

oauth2_scheme = OAuth2PasswordBearer(tokenUrl="/auth/login")

def get_current_user(token: str = Depends(oauth2_scheme)):
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username: str = payload.get("sub")
        if username is None:
            raise HTTPException(status_code=401, detail="Invalid authentication token")
        return {"username": username}
    except JWTError:
        raise HTTPException(status_code=401, detail="Invalid authentication token")
```

‚úÖ **What this does**

* Extracts JWT from the header.
* Validates token.
* Returns the username for downstream routes.

---

## **8.3 Passing User Context to LangChain Agents**

LangServe endpoints can access user context and feed it to the chain dynamically.

### **Example: Personalized Chain**

File: `chains/personalized_agent.py`

```python
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.schema import StrOutputParser

def get_personalized_chain():
    prompt = ChatPromptTemplate.from_template("""
    You are a helpful AI assistant.
    The current user is {user_name}.
    Use their name in your responses politely.
    User says: {input}
    """)
    model = ChatOpenAI(model="gpt-4o-mini")
    return prompt | model | StrOutputParser()
```

---

### **FastAPI Route that Passes User Context**

File: `routes/chat.py`

```python
from fastapi import APIRouter, Depends
from chains.personalized_agent import get_personalized_chain
from core.security import get_current_user

router = APIRouter(prefix="/chat", tags=["chat"])

@router.post("/")
async def chat_with_agent(input: str, user=Depends(get_current_user)):
    chain = get_personalized_chain()
    response = chain.invoke({"input": input, "user_name": user["username"]})
    return {"response": response}
```

‚úÖ **Result**
Each authenticated user gets personalized responses like:

> ‚ÄúHello **[user@example.com](mailto:user@example.com)**, how can I assist you today?‚Äù

---

## **8.4 Managing Session-Based Memory per User**

LangChain provides various memory types (e.g., `ConversationBufferMemory`, `ConversationSummaryMemory`).
You can store each user‚Äôs memory in a **per-user session dictionary** or a **database table**.

### **Example: In-Memory Store**

File: `core/memory_store.py`

```python
from langchain.memory import ConversationBufferMemory

user_sessions = {}

def get_user_memory(user_id: str):
    if user_id not in user_sessions:
        user_sessions[user_id] = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    return user_sessions[user_id]
```

---

### **Attach Memory to Chain**

File: `chains/chat_with_memory.py`

```python
from langchain.chains import ConversationChain
from langchain.chat_models import ChatOpenAI
from core.memory_store import get_user_memory

def get_memory_chain(user_id: str):
    memory = get_user_memory(user_id)
    llm = ChatOpenAI(model="gpt-4o-mini")
    return ConversationChain(llm=llm, memory=memory)
```

---

### **Route Using Memory Chain**

File: `routes/memory_chat.py`

```python
from fastapi import APIRouter, Depends
from core.security import get_current_user
from chains.chat_with_memory import get_memory_chain

router = APIRouter(prefix="/memory-chat", tags=["memory-chat"])

@router.post("/")
async def chat_with_memory(input: str, user=Depends(get_current_user)):
    chain = get_memory_chain(user["username"])
    response = chain.run(input)
    return {"response": response}
```

‚úÖ **Effect**
Each user‚Äôs chat context persists during their session.
Different users ‚Üí different memory threads.

---

## **8.5 Mount Routes in main.py**

```python
from fastapi import FastAPI
from routes import auth, chat, memory_chat

app = FastAPI(title="LangServe Auth Demo")

app.include_router(auth.router)
app.include_router(chat.router)
app.include_router(memory_chat.router)
```

---

## **8.6 Optional: Persistent Memory (Database or Redis)**

For production, you can store session memory in a database:

* Create a `user_sessions` table with fields: `user_id`, `session_data`, `last_updated`.
* Serialize `ConversationBufferMemory` with `pickle` or JSON.
* Load and save after each chat message.

---

## **8.7 Summary**

| Layer                          | Responsibility                   |
| ------------------------------ | -------------------------------- |
| `/auth/login`                  | Authenticate and return JWT      |
| `get_current_user()`           | Decode JWT and provide user info |
| `chains/personalized_agent.py` | Inject user name into prompts    |
| `core/memory_store.py`         | Maintain per-user session memory |
| `/memory-chat` route           | Keep chat context alive per user |
| LangServe                      | Serve endpoints as API routes    |

---

## **üß™ Lab Exercise**

**Task:**

1. Login as `user@example.com` (password: `user123`)
2. Use the token to call `/chat` and `/memory-chat` endpoints.
3. Observe how user-specific responses and memories differ.

**Hint:**
You can use `httpie` or `curl`:

```bash
http POST :8000/auth/login username=user@example.com password=user123
http POST :8000/memory-chat/ "input=Hello again" "Authorization: Bearer <token>"
```

---

## **Next Chapter Preview**

**Chapter 9: Monitoring & Tracing**

* Integrating LangFuse or OpenTelemetry
* Tracking chain usage per user
* Building analytics dashboards

---
