# **Chapter 10: Calling LangServe APIs from Streamlit**

In this chapter, weâ€™ll connect everything we built so far â€” the **LangServe API backend** (FastAPI + LangChain) â€” with a **Streamlit frontend** that interacts with it in real time.
This completes the loop between the user interface and your LLM or agentic logic deployed through LangServe.

---

## **1. Overview**

LangServe exposes LangChain chains, agents, or tools as REST APIs.
Your frontend (for example, Streamlit) can call these endpoints using:

* Pythonâ€™s built-in `requests` library (for simple synchronous calls)
* `aiohttp` or `httpx` (for asynchronous streaming responses)

### **Goal of this chapter**

You will learn how to:

* Make POST requests from Streamlit to LangServe
* Display streaming responses from LLM endpoints
* Handle structured JSON results and display them as tables or text

---

## **2. Example LangServe Endpoint Recap**

Letâ€™s assume you already have a LangServe app exposing a summarization chain like this:

```python
# server.py
from fastapi import FastAPI
from langserve import add_routes
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_community.llms import OpenAI

app = FastAPI(title="LangServe Demo")

prompt = PromptTemplate.from_template("Summarize the following text:\n\n{text}")
chain = LLMChain(llm=OpenAI(), prompt=prompt)

add_routes(app, chain, path="/summarize")
```

This means that a `POST` request to
`http://localhost:8000/summarize/invoke`
expects a JSON body:

```json
{"input": {"text": "Your input text here"}}
```

---

## **3. Calling LangServe Endpoint from Streamlit**

### **3.1 Using `requests` (Simple Case)**

```python
# app.py
import streamlit as st
import requests

st.title("LangServe Summarizer")

user_input = st.text_area("Enter text to summarize:")

if st.button("Summarize"):
    with st.spinner("Summarizing..."):
        response = requests.post(
            "http://localhost:8000/summarize/invoke",
            json={"input": {"text": user_input}}
        )
        if response.status_code == 200:
            result = response.json()
            st.subheader("Summary:")
            st.write(result["output"])
        else:
            st.error(f"Error: {response.status_code}")
```

âœ… **Explanation:**

* The `/invoke` route is a standard LangServe endpoint.
* You send your input inside an `{"input": {...}}` object.
* LangServe replies with JSON: `{"output": <model response>}`.

---

## **4. Displaying Streaming Responses**

For long-running or LLM responses, you can stream tokens in real-time.
LangServe supports `/stream` endpoints which yield partial results as the model generates text.

### **4.1 Using `aiohttp` for Streaming**

```python
# stream_app.py
import streamlit as st
import aiohttp
import asyncio

st.title("Streaming Response Demo")

user_input = st.text_area("Enter your text for summarization:")

async def get_stream():
    url = "http://localhost:8000/summarize/stream"
    async with aiohttp.ClientSession() as session:
        async with session.post(url, json={"input": {"text": user_input}}) as resp:
            st.write("Response stream:")
            placeholder = st.empty()
            accumulated_text = ""
            async for line in resp.content:
                decoded = line.decode("utf-8")
                accumulated_text += decoded
                placeholder.markdown(accumulated_text)

if st.button("Stream Summary"):
    asyncio.run(get_stream())
```

âœ… **Explanation:**

* `aiohttp` allows asynchronous streaming of partial tokens.
* `resp.content` yields text chunks as they arrive.
* We update the UI using a `placeholder` container.

---

## **5. Handling Structured JSON Responses**

LangServe chains can also return structured outputs, e.g., key-value JSONs.
Hereâ€™s how to display them neatly in Streamlit.

### **Example:**

Suppose your LangServe endpoint `/analyze` returns:

```json
{
  "output": {
    "sentiment": "Positive",
    "confidence": 0.94,
    "summary": "The text expresses happiness and satisfaction."
  }
}
```

### **Frontend Display:**

```python
import streamlit as st
import requests
import pandas as pd

st.title("LangServe JSON Response Example")

user_text = st.text_input("Enter text to analyze:")
if st.button("Analyze"):
    response = requests.post(
        "http://localhost:8000/analyze/invoke",
        json={"input": {"text": user_text}}
    )
    if response.status_code == 200:
        data = response.json()["output"]
        st.json(data)
        st.subheader("Tabular View:")
        st.dataframe(pd.DataFrame([data]))
    else:
        st.error("Request failed!")
```

âœ… **Result:**

* You get both a readable JSON and a pandas-styled table.
* Perfect for displaying structured responses (like predictions, scores, etc.).

---

## **6. Error Handling and Validation**

Always handle unexpected responses gracefully.

```python
try:
    response = requests.post(API_URL, json=payload)
    response.raise_for_status()
    result = response.json()
    st.success(result["output"])
except requests.exceptions.RequestException as e:
    st.error(f"Request failed: {e}")
except KeyError:
    st.error("Malformed response from server!")
```

---

## **7. Summary**

| **Topic**                        | **Key Concept**                                  |
| -------------------------------- | ------------------------------------------------ |
| Calling LangServe from Streamlit | Use `/invoke` or `/stream` endpoints             |
| Simple requests                  | Use `requests` library                           |
| Streaming responses              | Use `aiohttp` or `httpx.AsyncClient`             |
| Display JSON                     | Use `st.json()` or convert to `pandas.DataFrame` |
| Error handling                   | Always validate response structure               |

---

## **8. Practice Task**

âœ… **Exercise:**

1. Create a LangServe endpoint `/qa` that takes a question and context text and returns an answer.
2. Build a Streamlit app that:

   * Accepts a context paragraph and question.
   * Calls `/qa/invoke` endpoint.
   * Displays the answer and confidence score in a table.

ðŸ’¡ *Hint:* You can use a LangChain `RetrievalQA` or simple `LLMChain` for the backend.

---
