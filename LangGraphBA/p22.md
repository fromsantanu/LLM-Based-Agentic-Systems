# Chapter 22: Production Deployment

So far, we‚Äôve designed, tested, and refined LangGraph workflows. The final step is making them **production-ready**: exposing them as APIs, running them in scalable environments (like serverless platforms or Docker containers), and setting up monitoring to ensure reliability.

In this chapter, we‚Äôll cover:

* Exposing your LangGraph workflow as an **API endpoint**.
* Deploying using **serverless frameworks**.
* Running in **Docker containers**.
* Adding **monitoring & logging**.

---

## 22.1 Expose as an API Endpoint

LangGraph workflows can be served through a lightweight **FastAPI** or **Flask** application. This lets other systems call your graph as a REST endpoint.

### Example: Triage Graph API

```python
from fastapi import FastAPI
from langgraph.graph import StateGraph, END

# Example triage graph
def intake_node(state):
    return {"symptom": state.get("input", "Unknown")}

def triage_node(state):
    if "fever" in state["symptom"].lower():
        return {"priority": "High"}
    return {"priority": "Normal"}

graph = StateGraph(dict)
graph.add_node("intake", intake_node)
graph.add_node("triage", triage_node)
graph.set_entry_point("intake")
graph.add_edge("intake", "triage")
graph.add_edge("triage", END)

# Compile graph
app_graph = graph.compile()

# FastAPI app
app = FastAPI()

@app.post("/triage")
def run_triage(input: str):
    result = app_graph.invoke({"input": input})
    return result
```

‚û°Ô∏è Now you can run:

```bash
uvicorn app:app --reload
```

And access:

```bash
POST http://localhost:8000/triage
{ "input": "Fever and cough" }
```

---

## 22.2 Deploy Serverless

For **event-driven workflows** or lightweight APIs, serverless platforms like **AWS Lambda, Google Cloud Functions, or Vercel** are great options.

* **Pros**: Auto-scaling, pay-per-use, no server management.
* **Cons**: Cold starts, execution time/memory limits.

üîπ Example: Wrap the FastAPI app with **Mangum** (for AWS Lambda):

```python
from mangum import Mangum
handler = Mangum(app)  # app is your FastAPI instance
```

Deploy to Lambda ‚Üí now `/triage` is available serverlessly.

---

## 22.3 Deploy with Docker

For **full control** over dependencies and environments, Docker is the best option.

### Example `Dockerfile`

```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
```

Build & run:

```bash
docker build -t triage-api .
docker run -p 8000:8000 triage-api
```

Deploy this container on **Kubernetes, ECS, or GCP Cloud Run** for production scaling.

---

## 22.4 Monitoring & Logging

Once deployed, you need to **track performance and errors**.

* **Logging**: Use Python‚Äôs `logging` module or integrate with services like **ELK (Elasticsearch + Kibana)**.
* **Monitoring**: Add **Prometheus + Grafana** or hosted options (Datadog, New Relic).
* **Tracing**: Use **Langfuse** or **OpenTelemetry** to track workflow steps.

### Example: Add Logging

```python
import logging

logging.basicConfig(level=logging.INFO)

@app.post("/triage")
def run_triage(input: str):
    logging.info(f"Received input: {input}")
    result = app_graph.invoke({"input": input})
    logging.info(f"Graph output: {result}")
    return result
```

---

## ‚úÖ Key Takeaways

* Wrap your graph in **FastAPI/Flask** to expose it as an API.
* Use **serverless platforms** for lightweight, event-driven use cases.
* Use **Docker** for full control and production scaling.
* Add **logging, monitoring, and tracing** for reliability and debugging.

---

## üîπ Example Recap

We exposed a **Triage Graph** as an API endpoint, wrapped it for **serverless deployment**, containerized it with **Docker**, and added **monitoring hooks** for real-world reliability.

---

