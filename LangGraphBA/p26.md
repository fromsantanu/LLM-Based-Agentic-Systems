# Chapter 26: Document Workflow

Modern workflows often involve handling documents—scanned files, PDFs, or images. LangGraph makes it possible to design **document processing pipelines** that can handle upload, extraction, summarization, storage, and query-based retrieval.

---

## Key Idea

Build a **document ingestion pipeline** that can:

1. Accept uploaded files (PDFs, images).
2. Perform **OCR** (Optical Character Recognition) to extract text.
3. Summarize the extracted content for quick review.
4. Store the content in a database or vector store.
5. Retrieve documents or summaries when queried.

---

## Workflow Design

**Flow:**

```
Upload → OCR → Summarize → Store → Retrieve on Query
```

* **Upload Node**: Accepts PDF/image input.
* **OCR Node**: Uses an OCR tool (e.g., Tesseract, AWS Textract) to extract text.
* **Summarize Node**: Runs an LLM summarizer to produce a concise summary.
* **Store Node**: Persists extracted text + summary in a database/vector store.
* **Retrieve Node**: On a user query, fetches relevant docs and provides summaries.

---

## Example Graph

```python
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from typing import TypedDict, List

# Define state
class DocState(TypedDict):
    file_path: str
    text: str
    summary: str
    query: str
    retrieved_docs: List[str]

# Mock OCR function
def ocr_tool(state: DocState):
    # Imagine OCR logic here (e.g., pytesseract.image_to_string)
    return {"text": f"Extracted text from {state['file_path']}"}

# Summarizer function
def summarize_tool(state: DocState):
    return {"summary": f"Summary of: {state['text'][:100]}"}

# Storage
doc_db = {}

def store_tool(state: DocState):
    doc_db[state['file_path']] = {"text": state["text"], "summary": state["summary"]}
    return {}

# Retrieval
def retrieve_tool(state: DocState):
    # simple keyword match
    results = [v["summary"] for k,v in doc_db.items() if state["query"].lower() in v["text"].lower()]
    return {"retrieved_docs": results}

# Build graph
graph = StateGraph(DocState)

graph.add_node("ocr", ocr_tool)
graph.add_node("summarize", summarize_tool)
graph.add_node("store", store_tool)
graph.add_node("retrieve", retrieve_tool)

graph.set_entry_point("ocr")
graph.add_edge("ocr", "summarize")
graph.add_edge("summarize", "store")
graph.add_edge("store", END)

# Retrieval subflow
graph.add_edge("retrieve", END)

app = graph.compile()
```

---

## Example Run

```python
# Upload & Process
state = {"file_path": "contract1.pdf"}
state = app.invoke(state)

print(doc_db)  
# {'contract1.pdf': {'text': 'Extracted text from contract1.pdf',
#                    'summary': 'Summary of: Extracted text from contract1.pdf'}}

# Query
state = {"query": "contract terms"}
state = app.invoke(state, start_at="retrieve")
print(state["retrieved_docs"])
# ['Summary of: Extracted text from contract1.pdf']
```

---

## Real-World Use Cases

* **Healthcare**: Upload scanned patient records → OCR → Summarize → Store → Retrieve by patient ID.
* **Legal**: Ingest contracts → Extract clauses → Summarize → Query for specific legal terms.
* **Education**: Collect handwritten assignments → OCR → Summarize → Store in a searchable archive.

---

✅ With this workflow, you now have a **document ingestion pipeline** that can scale across domains—turning unstructured documents into structured, searchable knowledge.

---
