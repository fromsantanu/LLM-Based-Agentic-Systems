# Chapter 11: Parallel Execution

In many workflows, tasks donâ€™t need to be executed sequentially. Instead, they can run **concurrently**, saving time and making your system more efficient. LangGraph supports **parallel execution** by allowing multiple nodes to process data at the same time, as long as they donâ€™t depend on each otherâ€™s outputs.

---

## ðŸ”¹ Why Parallel Execution Matters

* **Efficiency**: Tasks that donâ€™t depend on each other can run simultaneously.
* **Speed**: Reduce total execution time for workflows with independent branches.
* **Flexibility**: Process multiple perspectives or formats concurrently.

---

## ðŸ”¹ How It Works in LangGraph

* You can connect one node to multiple downstream nodes.
* When the upstream node finishes, LangGraph **forks** the data to all connected nodes.
* Each downstream node runs **in parallel**.
* Results can be collected later using a **join node** or by aggregating the outputs.

---

## ðŸ”¹ Example Scenario

ðŸ‘‰ Suppose we want to summarize a text in **three different styles**:

1. **Formal**
2. **Casual**
3. **Bullet points**

These tasks are independent, so we can run them in parallel.

---

### âœ¨ Example Code

```python
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI

# Define our state (what flows between nodes)
class SummaryState(dict):
    text: str
    formal: str
    casual: str
    bullets: str

# Define nodes
def formal_summary(state: SummaryState):
    llm = ChatOpenAI(model="gpt-4o-mini")
    response = llm.invoke(f"Summarize formally: {state['text']}")
    state["formal"] = response.content
    return state

def casual_summary(state: SummaryState):
    llm = ChatOpenAI(model="gpt-4o-mini")
    response = llm.invoke(f"Summarize casually: {state['text']}")
    state["casual"] = response.content
    return state

def bullet_summary(state: SummaryState):
    llm = ChatOpenAI(model="gpt-4o-mini")
    response = llm.invoke(f"Summarize in bullet points: {state['text']}")
    state["bullets"] = response.content
    return state

# Build the graph
graph = StateGraph(SummaryState)

graph.add_node("formal", formal_summary)
graph.add_node("casual", casual_summary)
graph.add_node("bullets", bullet_summary)

# Start node connects to all three nodes in parallel
graph.set_entry_point("formal")
graph.add_edge("formal", END)
graph.set_entry_point("casual")
graph.add_edge("casual", END)
graph.set_entry_point("bullets")
graph.add_edge("bullets", END)

app = graph.compile()

# Run the graph
input_text = {"text": "LangGraph helps build agent workflows with state and control over execution."}
result = app.invoke(input_text)

print("Formal Summary:", result["formal"])
print("Casual Summary:", result["casual"])
print("Bullet Summary:", result["bullets"])
```

---

## ðŸ”¹ Key Points to Note

1. **Multiple entry points**: All three nodes (`formal`, `casual`, `bullets`) are connected directly to the entry point.
2. **Parallel execution**: Each summary is generated independently.
3. **Aggregation**: The results are collected back in the final state.

---

## ðŸ”¹ Real-World Applications

* **Market research**: Get multiple competitor analyses (e.g., SWOT, bullet points, narrative) in parallel.
* **Healthcare**: Run diagnosis predictions from multiple models concurrently.
* **Education**: Provide different styles of explanation (simple, detailed, analogy-based).
* **Business**: Translate product descriptions into multiple languages at once.

---

âœ… With parallel execution, you gain both **speed** and **versatility**. The next step is learning how to **combine results intelligently** after parallel processing.

---

