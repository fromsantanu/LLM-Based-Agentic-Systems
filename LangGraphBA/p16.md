# Chapter 16: Compliance & Validation

When building production-grade AI workflows, ensuring **trust, safety, and reliability** is just as important as functionality. LangGraph allows you to introduce **compliance checks and validation guardrails** to prevent unsafe or malformed outputs.

These guardrails ensure that:

* Outputs conform to a **defined schema** (e.g., JSON structure, API payload).
* Outputs are **moderated** against unsafe or non-compliant content.
* Sensitive domains (e.g., medical, legal, finance) always include **required disclaimers** or notes.

---

## ‚úÖ Why Compliance & Validation Matter

1. **Consistency** ‚Äì Guarantees that downstream nodes receive properly structured data.
2. **Safety** ‚Äì Prevents harmful or inappropriate responses from being passed forward.
3. **Trust** ‚Äì Builds confidence in your system for end-users and regulators.
4. **Domain Enforcement** ‚Äì Ensures that industry-specific workflows (like healthcare or banking) stay within rules.

---

## ‚öôÔ∏è How to Add Guardrails

### 1. **Schema Validation**

Use `pydantic` models (or JSON schema) to enforce structured output.

```python
from pydantic import BaseModel, ValidationError

class MedicalAdvice(BaseModel):
    advice: str
    disclaimer: str

def validate_output(output: dict):
    try:
        return MedicalAdvice(**output)
    except ValidationError as e:
        print("Validation failed:", e)
        return None
```

In a LangGraph node, wrap the LLM‚Äôs output with `validate_output` before passing it downstream.

---

### 2. **Content Moderation**

You can run every LLM output through a **moderation filter** (e.g., OpenAI‚Äôs moderation API, regex-based filters, or custom rules).

```python
def moderate_output(text: str):
    if "suicidal" in text.lower():
        raise ValueError("Blocked unsafe content.")
    return text
```

This prevents sensitive or harmful content from reaching users.

---

### 3. **Adding Mandatory Disclaimers**

For regulated use cases like **healthcare advice**, you can enforce disclaimers at the node level.

```python
def add_disclaimer(advice: str):
    disclaimer = "‚ö†Ô∏è This is not a substitute for professional medical consultation."
    return {"advice": advice, "disclaimer": disclaimer}
```

---

## üîπ Example: Medical Compliance Guardrail

Imagine a **Health Assistant Agent** that generates advice:

```python
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages

workflow = StateGraph(dict)

# Step 1: Generate advice
def generate_advice(state):
    return {"advice": "Drink more water to stay hydrated."}

# Step 2: Validate & Add Disclaimer
def compliance_check(state):
    validated = add_disclaimer(state["advice"])
    return validated

workflow.add_node("generate", generate_advice)
workflow.add_node("validate", compliance_check)

workflow.add_edge("generate", "validate")
workflow.add_edge("validate", END)

app = workflow.compile()
print(app.invoke({}))
```

**Output:**

```json
{
  "advice": "Drink more water to stay hydrated.",
  "disclaimer": "‚ö†Ô∏è This is not a substitute for professional medical consultation."
}
```

---

## üß© Best Practices

* **Always validate before external API calls** ‚Üí prevents malformed payloads.
* **Centralize compliance checks** in one or two nodes for easier auditing.
* **Log moderation failures** for review and improvement.
* **Customize disclaimers** by domain (e.g., ‚ÄúNot legal advice‚Äù for law, ‚ÄúNot financial advice‚Äù for fintech).
* **Combine schema + moderation** for the strongest guardrails.

---

‚úÖ With compliance and validation in place, your LangGraph apps can **scale safely in real-world production** without risking unreliable, unsafe, or non-compliant outputs.

---
