# Chapter 23: Optimization

As your LangGraph applications grow in complexity and scale, performance optimization becomes crucial. Optimizing for speed, cost, and resource efficiency ensures smooth execution of workflowsâ€”whether youâ€™re running lightweight experiments or production-grade systems.

This chapter covers **performance tuning, caching, and batching**, which are key levers for making LangGraph workflows more efficient.

---

## 23.1 Why Optimization Matters

* **Latency**: Users expect responses in real time. Optimized workflows reduce delays.
* **Cost**: Each API call (LLM, embeddings, search) costs money. Caching and batching can cut costs significantly.
* **Scalability**: Efficient systems scale better under high load.

---

## 23.2 Performance Tuning

Performance tuning focuses on **reducing redundant operations** and **minimizing bottlenecks**.

### Tips:

1. **Profile Your Workflow**

   * Use logging/metrics to identify slow nodes.
   * Example: Embedding lookups vs. LLM calls.

2. **Move Heavy Work Off Critical Path**

   * Precompute embeddings during ingestion instead of query time.
   * Pre-fetch documents if you know theyâ€™ll be needed.

3. **Asynchronous Execution**

   * Combine async nodes (Chapter 7) with batching to maximize throughput.

---

## 23.3 Caching

Caching stores results of expensive operations (like embeddings or LLM calls) to reuse later.

### Types of Caching:

* **Embeddings Cache**:
  Store vector embeddings for texts so repeated queries donâ€™t recompute them.

* **LLM Response Cache**:
  Cache LLM outputs for prompts that are repeated (e.g., common FAQs).

* **Graph-Level Cache**:
  Store intermediate node results across runs.

```python
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.prebuilt import create_llm_cache

# Example: Create a cache for embeddings or LLM outputs
cache = create_llm_cache(SqliteSaver("cache.db"))

# Reuse cached outputs across runs
llm = ChatOpenAI(model="gpt-4o-mini", cache=cache)
```

ðŸ”¹ **Example Use Case**:
Cache embeddings across runs to avoid recomputation and reduce cost.

---

## 23.4 Batching

Batching groups multiple requests into one call, reducing round-trips and overhead.

### Example: Batch Embedding Requests

```python
from openai import OpenAI
client = OpenAI()

texts = ["doc1 content", "doc2 content", "doc3 content"]

# Batch call instead of one-by-one
embeddings = client.embeddings.create(
    model="text-embedding-3-small",
    input=texts
)
```

* **Benefits**:

  * Faster total runtime.
  * Lower API overhead.
  * Often cheaper than single calls.

---

## 23.5 Putting It All Together

An optimized LangGraph workflow typically combines:

* **Caching** embeddings to avoid recomputation.
* **Batching** queries for speed.
* **Profiling** and tuning bottlenecks.

---

## 23.6 Example: Optimized RAG Workflow

1. **Ingestion Phase**

   * Batch embeddings creation.
   * Cache results in vector DB.

2. **Query Phase**

   * Use cached embeddings for known documents.
   * Run LLM summarization with caching enabled for repeated prompts.

```python
# Pseudocode for optimized graph
graph = StateGraph()

graph.add_node("embed", batch_embed_and_cache)
graph.add_node("retrieve", retrieve_from_vector_db)
graph.add_node("summarize", cached_llm_summarizer)

graph.add_edge("embed", "retrieve")
graph.add_edge("retrieve", "summarize")
```

---

## 23.7 Checklist for Optimization

âœ… Profile nodes for latency and cost.
âœ… Cache embeddings and repeated LLM outputs.
âœ… Batch requests where supported.
âœ… Precompute when possible.
âœ… Monitor performance in production.

---

ðŸ”¹ **Example Recap**: Cache embeddings across runs to reduce cost.

---
