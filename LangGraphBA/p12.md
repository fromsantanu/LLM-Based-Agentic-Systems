# Chapter 12: LangChain Compatibility

One of the most powerful aspects of **LangGraph** is that itâ€™s designed to be **compatible with LangChain**. This means you can reuse the tools, retrievers, and agents you already built with LangChain inside a LangGraph workflow.

LangChain provides a wide ecosystem of integrations (vector stores, retrievers, tools, agents). LangGraph brings orchestration, state, and reliability on top. By combining the two, you can get the best of both worlds:

* **LangChain** â†’ ecosystem of components.
* **LangGraph** â†’ orchestration, state management, branching, retries.

---

## Key Compatibility Points

1. **Tools**

   * Any LangChain tool (`BaseTool`) can be wrapped inside a LangGraph node.
   * Example: Calculator, Search API, or custom Python function.

2. **Retrievers**

   * LangChain retrievers can be plugged into LangGraph to power Retrieval-Augmented Generation (RAG).
   * Example: Use a FAISS retriever to fetch relevant documents, then pass results to an LLM node.

3. **Agents**

   * You can insert a LangChain agent (like `initialize_agent`) as a node in your LangGraph.
   * Useful when you want the agent to reason through multiple tools, while still benefiting from LangGraphâ€™s flow control.

---

## Example: Document Q&A with a Retriever Node

Suppose you have a vector store retriever built in LangChain:

```python
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings

# Build retriever
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.load_local("docs_index", embeddings)
retriever = vectorstore.as_retriever()
```

You can plug this **retriever** into LangGraph as a node:

```python
from langgraph.graph import StateGraph, END

# Node function
def retrieve_docs(state):
    query = state["question"]
    docs = retriever.get_relevant_documents(query)
    return {"docs": docs}

# Define graph
workflow = StateGraph(dict)

workflow.add_node("retriever", retrieve_docs)
workflow.add_node("llm", lambda state: {
    "answer": f"Based on docs: {state['docs']}"
})

workflow.add_edge("retriever", "llm")
workflow.set_entry_point("retriever")
workflow.set_finish_point("llm")

graph = workflow.compile()
```

---

## Execution

```python
result = graph.invoke({"question": "What is LangGraph?"})
print(result["answer"])
```

ðŸ”¹ Flow:

1. `retriever` fetches documents with LangChain.
2. `llm` consumes retrieved docs and produces an answer.

---

## Takeaways

* LangGraph doesnâ€™t replace LangChainâ€”it enhances it.
* You can embed **LangChain retrievers, tools, and agents** as nodes.
* This makes migration easier: reuse your existing LangChain components in a structured LangGraph workflow.

---

âœ… Next, you may want to explore how to **deploy LangGraph workflows in production**, combining both LangChain integrations and LangGraphâ€™s orchestration.

---
