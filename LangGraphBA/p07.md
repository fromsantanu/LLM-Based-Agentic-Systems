# Chapter 7: Async & Streaming

In real-world applications, especially those involving **LLMs and user interaction**, responsiveness is critical. LangGraph provides support for **asynchronous execution** and **streaming responses**, enabling smoother, faster, and more interactive workflows.

---

## 1. Why Async Matters

* **Async (Asynchronous Execution):**
  Lets the graph perform tasks without blocking the main execution flow.
  Example: Fetching data from APIs, running multiple model calls in parallel.

* **Streaming:**
  Enables partial results to be sent to the user **while the computation is ongoing**, instead of waiting for the full result.
  Example: Showing an LLMâ€™s output as itâ€™s being typed.

---

## 2. Async Tasks in LangGraph

LangGraph supports async nodes and execution. You can declare your node functions as **`async def`**.

```python
from langgraph.graph import StateGraph

async def fetch_data(state):
    # Simulating async API call
    await asyncio.sleep(1)
    return {"data": "Fetched asynchronously!"}

graph = StateGraph(dict)
graph.add_node("fetch_data", fetch_data)
```

When executed, this node wonâ€™t block the rest of the event loop, making it suitable for scalable agent workflows.

---

## 3. Streaming Responses

LangGraph provides built-in utilities to **stream outputs**.
This is especially useful for LLMs that support token-by-token generation.

```python
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph

llm = ChatOpenAI(streaming=True)

async def stream_llm(state):
    response = ""
    async for chunk in llm.astream("Tell me a story about LangGraph"):
        print(chunk, end="", flush=True)  # Stream in real-time
        response += chunk
    return {"story": response}

graph = StateGraph(dict)
graph.add_node("stream_llm", stream_llm)
```

ðŸ‘‰ Here, the output is shown **as itâ€™s generated**, creating a chat-like streaming effect.

---

## 4. Combining Async + Streaming

Often, youâ€™ll want to:

1. Fetch data asynchronously.
2. Stream results back to the user in real time.

```python
async def async_streaming_pipeline(state):
    # Step 1: Async data fetch
    data = await fetch_external_data()

    # Step 2: Stream generation based on data
    response = ""
    async for chunk in llm.astream(f"Summarize this data: {data}"):
        yield chunk   # Streaming chunk
        response += chunk

    return {"summary": response}
```

This makes your workflow **non-blocking and interactive**.

---

## 5. Use Cases

* **LLM Chatbots** â†’ Stream answers to users while the model is thinking.
* **Data Pipelines** â†’ Fetch multiple APIs asynchronously for faster response times.
* **Agent Systems** â†’ Run background tasks while continuing the main workflow.
* **Healthcare dashboards** â†’ Show live patient updates from monitoring devices.

---

## ðŸ”¹ Example: Stream an LLMâ€™s Response While Itâ€™s Being Generated

```python
async def stream_chat(state):
    prompt = state.get("prompt", "Explain async & streaming in simple terms.")
    response = ""
    async for chunk in llm.astream(prompt):
        print(chunk, end="", flush=True)  # live stream
        response += chunk
    return {"output": response}
```

When run, youâ€™ll see the LLMâ€™s answer appear **token by token**, just like in a real chat app.

---

âœ… With async and streaming, LangGraph workflows become **faster, scalable, and more user-friendly**, crucial for production-ready applications.

---
